{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import yaml\nfrom pprint import pprint\nwith open('../input/localnb001-transformers-ner/config.yml') as file:\n    CFG = yaml.load(file, Loader=yaml.FullLoader)\npprint(CFG)\n\nBATCH_SIZE = CFG[\"test_batch_size\"]\nTRAIN = False\nMAX_LEN = CFG[\"max_len\"]\nUSE_POS = CFG[\"use_pos\"]\nDEBUG = CFG[\"debug\"]\nTEXT_LEN = CFG[\"text_len\"]\nTAGS_VALS = CFG[\"tags_vals\"]\nUSE_TPU = False","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:50:31.007924Z","iopub.execute_input":"2021-05-30T05:50:31.008239Z","iopub.status.idle":"2021-05-30T05:50:31.023651Z","shell.execute_reply.started":"2021-05-30T05:50:31.008209Z","shell.execute_reply":"2021-05-30T05:50:31.022907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if USE_TPU:\n    ##### PyTorch-XLA installation via internet connection\n\n    ##!cp -r ../input/localnb001-transformers-ner/* .\n    #!cp -r ../input/d/riow1983/localnb001-transformers-ner/* .\n    #!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev\n\n\n\n\n\n    ##### PyTorch-XLA installation via source code (offline installation)\n\n    # Credit to https://www.kaggle.com/joshi98kishan/foldtraining-pytorch-tpu-8-cores/data?scriptVersionId=48061653\n    #Copying all the required wheel files from the created kaggle dataset to the working dir.\n    !cp ../input/pytorch-xla-setup-script/torch-nightly-cp37-cp37m-linux_x86_64.whl ./torch-nightly-cp37-cp37m-linux_x86_64.whl\n    !cp ../input/pytorch-xla-setup-script/torch_xla-nightly-cp37-cp37m-linux_x86_64.whl ./torch_xla-nightly-cp37-cp37m-linux_x86_64.whl\n    !cp ../input/pytorch-xla-setup-script/torchvision-nightly-cp37-cp37m-linux_x86_64.whl ./torchvision-nightly-cp37-cp37m-linux_x86_64.whl\n\n    # This deb files are the dependencies, copying them to the working dir.\n    !cp ../input/pytorch-xla-setup-script/libgfortran4_7.5.0-3ubuntu1_18.04_amd64.deb ./libgfortran4_7.5.0-3ubuntu1_18.04_amd64.deb\n    !cp ../input/pytorch-xla-setup-script/libomp5_5.0.1-1_amd64.deb ./libomp5_5.0.1-1_amd64.deb\n    !cp ../input/pytorch-xla-setup-script/libopenblas-base_0.2.20ds-4_amd64.deb ./libopenblas-base_0.2.20ds-4_amd64.deb\n    !cp ../input/pytorch-xla-setup-script/libopenblas-dev_0.2.20ds-4_amd64.deb ./libopenblas-dev_0.2.20ds-4_amd64.deb\n\n    #installing pytorch-xla by running this script\n    !python ../input/pytorch-xla-setup-script/pytorch-xla-env-setup.py --version nightly\n\n    #Now, istalling depedencies\n    !dpkg -i ./libgfortran4_7.5.0-3ubuntu1_18.04_amd64.deb\n    !dpkg -i ./libomp5_5.0.1-1_amd64.deb\n    !dpkg -i ./libopenblas-base_0.2.20ds-4_amd64.deb\n    !dpkg -i ./libopenblas-dev_0.2.20ds-4_amd64.deb\n\n    # Removing wheel and deb files, as we don't need them now.\n    !rm torch-nightly-cp37-cp37m-linux_x86_64.whl \n    !rm torch_xla-nightly-cp37-cp37m-linux_x86_64.whl \n    !rm torchvision-nightly-cp37-cp37m-linux_x86_64.whl\n    !rm libgfortran4_7.5.0-3ubuntu1_18.04_amd64.deb \n    !rm libomp5_5.0.1-1_amd64.deb \n    !rm libopenblas-base_0.2.20ds-4_amd64.deb \n    !rm libopenblas-dev_0.2.20ds-4_amd64.deb","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:50:31.024889Z","iopub.execute_input":"2021-05-30T05:50:31.025123Z","iopub.status.idle":"2021-05-30T05:50:31.111735Z","shell.execute_reply.started":"2021-05-30T05:50:31.0251Z","shell.execute_reply":"2021-05-30T05:50:31.110809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing pytorch and the library for TPU execution\n\nimport torch\nif USE_TPU:\n    import torch_xla\n    import torch_xla.core.xla_model as xm","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:50:31.113789Z","iopub.execute_input":"2021-05-30T05:50:31.114176Z","iopub.status.idle":"2021-05-30T05:50:32.32356Z","shell.execute_reply.started":"2021-05-30T05:50:31.114138Z","shell.execute_reply":"2021-05-30T05:50:32.322696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertForTokenClassification, BertTokenizer, BertConfig, BertModel\n\nif USE_TPU:\n    # Preparing for TPU usage\n    dev = xm.xla_device()\nelse:\n    #dev = torch.device('cuda:0')\n    dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(dev)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:50:32.327023Z","iopub.execute_input":"2021-05-30T05:50:32.327313Z","iopub.status.idle":"2021-05-30T05:50:32.750138Z","shell.execute_reply.started":"2021-05-30T05:50:32.327286Z","shell.execute_reply":"2021-05-30T05:50:32.749194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining some key variables that will be used later on in the training\n# test = False\n# use_pos = False\n# CV = 1\n# MAX_LEN = 290\n# BATCH_SIZE = 4 #16\ntokenizer = BertTokenizer.from_pretrained('../input/localnb001-transformers-ner/bert-base-uncased-vocab.txt')\n#tokenizer = BertTokenizer.from_pretrained('bert-base-cased-vocab.txt')\n#tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n#tokenizer = BertTokenizer.from_pretrained(f'../input/localnb001-transformers-ner/bert-base-cased-ner-cv{CV}.pt')\n#tokenizer = BertTokenizer.from_pretrained(f'../input/localnb001-transformers-ner')\n#tokenizer = BertTokenizer.from_pretrained('../input/d/riow1983/localnb001-transformers-ner/bert-base-cased')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:50:32.751486Z","iopub.execute_input":"2021-05-30T05:50:32.751824Z","iopub.status.idle":"2021-05-30T05:50:32.79876Z","shell.execute_reply.started":"2021-05-30T05:50:32.751788Z","shell.execute_reply":"2021-05-30T05:50:32.797892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp ../input/localnb001-transformers-ner/bridge.py ./bridge.py\n#from bridge import *\n\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm.notebook import tqdm\nimport gc\nimport re\nimport pickle\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nif USE_POS:\n    import spacy\n    nlp = spacy.load(\"en_core_web_sm\")","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:50:32.800065Z","iopub.execute_input":"2021-05-30T05:50:32.80054Z","iopub.status.idle":"2021-05-30T05:50:33.451282Z","shell.execute_reply.started":"2021-05-30T05:50:32.800501Z","shell.execute_reply":"2021-05-30T05:50:33.450301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"markdown","source":"Credit to: https://www.kaggle.com/c/coleridgeinitiative-show-us-the-data/discussion/230341","metadata":{}},{"cell_type":"code","source":"!python ./bridge.py {TRAIN} {MAX_LEN} {USE_POS} {DEBUG} {TEXT_LEN} {TAGS_VALS}","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:50:33.453522Z","iopub.execute_input":"2021-05-30T05:50:33.453796Z","iopub.status.idle":"2021-05-30T05:50:40.346632Z","shell.execute_reply.started":"2021-05-30T05:50:33.453766Z","shell.execute_reply":"2021-05-30T05:50:40.345643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = pickle.load(open(\"./sentences.pkl\", 'rb'))\nprint(sentences[0])","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:50:40.349764Z","iopub.execute_input":"2021-05-30T05:50:40.350024Z","iopub.status.idle":"2021-05-30T05:50:40.359534Z","shell.execute_reply.started":"2021-05-30T05:50:40.349997Z","shell.execute_reply":"2021-05-30T05:50:40.358412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_labels = len(TAGS_VALS.split())","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:50:40.361935Z","iopub.execute_input":"2021-05-30T05:50:40.36257Z","iopub.status.idle":"2021-05-30T05:50:40.36732Z","shell.execute_reply.started":"2021-05-30T05:50:40.362529Z","shell.execute_reply":"2021-05-30T05:50:40.366028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n# Deprecated from here","metadata":{}},{"cell_type":"code","source":"# def get_text(filename, test=test):\n#     if test:\n#         df = pd.read_json('../input/coleridgeinitiative-show-us-the-data/test/{}.json'.format(filename))\n#     else:\n#         df = pd.read_json('../input/coleridgeinitiative-show-us-the-data/train/{}.json'.format(filename))\n#     text = \" \".join(list(df['text']))\n#     return text\n\n\n# def clean_text(txt):\n#     return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n\n\n\n\n# def convert_tokens(x, m, max_len, train=False, verbose=False):\n#     \"\"\"\n#     Args:\n#         x: df row\n#         m: row index\n#     Returns:\n#         df\n    \n#     ex) convert_tokens(row,i, MAX_LEN)\n#     \"\"\"\n#     df = pd.DataFrame()\n#     if use_pos:\n#         text = x[\"tok\"]\n#         pos = x[\"pos\"]\n#     else:\n#         text = x['text'].replace('\\uf0b7','').split()\n\n#     if train:\n#         entity = x['dataset_label']\n\n#         ## main\n#         tokens=[]\n#         k=0\n#         for i,x in enumerate(text):\n\n#             if k==0:\n#                 if x==entity.split()[0]:\n#                     entity_len = len(entity.split())\n#                     if entity == ' '.join(text[i:i+entity_len]):\n#                         tokens.extend(['o-dataset']*len(entity.split()))\n#                         k = entity_len\n#                     else:\n#                         tokens.append('o')\n#                 else:\n#                     tokens.append('o')\n\n\n#             k = max(0,k-1)\n    \n\n#     k=0\n#     sentence_hash=[]\n#     for i in range(0,len(text),max_len):\n#         if verbose:\n#             print(\"Is length of text[i:i+max_len] 290?\", len(text[i:i+max_len]))\n#         sentence_hash.extend([f'sentence#{k}']* len(text[i:i+max_len]))\n#         k+=1\n\n#     #df['word']=list(map(str,text))\n#     df['word'] = text\n#     if use_pos:\n#         df['pos'] = pos\n#     else:\n#         df['pos'] = None\n#     df['sentence'] = f'sentence{m}'\n#     df['sentence#'] = sentence_hash\n#     if train:\n#         df['tag'] = tokens\n\n#     return df","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:50:40.369053Z","iopub.execute_input":"2021-05-30T05:50:40.369884Z","iopub.status.idle":"2021-05-30T05:50:40.376499Z","shell.execute_reply.started":"2021-05-30T05:50:40.369643Z","shell.execute_reply":"2021-05-30T05:50:40.375539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ## read data\n# if test:\n#     df_test = pd.read_csv(\"../input/coleridgeinitiative-show-us-the-data/sample_submission.csv\")\n# else:\n#     df_test = pd.read_csv(\"../input/coleridgeinitiative-show-us-the-data/train.csv\")\n# df_test['text'] = df_test['Id'].apply(get_text)\n\n# ## clean text\n# df_test[\"text\"] = df_test[\"text\"].apply(lambda x: clean_text(x))\n\n# df_test","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-05-30T05:50:40.377844Z","iopub.execute_input":"2021-05-30T05:50:40.378411Z","iopub.status.idle":"2021-05-30T05:50:40.388428Z","shell.execute_reply.started":"2021-05-30T05:50:40.378342Z","shell.execute_reply":"2021-05-30T05:50:40.387671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #num_labels = dataset[\"tag\"].nunique() + 1\n# num_labels = 3","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:50:40.389952Z","iopub.execute_input":"2021-05-30T05:50:40.390639Z","iopub.status.idle":"2021-05-30T05:50:40.401383Z","shell.execute_reply.started":"2021-05-30T05:50:40.390598Z","shell.execute_reply":"2021-05-30T05:50:40.400488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Creating a class to pull the words from the columns and create them into sentences\n\n# class SentenceGetter(object):\n    \n#     def __init__(self, dataset):\n#         self.n_sent = 1\n#         self.dataset = dataset\n#         self.empty = False\n#         if use_pos:\n#             agg_func = lambda s: [(w,p) for w,p in zip(s[\"word\"].values.tolist(),\n#                                                        s[\"pos\"].values.tolist())]\n#         else:\n#             agg_func = lambda s: [(w,) for w in s[\"word\"].values.tolist()]\n#         self.grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n#         self.sentences = [s for s in self.grouped]\n    \n#     def get_next(self):\n#         try:\n#             s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n#             self.n_sent += 1\n#             return s\n#         except:\n#             return None\n\n# #### RIOW\n# #getter = SentenceGetter(dataset)\n# #### RIOWRIOW","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:50:40.402924Z","iopub.execute_input":"2021-05-30T05:50:40.403293Z","iopub.status.idle":"2021-05-30T05:50:40.410099Z","shell.execute_reply.started":"2021-05-30T05:50:40.403258Z","shell.execute_reply":"2021-05-30T05:50:40.409303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deprecated above\n***","metadata":{}},{"cell_type":"markdown","source":"- [x] ToDo: [localnb001-transformers-ner.ipynb](https://github.com/riow1983/Kaggle-Coleridge-Initiative/blob/main/notebooks/localnb001-transformers-ner.ipynb)を参考にDataloader, pred関数を実装する \n- [x] ToDo: finalテーブルをベースに, pred entityを加えたsubmission.csvを作成する処理を実装する","metadata":{}},{"cell_type":"markdown","source":"# Dataloader","metadata":{}},{"cell_type":"code","source":"# example: https://huggingface.co/transformers/model_doc/bert.html\n'''\nfrom transformers import BertTokenizer, BertModel\nimport torch\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:50:40.412854Z","iopub.execute_input":"2021-05-30T05:50:40.413759Z","iopub.status.idle":"2021-05-30T05:50:40.424674Z","shell.execute_reply.started":"2021-05-30T05:50:40.413708Z","shell.execute_reply":"2021-05-30T05:50:40.423768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### RIOW\n# class CustomDataset(Dataset):\n#     #### RIOW\n# #     def __init__(self, tokenizer, sentences, poses, max_len):\n# #         self.len = len(sentences)\n# #         self.sentences = sentences\n# #         self.poses = poses\n# #         self.tokenizer = tokenizer\n# #         self.max_len = max_len\n#     def __init__(self, tokenizer, df, max_len):\n#         self.df = df\n#         #self.lengths = [len(d) for d in self.df[\"text\"]]\n#         self.lengths = [np.ceil(len(d.split(\" \"))/MAX_LEN).astype(int) for d in self.df[\"text\"]]\n#         self.tokenizer = tokenizer\n#         self.max_len = max_len\n#     #### RIOWRIOW\n    \n#     def __len__(self):\n#         #### RIOW\n#         #return self.len\n#         return sum(self.lengths)\n#         #return sum([np.ceil(l/MAX_LEN).astype(int) for l in self.lengths])\n#         #### RIOWRIOW\n        \n#     def index_converter(self, text_lengths, original_index, verbose=False):\n#         # reference:\n#         # [1] https://gist.github.com/Miladiouss/6ba0876f0e2b65d0178be7274f61ad2f\n#         accum = np.add.accumulate(text_lengths)\n#         sentence_index = len(np.argwhere(accum <= original_index))\n#         if verbose:\n#             print(f\"*********** original_index: {original_index} *************\")\n#             print(\"sentence_index:\", sentence_index)\n#         #index_wrt_sentence = (original_index - np.insert(accum, 0, 0)[sentence_index]) // MAX_LEN\n#         #index_wrt_sentence = np.floor((original_index - np.insert(accum, 0, 0)[sentence_index]) / MAX_LEN).astype(int)\n#         index_wrt_sentence = original_index - np.insert(accum, 0, 0)[sentence_index]\n#         if verbose:\n#             print(\"index_wrt_sentence:\", index_wrt_sentence)\n#             print()\n#         return sentence_index, index_wrt_sentence\n        \n#     def __getitem__(self, index):\n#         #### RIOW\n# #         sentence = str(self.sentences[index])\n# #         if use_pos:\n# #             pos = str(self.poses[index])\n# #         else:\n# #             pos = None\n        \n#         #df = self.df[index]\n#         #sentence_index, index_wrt_sentence = self.index_converter(self.lengths, index*MAX_LEN)\n#         sentence_index, index_wrt_sentence = self.index_converter(self.lengths, index, verbose=False)\n#         #df = self.df[converted_index]\n#         df = self.df[self.df.index==sentence_index]\n#         # POS tagging\n#         if use_pos:\n#             tok, pos = [], []\n#             #bar = tqdm(total = df.shape[0])\n#             for doc in nlp.pipe(df['text'].values, batch_size=50, n_process=-1):\n#                 if doc.is_parsed:\n#                     tok.append([n.text for n in doc])\n#                     pos.append([n.pos_ for n in doc])\n#                 else:\n#                     # We want to make sure that the lists of parsed results have the\n#                     # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n#                     tok.append(None)\n#                     pos.append(None)\n#                 #bar.update(1)\n#             df[\"tok\"] = tok\n#             df[\"pos\"] = pos\n#         else:\n#             df[\"tok\"] = None\n#             df[\"pos\"] = None\n        \n#         # process\n#         dataset = pd.DataFrame()\n#         #bar = tqdm(total = df.shape[0])\n#         for i,row in df.iterrows():\n#             _df = convert_tokens(row,i, MAX_LEN, verbose=False)\n#             dataset = dataset.append(_df,ignore_index=True)\n#             #bar.update(1)\n            \n#         dataset[\"sentence_idx\"] = dataset[\"sentence\"] + dataset[\"sentence#\"]\n#         #dataset = dataset[[\"sentence\", \"sentence_idx\", \"word\", \"pos\"]].copy()\n#         #dataset.rename(columns={\"token\":\"word\"}, inplace=True)\n        \n#         getter = SentenceGetter(dataset)\n        \n#         #print(\"getter.sentences:\", getter.sentences)\n#         #for sent in getter.sentences:\n#         #    print(\"lenght of sent:\", len(sent))\n            \n#         sentences = [' '.join([s[0] for s in sent]) for sent in getter.sentences]\n#         if use_pos:\n#             poses = [' '.join([s[1] for s in sent]) for sent in getter.sentences]\n#         else:\n#             poses = None\n            \n#         #print(\"len(sentences):\", len(sentences))\n#         #for sentence in sentences:\n#         #    print(\"len(sentence.split(\" \")):\", len(sentence.split(\" \")))\n#         #print(\"sentences:\", sentences)\n#         #print(\"index_wrt_sentence:\", index_wrt_sentence)\n        \n#         sentence = str(sentences[index_wrt_sentence])\n#         if use_pos:\n#             pos = str(poses[index_wrt_sentence])\n#         else:\n#             pos = None\n#         #### RIOWRIOW\n        \n    \n#         inputs = self.tokenizer.encode_plus(\n#             sentence,\n#             #### RIOW\n#             #None,\n#             pos,\n#             #### RIOWRIOW\n#             add_special_tokens=True,\n#             max_length=self.max_len,\n#             pad_to_max_length=True,\n#             return_token_type_ids=True\n#         )\n#         ids = inputs['input_ids']\n#         mask = inputs['attention_mask']\n\n#         return {\n#             'ids': torch.tensor(ids, dtype=torch.long),\n#             'mask': torch.tensor(mask, dtype=torch.long)\n#         } \n    \n    \nclass CustomDataset(Dataset):\n    def __init__(self, sentences, tokenizer, max_len):\n        self.len = len(sentences)\n        self.sentences = sentences\n        #self.poses = poses\n        #self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __getitem__(self, index):\n        tmp = self.sentences[index]\n        sentence = \" \".join(tmp[0])\n        \n        if USE_POS:\n            #pos = str(self.poses[index])\n            pos = \" \".join(tmp[1])\n        else:\n            #pos = self.poses # which is None\n            pos = tmp[1]\n        \n        inputs = self.tokenizer.encode_plus(\n            sentence,\n            pos,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            truncation=True,\n            pad_to_max_length=True, # future warning (deprecated)\n            #padding=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n\n        if TRAIN:\n            #label = self.labels[index]\n            label = list(tmp[2])\n            label.extend([2]*self.max_len) # tag2idx = {'o':0, 'o-dataset':1, 'pad':2}\n            label = label[:self.max_len]\n        \n            return {'ids': torch.tensor(ids, dtype=torch.long),\n                    'mask': torch.tensor(mask, dtype=torch.long),\n                    'tags': torch.tensor(label, dtype=torch.long)} \n        else:\n            return {'ids': torch.tensor(ids, dtype=torch.long),\n                    'mask': torch.tensor(mask, dtype=torch.long)} \n    \n    def __len__(self):\n        return self.len\n    \n    \n    \n#### RIOWRIOW","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:50:40.426194Z","iopub.execute_input":"2021-05-30T05:50:40.426964Z","iopub.status.idle":"2021-05-30T05:50:40.440945Z","shell.execute_reply.started":"2021-05-30T05:50:40.426885Z","shell.execute_reply":"2021-05-30T05:50:40.439981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### RIOW\n#testing_set = CustomDataset(tokenizer, sentences, poses, MAX_LEN)\n#testing_set = CustomDataset(tokenizer, df_test, MAX_LEN)\ntesting_set = CustomDataset(sentences, tokenizer, MAX_LEN)\n#### RIOWRIOW\n\n# cf. \n'''Warning:\nTruncation was not explicitly activated but `max_length` is provided a specific value, \nplease use `truncation=True` to explicitly truncate examples to max length. \n\n... or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:50:40.44409Z","iopub.execute_input":"2021-05-30T05:50:40.444575Z","iopub.status.idle":"2021-05-30T05:50:40.455426Z","shell.execute_reply.started":"2021-05-30T05:50:40.444547Z","shell.execute_reply":"2021-05-30T05:50:40.45441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_params = {'batch_size': BATCH_SIZE,\n               'shuffle': False,\n               'num_workers': 8 #0\n                }\n\ntesting_loader = DataLoader(testing_set, **test_params)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:50:40.457101Z","iopub.execute_input":"2021-05-30T05:50:40.457634Z","iopub.status.idle":"2021-05-30T05:50:40.463753Z","shell.execute_reply.started":"2021-05-30T05:50:40.457579Z","shell.execute_reply":"2021-05-30T05:50:40.462796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the fine-tuned pre-trained BERT model","metadata":{}},{"cell_type":"code","source":"# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n\nclass BERTClass(torch.nn.Module):\n    def __init__(self):\n        super(BERTClass, self).__init__()\n        #self.l1 = transformers.BertForTokenClassification.from_pretrained('../input/localnb001-transformers-ner/bert-base-cased')\n        #self.l1 = transformers.BertForTokenClassification.from_pretrained(f'../input/localnb001-transformers-ner/bert-base-cased-ner-cv{CV}.pth', num_labels=num_labels)\n        #self.l1 = transformers.BertForTokenClassification.from_pretrained(f'../input/localnb001-transformers-ner/bert-base-cased-ner-cv{CV}.bin')\n        #self.l1 = transformers.BertForTokenClassification.from_pretrained('../input/d/riow1983/localnb001-transformers-ner', num_labels=num_labels)\n        self.l1 = transformers.BertForTokenClassification.from_pretrained('../input/localnb001-transformers-ner', num_labels=num_labels)\n        #self.l1 = transformers.BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=num_labels) # This requires internet connection.\n        \n        # self.l2 = torch.nn.Dropout(0.3)\n        # self.l3 = torch.nn.Linear(768, 200)\n    \n    def forward(self, ids, mask, labels):\n        output_1= self.l1(ids, mask, labels = labels)\n        # output_2 = self.l2(output_1[0])\n        # output = self.l3(output_2)\n        return output_1","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:50:40.465129Z","iopub.execute_input":"2021-05-30T05:50:40.465753Z","iopub.status.idle":"2021-05-30T05:50:40.473017Z","shell.execute_reply.started":"2021-05-30T05:50:40.4657Z","shell.execute_reply":"2021-05-30T05:50:40.472103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BERTClass()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:50:40.474406Z","iopub.execute_input":"2021-05-30T05:50:40.47509Z","iopub.status.idle":"2021-05-30T05:50:48.518842Z","shell.execute_reply.started":"2021-05-30T05:50:40.475053Z","shell.execute_reply":"2021-05-30T05:50:48.518083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print model's state_dict\nprint(\"Model's state_dict:\")\nfor param_tensor in model.state_dict():\n    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:50:48.520217Z","iopub.execute_input":"2021-05-30T05:50:48.520546Z","iopub.status.idle":"2021-05-30T05:50:48.843707Z","shell.execute_reply.started":"2021-05-30T05:50:48.520511Z","shell.execute_reply":"2021-05-30T05:50:48.842823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(USE_TPU)\nprint(dev)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:50:48.844994Z","iopub.execute_input":"2021-05-30T05:50:48.845362Z","iopub.status.idle":"2021-05-30T05:50:48.853032Z","shell.execute_reply.started":"2021-05-30T05:50:48.845323Z","shell.execute_reply":"2021-05-30T05:50:48.852156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save on CPU, Load on GPU (Example)\n```\n# Save\ntorch.save(net.state_dict(), PATH)\n\n# Load\ndevice = torch.device(\"cuda\")\nmodel = Net()\n# Choose whatever GPU device number you want\nmodel.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))\n# Make sure to call input = input.to(device) on any input tensors that you feed to the model\nmodel.to(device)\n```\nrefenrece: https://pytorch.org/tutorials/recipes/recipes/save_load_across_devices.html","metadata":{}},{"cell_type":"code","source":"# ToDo: cased -> uncased\n#output_model = f\"../input/localnb001-transformers-ner/bert-base-cased-ner-pad-cv{CV}.pth\"\nCV = 1\nif USE_POS:\n    output_model = f\"../input/localnb001-transformers-ner/bert-base-cased-ner-pad-cv{CV}-epochs5.pth\"\nelse:\n    output_model = f\"../input/localnb001-transformers-ner/bert-base-cased-ner-pad-nopos-cv{CV}-epochs5.pth\"\n\nif USE_TPU:\n    checkpoint = torch.load(output_model, map_location='tpu')\nelse:\n    #checkpoint = torch.load(output_model, map_location=torch.device('cpu'))\n    checkpoint = torch.load(output_model, map_location=\"cuda:0\")\n\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.to(dev)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:50:48.856152Z","iopub.execute_input":"2021-05-30T05:50:48.856448Z","iopub.status.idle":"2021-05-30T05:51:03.207323Z","shell.execute_reply.started":"2021-05-30T05:50:48.856419Z","shell.execute_reply":"2021-05-30T05:51:03.206493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"~~ToDo: fine-tuned modelの保存ファイルの拡張子は.pthではなく.binにする~~  \n~~reference: https://github.com/huggingface/transformers/issues/1620~~","metadata":{}},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"#tags_vals = ['o-dataset', 'o']\n#tags_vals = ['o', 'o-dataset', 'pad'] # the order is the same as localnb001\ntags_vals = TAGS_VALS.split()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:03.211202Z","iopub.execute_input":"2021-05-30T05:51:03.211459Z","iopub.status.idle":"2021-05-30T05:51:03.215281Z","shell.execute_reply.started":"2021-05-30T05:51:03.211433Z","shell.execute_reply":"2021-05-30T05:51:03.214184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pred(model, testing_loader, verbose=False):\n    model.eval()\n    \n    predictions = []\n    with torch.no_grad():\n        for _, data in enumerate(testing_loader, 0):\n            ids = data['ids'].to(dev, dtype = torch.long)\n            print(_, ids.shape)\n            mask = data['mask'].to(dev, dtype = torch.long)\n\n            #output = model(ids, mask, mask)\n            output = model(ids, mask, labels=None)\n            #loss, logits = output[:2]\n            logits = output[0]\n            logits = logits.detach().cpu().numpy()\n            \n            predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n            if verbose:\n                #print(\"predictions:\", predictions)\n                print(\"length of predictions:\", len(predictions))\n                for p in predictions:\n                    print(\"length of p:\", len(p))\n                print()\n                \n        pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n        if verbose:\n            print(\"pred_tags:\", pred_tags)\n            print()\n        \n    return pred_tags","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:03.217436Z","iopub.execute_input":"2021-05-30T05:51:03.217812Z","iopub.status.idle":"2021-05-30T05:51:03.228044Z","shell.execute_reply.started":"2021-05-30T05:51:03.217776Z","shell.execute_reply":"2021-05-30T05:51:03.22723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To get the results on the validation set. This data is not seen by the model\n\npred_tags = pred(model, testing_loader, verbose=False)\n\n'''Warning:\nTruncation was not explicitly activated but `max_length` is provided a specific value, \nplease use `truncation=True` to explicitly truncate examples to max length. \nDefaulting to 'longest_first' truncation strategy. \n\nIf you encode pairs of sequences (GLUE-style) with the tokenizer \nyou can select this strategy more precisely by providing a specific strategy to `truncation`.\n\n\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: \nThe `pad_to_max_length` argument is deprecated and will be removed in a future version, \nuse `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, \nor use `padding='max_length'` to pad to a max length. \nIn this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or \nleave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n'''","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:03.229287Z","iopub.execute_input":"2021-05-30T05:51:03.229747Z","iopub.status.idle":"2021-05-30T05:51:05.486714Z","shell.execute_reply.started":"2021-05-30T05:51:03.2297Z","shell.execute_reply":"2021-05-30T05:51:05.485542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred_tags 内容確認\nnp.unique(pred_tags)\n# EPOCHS=1では全て'o'で予測してしまっていた\n# EPOCHS=5では逆に全て'o-dataset'で予測してしまっている","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.488193Z","iopub.execute_input":"2021-05-30T05:51:05.488555Z","iopub.status.idle":"2021-05-30T05:51:05.496745Z","shell.execute_reply.started":"2021-05-30T05:51:05.488514Z","shell.execute_reply":"2021-05-30T05:51:05.495736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(len(pred_tags), pred_tags[:10])\n# 175 * MAX_LEN(=290) = 50750","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.49832Z","iopub.execute_input":"2021-05-30T05:51:05.498741Z","iopub.status.idle":"2021-05-30T05:51:05.506572Z","shell.execute_reply.started":"2021-05-30T05:51:05.498694Z","shell.execute_reply":"2021-05-30T05:51:05.505677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"finalテーブルをベースに, pred entityを加えたsubmission.csvを作成する処理を実装する","metadata":{}},{"cell_type":"code","source":"# Id = \"2100032a-7c33-4bff-97ef-690822c43466\"\n# pstns = [tmp[-1]==Id for tmp in sentences]\n# positions_contracted = np.array(pstns)\n# positions_contracted","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.50793Z","iopub.execute_input":"2021-05-30T05:51:05.508397Z","iopub.status.idle":"2021-05-30T05:51:05.51682Z","shell.execute_reply.started":"2021-05-30T05:51:05.508359Z","shell.execute_reply":"2021-05-30T05:51:05.516004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Id = \"2100032a-7c33-4bff-97ef-690822c43466\"\n# pstns = [tmp[-1]==Id for tmp in sentences]\n# positions_contracted = np.array(pstns)\n# positions_dilated = np.array([p for p in positions for i in range(MAX_LEN)])\n# print(np.sort(np.where(positions_dilated==1)[0]))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.518445Z","iopub.execute_input":"2021-05-30T05:51:05.518675Z","iopub.status.idle":"2021-05-30T05:51:05.526354Z","shell.execute_reply.started":"2021-05-30T05:51:05.518653Z","shell.execute_reply":"2021-05-30T05:51:05.525554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#np.array(sentences)[positions_contracted]","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.527603Z","iopub.execute_input":"2021-05-30T05:51:05.527962Z","iopub.status.idle":"2021-05-30T05:51:05.536002Z","shell.execute_reply.started":"2021-05-30T05:51:05.527929Z","shell.execute_reply":"2021-05-30T05:51:05.535215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tmp = pd.read_json(f'../input/coleridgeinitiative-show-us-the-data/test/{Id}.json')\n# text = \" \".join(list(tmp['text']))\n# text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n# text[:100] #string","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.537458Z","iopub.execute_input":"2021-05-30T05:51:05.53785Z","iopub.status.idle":"2021-05-30T05:51:05.544986Z","shell.execute_reply.started":"2021-05-30T05:51:05.537813Z","shell.execute_reply":"2021-05-30T05:51:05.544116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# text = np.array(text.split())\n# text","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.548069Z","iopub.execute_input":"2021-05-30T05:51:05.548378Z","iopub.status.idle":"2021-05-30T05:51:05.554322Z","shell.execute_reply.started":"2021-05-30T05:51:05.548343Z","shell.execute_reply":"2021-05-30T05:51:05.553535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# np.ceil(len(text)/MAX_LEN)*MAX_LEN","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.555665Z","iopub.execute_input":"2021-05-30T05:51:05.556054Z","iopub.status.idle":"2021-05-30T05:51:05.566287Z","shell.execute_reply.started":"2021-05-30T05:51:05.556014Z","shell.execute_reply":"2021-05-30T05:51:05.565426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#text = np.pad(text, (0, int(np.ceil(len(text)/MAX_LEN)*MAX_LEN)-len(text)), 'constant', constant_values=\"[PAD]\")","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.569288Z","iopub.execute_input":"2021-05-30T05:51:05.56958Z","iopub.status.idle":"2021-05-30T05:51:05.575015Z","shell.execute_reply.started":"2021-05-30T05:51:05.569555Z","shell.execute_reply":"2021-05-30T05:51:05.574106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#\"|\".join(text[np.array([0,1,2])]).strip(\"|\")","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.57633Z","iopub.execute_input":"2021-05-30T05:51:05.576766Z","iopub.status.idle":"2021-05-30T05:51:05.583286Z","shell.execute_reply.started":"2021-05-30T05:51:05.576713Z","shell.execute_reply":"2021-05-30T05:51:05.582437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Id = \"2f392438-e215-4169-bebf-21ac4ff253e1\"\n# positions = [tmp[-1]==Id for tmp in sentences]\n# positions = np.array([p for p in positions for i in range(MAX_LEN)])\n# print(np.sort(np.where(positions==1)[0]))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.584127Z","iopub.execute_input":"2021-05-30T05:51:05.584399Z","iopub.status.idle":"2021-05-30T05:51:05.592548Z","shell.execute_reply.started":"2021-05-30T05:51:05.584375Z","shell.execute_reply":"2021-05-30T05:51:05.591641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Id = \"3f316b38-1a24-45a9-8d8c-4e05a42257c6\"\n# positions = [tmp[-1]==Id for tmp in sentences]\n# positions = np.array([p for p in positions for i in range(MAX_LEN)])\n# print(np.sort(np.where(positions==1)[0]))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.593639Z","iopub.execute_input":"2021-05-30T05:51:05.593952Z","iopub.status.idle":"2021-05-30T05:51:05.602036Z","shell.execute_reply.started":"2021-05-30T05:51:05.593927Z","shell.execute_reply":"2021-05-30T05:51:05.600151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Id = \"8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60\"\n# positions = [tmp[-1]==Id for tmp in sentences]\n# positions = np.array([p for p in positions for i in range(MAX_LEN)])\n# print(np.sort(np.where(positions==1)[0]))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.604496Z","iopub.execute_input":"2021-05-30T05:51:05.605162Z","iopub.status.idle":"2021-05-30T05:51:05.610503Z","shell.execute_reply.started":"2021-05-30T05:51:05.605124Z","shell.execute_reply":"2021-05-30T05:51:05.60971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#np.array(pred_tags)[positions]","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.611943Z","iopub.execute_input":"2021-05-30T05:51:05.612304Z","iopub.status.idle":"2021-05-30T05:51:05.619159Z","shell.execute_reply.started":"2021-05-30T05:51:05.612269Z","shell.execute_reply":"2021-05-30T05:51:05.618337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#np.array(pred_tags)[np.array([0,1,2])]","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.620515Z","iopub.execute_input":"2021-05-30T05:51:05.620932Z","iopub.status.idle":"2021-05-30T05:51:05.627967Z","shell.execute_reply.started":"2021-05-30T05:51:05.620896Z","shell.execute_reply":"2021-05-30T05:51:05.627033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_submission_file(df, sentences, pred_tags, finding_tag=\"o-dataset\", verbose=False):\n    \"\"\"\n    Args:\n        df: pd.DataFrame\n        sentences: List[tuple[np.array, ..., str]]\n        pred_tagas: List[str]\n    Returns:\n        df: pd.DataFrame\n    \"\"\"\n    #df[\"PredictionString\"] = \"\"\n    for i,row in tqdm(df.iterrows()):\n        Id = row[\"Id\"]\n        pstns = [tmp[-1]==Id for tmp in sentences]\n        #positions_contracted = np.array(pstns)\n        positions_dilated = np.array([p for p in pstns for i in range(MAX_LEN)])\n        \n        pred_tags_per_pub = np.array(pred_tags)[positions_dilated]\n        \n        tmp = pd.read_json(f'../input/coleridgeinitiative-show-us-the-data/test/{Id}.json')\n        text = \" \".join(list(tmp['text']))\n        text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip() #clean_text\n        \n        text = np.array(text.split()) # to numpy array\n        \n        if TEXT_LEN > 0:\n            text = text[:TEXT_LEN]\n        \n        desired_len = int(np.ceil(len(text)/MAX_LEN)*MAX_LEN) # padding\n        text = np.pad(text, (0, desired_len-len(text)), 'constant', constant_values=\"[PAD]\")\n        \n        if verbose:\n            print(\"len(pred_tags_per_pub): \", len(pred_tags_per_pub))\n            print(\"len(text): \", len(text))\n        \n        assert len(pred_tags_per_pub) == len(text)\n        cond = np.where(pred_tags_per_pub==finding_tag)[0] # extract finding tokens\n        pred_string = \"|\".join(text[cond]).strip(\"|\")\n        #pred_string = \" \".join(np.where(pred_tags_per_pub==finding_tag, text, \"|\")).strip(\"|\")\n        \n        if verbose:\n            print(\"row#: \", i)\n            print(\"pred_string:\\n\", pred_string, type(pred_string))\n        \n        df.loc[i, \"PredictionString\"] = pred_string\n        #df.at[i, \"PredictionString\"] = pred_string\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.630957Z","iopub.execute_input":"2021-05-30T05:51:05.631214Z","iopub.status.idle":"2021-05-30T05:51:05.642999Z","shell.execute_reply.started":"2021-05-30T05:51:05.631189Z","shell.execute_reply":"2021-05-30T05:51:05.642115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/coleridgeinitiative-show-us-the-data/sample_submission.csv\")\ndf = make_submission_file(df, sentences, pred_tags, finding_tag=\"o-dataset\", verbose=False)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.645482Z","iopub.execute_input":"2021-05-30T05:51:05.645871Z","iopub.status.idle":"2021-05-30T05:51:05.910454Z","shell.execute_reply.started":"2021-05-30T05:51:05.645834Z","shell.execute_reply":"2021-05-30T05:51:05.909597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.91183Z","iopub.execute_input":"2021-05-30T05:51:05.912188Z","iopub.status.idle":"2021-05-30T05:51:05.920674Z","shell.execute_reply.started":"2021-05-30T05:51:05.912152Z","shell.execute_reply":"2021-05-30T05:51:05.919742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Id = \"8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60\"\n# tmp = pd.read_json(f'../input/coleridgeinitiative-show-us-the-data/test/{Id}.json')\n# text = \" \".join(list(tmp['text']))\n# text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n# text[:100] #string","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.921928Z","iopub.execute_input":"2021-05-30T05:51:05.922345Z","iopub.status.idle":"2021-05-30T05:51:05.926991Z","shell.execute_reply.started":"2021-05-30T05:51:05.922305Z","shell.execute_reply":"2021-05-30T05:51:05.925938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def extractor(df, pred_tags, max_len):\n#     \"\"\"\n#     Args:\n#         df: pd.DataFrame\n#         pred_tags: List of Str\n#         max_len: Int\n#     Returns:\n#         df: pd.DataFrame\n#     \"\"\"\n#     df[\"PredictionString\"] = \"\"\n#     total_length = 0\n#     for i, row in df.iterrows():\n#         wordlist = row[\"text\"].split(\" \")\n#         units = len(wordlist) // max_len\n#         resid = len(wordlist) - (units*max_len)\n        \n#         first = wordlist[:units*max_len]\n#         if resid > 0:\n#             second = wordlist[-resid:]\n#             second = np.pad(second, (0, max_len-len(second)), 'constant', constant_values=\"[PAD]\")\n#             first = np.append(first, second)\n#         first = np.array(first)\n#         length = len(first)\n#         sub_pred_tags = np.array(pred_tags[total_length:total_length+length])\n        \n#         datalist = first[np.where(sub_pred_tags=='o-dataset')[0]]\n#         datastring = \"|\".join(datalist).strip(\"|\")\n#         df.at[i, \"PredictionString\"] = datastring\n#         total_length += length\n#     return df","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.928585Z","iopub.execute_input":"2021-05-30T05:51:05.929125Z","iopub.status.idle":"2021-05-30T05:51:05.934938Z","shell.execute_reply.started":"2021-05-30T05:51:05.929072Z","shell.execute_reply":"2021-05-30T05:51:05.933969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = extractor(df_test, pred_tags, MAX_LEN)\n# df","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.936643Z","iopub.execute_input":"2021-05-30T05:51:05.937162Z","iopub.status.idle":"2021-05-30T05:51:05.942976Z","shell.execute_reply.started":"2021-05-30T05:51:05.937125Z","shell.execute_reply":"2021-05-30T05:51:05.94199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del df_test, pred_tags\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.944875Z","iopub.execute_input":"2021-05-30T05:51:05.945293Z","iopub.status.idle":"2021-05-30T05:51:05.950947Z","shell.execute_reply.started":"2021-05-30T05:51:05.94526Z","shell.execute_reply":"2021-05-30T05:51:05.949877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T05:51:05.95228Z","iopub.execute_input":"2021-05-30T05:51:05.952638Z","iopub.status.idle":"2021-05-30T05:51:05.959822Z","shell.execute_reply.started":"2021-05-30T05:51:05.952597Z","shell.execute_reply":"2021-05-30T05:51:05.958909Z"},"trusted":true},"execution_count":null,"outputs":[]}]}