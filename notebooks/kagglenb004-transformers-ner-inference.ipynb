{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"use_tpu = False\n\nif use_tpu:\n    ##### PyTorch-XLA installation via internet connection\n\n    ##!cp -r ../input/localnb001-transformers-ner/* .\n    #!cp -r ../input/d/riow1983/localnb001-transformers-ner/* .\n    #!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev\n\n\n\n\n\n    ##### PyTorch-XLA installation via source code (offline installation)\n\n    # Credit to https://www.kaggle.com/joshi98kishan/foldtraining-pytorch-tpu-8-cores/data?scriptVersionId=48061653\n    #Copying all the required wheel files from the created kaggle dataset to the working dir.\n    !cp ../input/pytorch-xla-setup-script/torch-nightly-cp37-cp37m-linux_x86_64.whl ./torch-nightly-cp37-cp37m-linux_x86_64.whl\n    !cp ../input/pytorch-xla-setup-script/torch_xla-nightly-cp37-cp37m-linux_x86_64.whl ./torch_xla-nightly-cp37-cp37m-linux_x86_64.whl\n    !cp ../input/pytorch-xla-setup-script/torchvision-nightly-cp37-cp37m-linux_x86_64.whl ./torchvision-nightly-cp37-cp37m-linux_x86_64.whl\n\n    # This deb files are the dependencies, copying them to the working dir.\n    !cp ../input/pytorch-xla-setup-script/libgfortran4_7.5.0-3ubuntu1_18.04_amd64.deb ./libgfortran4_7.5.0-3ubuntu1_18.04_amd64.deb\n    !cp ../input/pytorch-xla-setup-script/libomp5_5.0.1-1_amd64.deb ./libomp5_5.0.1-1_amd64.deb\n    !cp ../input/pytorch-xla-setup-script/libopenblas-base_0.2.20ds-4_amd64.deb ./libopenblas-base_0.2.20ds-4_amd64.deb\n    !cp ../input/pytorch-xla-setup-script/libopenblas-dev_0.2.20ds-4_amd64.deb ./libopenblas-dev_0.2.20ds-4_amd64.deb\n\n    #installing pytorch-xla by running this script\n    !python ../input/pytorch-xla-setup-script/pytorch-xla-env-setup.py --version nightly\n\n    #Now, istalling depedencies\n    !dpkg -i ./libgfortran4_7.5.0-3ubuntu1_18.04_amd64.deb\n    !dpkg -i ./libomp5_5.0.1-1_amd64.deb\n    !dpkg -i ./libopenblas-base_0.2.20ds-4_amd64.deb\n    !dpkg -i ./libopenblas-dev_0.2.20ds-4_amd64.deb\n\n    # Removing wheel and deb files, as we don't need them now.\n    !rm torch-nightly-cp37-cp37m-linux_x86_64.whl \n    !rm torch_xla-nightly-cp37-cp37m-linux_x86_64.whl \n    !rm torchvision-nightly-cp37-cp37m-linux_x86_64.whl\n    !rm libgfortran4_7.5.0-3ubuntu1_18.04_amd64.deb \n    !rm libomp5_5.0.1-1_amd64.deb \n    !rm libopenblas-base_0.2.20ds-4_amd64.deb \n    !rm libopenblas-dev_0.2.20ds-4_amd64.deb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing pytorch and the library for TPU execution\n\nimport torch\nif use_tpu:\n    import torch_xla\n    import torch_xla.core.xla_model as xm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertForTokenClassification, BertTokenizer, BertConfig, BertModel\n\nif use_tpu:\n    # Preparing for TPU usage\n    dev = xm.xla_device()\nelse:\n    #dev = torch.device('cuda:0')\n    dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(dev)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining some key variables that will be used later on in the training\ntest = True\nuse_pos = False\nCV = 1\nMAX_LEN = 290\nBATCH_SIZE = 48 #16\ntokenizer = BertTokenizer.from_pretrained('../input/localnb001-transformers-ner/bert-base-cased-vocab.txt')\n#tokenizer = BertTokenizer.from_pretrained('bert-base-cased-vocab.txt')\n#tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n#tokenizer = BertTokenizer.from_pretrained(f'../input/localnb001-transformers-ner/bert-base-cased-ner-cv{CV}.pt')\n#tokenizer = BertTokenizer.from_pretrained(f'../input/localnb001-transformers-ner')\n#tokenizer = BertTokenizer.from_pretrained('../input/d/riow1983/localnb001-transformers-ner/bert-base-cased')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport gc\nimport re\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nif use_pos:\n    import spacy\n    nlp = spacy.load(\"en_core_web_sm\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"markdown","source":"Credit to: https://www.kaggle.com/c/coleridgeinitiative-show-us-the-data/discussion/230341","metadata":{}},{"cell_type":"code","source":"def get_text(filename, test=test):\n    if test:\n        df = pd.read_json('../input/coleridgeinitiative-show-us-the-data/test/{}.json'.format(filename))\n    else:\n        df = pd.read_json('../input/coleridgeinitiative-show-us-the-data/train/{}.json'.format(filename))\n    text = \" \".join(list(df['text']))\n    return text\n\n\ndef clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n\n\n\n\ndef convert_tokens(x, m, max_len, train=False, verbose=False):\n    \"\"\"\n    Args:\n        x: df row\n        m: row index\n    Returns:\n        df\n    \n    ex) convert_tokens(row,i, MAX_LEN)\n    \"\"\"\n    df = pd.DataFrame()\n    if use_pos:\n        text = x[\"tok\"]\n        pos = x[\"pos\"]\n    else:\n        text = x['text'].replace('\\uf0b7','').split()\n\n    if train:\n        entity = x['dataset_label']\n\n        ## main\n        tokens=[]\n        k=0\n        for i,x in enumerate(text):\n\n            if k==0:\n                if x==entity.split()[0]:\n                    entity_len = len(entity.split())\n                    if entity == ' '.join(text[i:i+entity_len]):\n                        tokens.extend(['o-dataset']*len(entity.split()))\n                        k = entity_len\n                    else:\n                        tokens.append('o')\n                else:\n                    tokens.append('o')\n\n\n            k = max(0,k-1)\n    \n\n    k=0\n    sentence_hash=[]\n    for i in range(0,len(text),max_len):\n        if verbose:\n            print(\"Is length of text[i:i+max_len] 290?\", len(text[i:i+max_len]))\n        sentence_hash.extend([f'sentence#{k}']* len(text[i:i+max_len]))\n        k+=1\n\n    #df['word']=list(map(str,text))\n    df['word'] = text\n    if use_pos:\n        df['pos'] = pos\n    else:\n        df['pos'] = None\n    df['sentence'] = f'sentence{m}'\n    df['sentence#'] = sentence_hash\n    if train:\n        df['tag'] = tokens\n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## read data\nif test:\n    df_test = pd.read_csv(\"../input/coleridgeinitiative-show-us-the-data/sample_submission.csv\")\nelse:\n    df_test = pd.read_csv(\"../input/coleridgeinitiative-show-us-the-data/train.csv\")\ndf_test['text'] = df_test['Id'].apply(get_text)\n\n## clean text\ndf_test[\"text\"] = df_test[\"text\"].apply(lambda x: clean_text(x))\n\ndf_test","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for d in df_test[\"text\"][:3]:\n#     print(type(d))\n#     #print(d.split(\" \"))\n#     print(len(d.split(\" \")))\n#     print(len(d))\n#     print()\n#     print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### RIOW\n# ## POS tagging\n# if use_pos:\n#     tok, pos = [], []\n#     bar = tqdm(total = df_test.shape[0])\n#     for doc in nlp.pipe(df_test['text'].values, batch_size=50, n_process=-1):\n#         if doc.is_parsed:\n#             tok.append([n.text for n in doc])\n#             pos.append([n.pos_ for n in doc])\n#         else:\n#             # We want to make sure that the lists of parsed results have the\n#             # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n#             tok.append(None)\n#             pos.append(None)\n#         bar.update(1)\n#     df_test[\"tok\"] = tok\n#     df_test[\"pos\"] = pos\n# else:\n#     df_test[\"tok\"] = None\n#     df_test[\"pos\"] = None\n\n\n# ## process\n# dataset = pd.DataFrame()\n# bar = tqdm(total = df_test.shape[0])\n# for i,row in tqdm(df_test.iterrows()):\n#     df = convert_tokens(row,i, MAX_LEN)\n#     dataset = dataset.append(df,ignore_index=True)\n#     bar.update(1)\n    \n# dataset\n#### RIOWRIOW","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### RIOW\n# #df_test\n# del df_test, df\n# gc.collect()\n#### RIOWRIOW","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### RIOW\n# dataset[\"sentence_idx\"] = dataset[\"sentence\"] + dataset[\"sentence#\"]\n\n# sentence_vals = list(set(dataset[\"sentence_idx\"].values))\n# sentence2idx = {v: i for i, v in enumerate(sentence_vals)}\n# del sentence_vals\n# gc.collect()\n\n# dataset[\"sentence_idx\"] = dataset[\"sentence_idx\"].apply(lambda x: sentence2idx[x])\n# del sentence2idx\n# gc.collect()\n\n# dataset = dataset[[\"sentence\", \"sentence_idx\", \"token\", \"pos\"]].copy()\n# dataset.rename(columns={\"token\":\"word\"}, inplace=True)\n# dataset\n#### RIOWRIOW","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#np.sort(dataset[\"sentence_idx\"].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#num_labels = dataset[\"tag\"].nunique() + 1\nnum_labels = 3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a class to pull the words from the columns and create them into sentences\n\nclass SentenceGetter(object):\n    \n    def __init__(self, dataset):\n        self.n_sent = 1\n        self.dataset = dataset\n        self.empty = False\n        if use_pos:\n            agg_func = lambda s: [(w,p) for w,p in zip(s[\"word\"].values.tolist(),\n                                                       s[\"pos\"].values.tolist())]\n        else:\n            agg_func = lambda s: [(w,) for w in s[\"word\"].values.tolist()]\n        self.grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n    \n    def get_next(self):\n        try:\n            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n            self.n_sent += 1\n            return s\n        except:\n            return None\n\n#### RIOW\n#getter = SentenceGetter(dataset)\n#### RIOWRIOW","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating new lists and dicts that will be used at a later stage for reference and processing\n\n#### RIOW\n# sentences = [' '.join([s[0] for s in sent]) for sent in getter.sentences]\n# if use_pos:\n#     poses = [' '.join([s[1] for s in sent]) for sent in getter.sentences]\n# else:\n#     poses = None\n#### RIOWRIOW","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sentences[0]\n#poses[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- [x] ToDo: [localnb001-transformers-ner.ipynb](https://github.com/riow1983/Kaggle-Coleridge-Initiative/blob/main/notebooks/localnb001-transformers-ner.ipynb)を参考にDataloader, pred関数を実装する \n- [x] ToDo: finalテーブルをベースに, pred entityを加えたsubmission.csvを作成する処理を実装する","metadata":{}},{"cell_type":"markdown","source":"# Dataloader","metadata":{}},{"cell_type":"code","source":"# example: https://huggingface.co/transformers/model_doc/bert.html\n'''\nfrom transformers import BertTokenizer, BertModel\nimport torch\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    #### RIOW\n#     def __init__(self, tokenizer, sentences, poses, max_len):\n#         self.len = len(sentences)\n#         self.sentences = sentences\n#         self.poses = poses\n#         self.tokenizer = tokenizer\n#         self.max_len = max_len\n    def __init__(self, tokenizer, df, max_len):\n        self.df = df\n        #self.lengths = [len(d) for d in self.df[\"text\"]]\n        self.lengths = [np.ceil(len(d.split(\" \"))/MAX_LEN).astype(int) for d in self.df[\"text\"]]\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    #### RIOWRIOW\n    \n    def __len__(self):\n        #### RIOW\n        #return self.len\n        return sum(self.lengths)\n        #return sum([np.ceil(l/MAX_LEN).astype(int) for l in self.lengths])\n        #### RIOWRIOW\n        \n    def index_converter(self, text_lengths, original_index, verbose=False):\n        # reference:\n        # [1] https://gist.github.com/Miladiouss/6ba0876f0e2b65d0178be7274f61ad2f\n        accum = np.add.accumulate(text_lengths)\n        sentence_index = len(np.argwhere(accum <= original_index))\n        if verbose:\n            print(f\"*********** original_index: {original_index} *************\")\n            print(\"sentence_index:\", sentence_index)\n        #index_wrt_sentence = (original_index - np.insert(accum, 0, 0)[sentence_index]) // MAX_LEN\n        #index_wrt_sentence = np.floor((original_index - np.insert(accum, 0, 0)[sentence_index]) / MAX_LEN).astype(int)\n        index_wrt_sentence = original_index - np.insert(accum, 0, 0)[sentence_index]\n        if verbose:\n            print(\"index_wrt_sentence:\", index_wrt_sentence)\n            print()\n        return sentence_index, index_wrt_sentence\n        \n    def __getitem__(self, index):\n        #### RIOW\n#         sentence = str(self.sentences[index])\n#         if use_pos:\n#             pos = str(self.poses[index])\n#         else:\n#             pos = None\n        \n        #df = self.df[index]\n        #sentence_index, index_wrt_sentence = self.index_converter(self.lengths, index*MAX_LEN)\n        sentence_index, index_wrt_sentence = self.index_converter(self.lengths, index, verbose=False)\n        #df = self.df[converted_index]\n        df = self.df[self.df.index==sentence_index]\n        # POS tagging\n        if use_pos:\n            tok, pos = [], []\n            #bar = tqdm(total = df.shape[0])\n            for doc in nlp.pipe(df['text'].values, batch_size=50, n_process=-1):\n                if doc.is_parsed:\n                    tok.append([n.text for n in doc])\n                    pos.append([n.pos_ for n in doc])\n                else:\n                    # We want to make sure that the lists of parsed results have the\n                    # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n                    tok.append(None)\n                    pos.append(None)\n                #bar.update(1)\n            df[\"tok\"] = tok\n            df[\"pos\"] = pos\n        else:\n            df[\"tok\"] = None\n            df[\"pos\"] = None\n        \n        # process\n        dataset = pd.DataFrame()\n        #bar = tqdm(total = df.shape[0])\n        for i,row in df.iterrows():\n            _df = convert_tokens(row,i, MAX_LEN, verbose=False)\n            dataset = dataset.append(_df,ignore_index=True)\n            #bar.update(1)\n            \n        dataset[\"sentence_idx\"] = dataset[\"sentence\"] + dataset[\"sentence#\"]\n        #dataset = dataset[[\"sentence\", \"sentence_idx\", \"word\", \"pos\"]].copy()\n        #dataset.rename(columns={\"token\":\"word\"}, inplace=True)\n        \n        getter = SentenceGetter(dataset)\n        \n        #print(\"getter.sentences:\", getter.sentences)\n        #for sent in getter.sentences:\n        #    print(\"lenght of sent:\", len(sent))\n            \n        sentences = [' '.join([s[0] for s in sent]) for sent in getter.sentences]\n        if use_pos:\n            poses = [' '.join([s[1] for s in sent]) for sent in getter.sentences]\n        else:\n            poses = None\n            \n        #print(\"len(sentences):\", len(sentences))\n        #for sentence in sentences:\n        #    print(\"len(sentence.split(\" \")):\", len(sentence.split(\" \")))\n        #print(\"sentences:\", sentences)\n        #print(\"index_wrt_sentence:\", index_wrt_sentence)\n        \n        sentence = str(sentences[index_wrt_sentence])\n        if use_pos:\n            pos = str(poses[index_wrt_sentence])\n        else:\n            pos = None\n        #### RIOWRIOW\n        \n    \n        inputs = self.tokenizer.encode_plus(\n            sentence,\n            #### RIOW\n            #None,\n            pos,\n            #### RIOWRIOW\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long)\n        } ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### RIOW\n#testing_set = CustomDataset(tokenizer, sentences, poses, MAX_LEN)\ntesting_set = CustomDataset(tokenizer, df_test, MAX_LEN)\n#### RIOWRIOW\n\n# cf. \n'''Warning:\nTruncation was not explicitly activated but `max_length` is provided a specific value, \nplease use `truncation=True` to explicitly truncate examples to max length. \n\n... or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_params = {'batch_size': BATCH_SIZE,\n               'shuffle': False,\n               'num_workers': 8 #0\n                }\n\ntesting_loader = DataLoader(testing_set, **test_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the fine-tuned pre-trained BERT model","metadata":{}},{"cell_type":"code","source":"# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n\nclass BERTClass(torch.nn.Module):\n    def __init__(self):\n        super(BERTClass, self).__init__()\n        #self.l1 = transformers.BertForTokenClassification.from_pretrained('../input/localnb001-transformers-ner/bert-base-cased')\n        #self.l1 = transformers.BertForTokenClassification.from_pretrained(f'../input/localnb001-transformers-ner/bert-base-cased-ner-cv{CV}.pth', num_labels=num_labels)\n        #self.l1 = transformers.BertForTokenClassification.from_pretrained(f'../input/localnb001-transformers-ner/bert-base-cased-ner-cv{CV}.bin')\n        #self.l1 = transformers.BertForTokenClassification.from_pretrained('../input/d/riow1983/localnb001-transformers-ner', num_labels=num_labels)\n        self.l1 = transformers.BertForTokenClassification.from_pretrained('../input/localnb001-transformers-ner', num_labels=num_labels)\n        #self.l1 = transformers.BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=num_labels) # This requires internet connection.\n        \n        # self.l2 = torch.nn.Dropout(0.3)\n        # self.l3 = torch.nn.Linear(768, 200)\n    \n    def forward(self, ids, mask, labels):\n        output_1= self.l1(ids, mask, labels = labels)\n        # output_2 = self.l2(output_1[0])\n        # output = self.l3(output_2)\n        return output_1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BERTClass()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print model's state_dict\nprint(\"Model's state_dict:\")\nfor param_tensor in model.state_dict():\n    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(use_tpu)\nprint(dev)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save on CPU, Load on GPU (Example)\n```\n# Save\ntorch.save(net.state_dict(), PATH)\n\n# Load\ndevice = torch.device(\"cuda\")\nmodel = Net()\n# Choose whatever GPU device number you want\nmodel.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))\n# Make sure to call input = input.to(device) on any input tensors that you feed to the model\nmodel.to(device)\n```\nrefenrece: https://pytorch.org/tutorials/recipes/recipes/save_load_across_devices.html","metadata":{}},{"cell_type":"code","source":"#output_model = f\"../input/localnb001-transformers-ner/bert-base-cased-ner-pad-cv{CV}.pth\"\nif use_pos:\n    output_model = f\"../input/localnb001-transformers-ner/bert-base-cased-ner-pad-cv{CV}-epochs5.pth\"\nelse:\n    output_model = f\"../input/localnb001-transformers-ner/bert-base-cased-ner-pad-nopos-cv{CV}-epochs5.pth\"\n\nif use_tpu:\n    checkpoint = torch.load(output_model, map_location='tpu')\nelse:\n    #checkpoint = torch.load(output_model, map_location=torch.device('cpu'))\n    checkpoint = torch.load(output_model, map_location=\"cuda:0\")\n\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.to(dev)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"~~ToDo: fine-tuned modelの保存ファイルの拡張子は.pthではなく.binにする~~  \n~~reference: https://github.com/huggingface/transformers/issues/1620~~","metadata":{}},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"#tags_vals = ['o-dataset', 'o']\ntags_vals = ['o', 'o-dataset', 'pad'] # the order is the same as localnb001","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pred(model, testing_loader, verbose=False):\n    model.eval()\n    \n    predictions = []\n    with torch.no_grad():\n        for _, data in enumerate(testing_loader, 0):\n            ids = data['ids'].to(dev, dtype = torch.long)\n            print(_, ids.shape)\n            mask = data['mask'].to(dev, dtype = torch.long)\n\n            #output = model(ids, mask, mask)\n            output = model(ids, mask, labels=None)\n            #loss, logits = output[:2]\n            logits = output[0]\n            logits = logits.detach().cpu().numpy()\n            \n            predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n            if verbose:\n                #print(\"predictions:\", predictions)\n                print(\"length of predictions:\", len(predictions))\n                for p in predictions:\n                    print(\"length of p:\", len(p))\n                print()\n                \n        pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n        if verbose:\n            print(\"pred_tags:\", pred_tags)\n            print()\n        \n    return pred_tags","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To get the results on the validation set. This data is not seen by the model\n\npred_tags = pred(model, testing_loader, verbose=False)\n\n'''Warning:\nTruncation was not explicitly activated but `max_length` is provided a specific value, \nplease use `truncation=True` to explicitly truncate examples to max length. \nDefaulting to 'longest_first' truncation strategy. \n\nIf you encode pairs of sequences (GLUE-style) with the tokenizer \nyou can select this strategy more precisely by providing a specific strategy to `truncation`.\n\n\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: \nThe `pad_to_max_length` argument is deprecated and will be removed in a future version, \nuse `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, \nor use `padding='max_length'` to pad to a max length. \nIn this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or \nleave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(len(pred_tags), pred_tags[:10])\n# 175 * MAX_LEN(=290) = 50750","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"finalテーブルをベースに, pred entityを加えたsubmission.csvを作成する処理を実装する","metadata":{}},{"cell_type":"code","source":"len(pred_tags)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(pred_tags)\n# EPOCHS=1では全て'o'で予測してしまっていた\n# EPOCHS=5では逆に全て'o-dataset'で予測してしまっている","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extractor(df, pred_tags, max_len):\n    \"\"\"\n    Args:\n        df: pd.DataFrame\n        pred_tags: List of Str\n        max_len: Int\n    Returns:\n        df: pd.DataFrame\n    \"\"\"\n    df[\"PredictionString\"] = \"\"\n    total_length = 0\n    for i, row in df.iterrows():\n        wordlist = row[\"text\"].split(\" \")\n        units = len(wordlist) // max_len\n        resid = len(wordlist) - (units*max_len)\n        \n        first = wordlist[:units*max_len]\n        if resid > 0:\n            second = wordlist[-resid:]\n            second = np.pad(second, (0, max_len-len(second)), 'constant', constant_values=\"[PAD]\")\n            first = np.append(first, second)\n        first = np.array(first)\n        length = len(first)\n        sub_pred_tags = np.array(pred_tags[total_length:total_length+length])\n        \n        datalist = first[np.where(sub_pred_tags=='o-dataset')[0]]\n        datastring = \"|\".join(datalist).strip(\"|\")\n        df.at[i, \"PredictionString\"] = datastring\n        total_length += length\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = extractor(df_test, pred_tags, MAX_LEN)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df_test, pred_tags\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# res = dict()\n# for i, idx in enumerate(dataset[\"sentence_idx\"].unique()):\n#     seq = dataset[dataset[\"sentence_idx\"]==idx][\"word\"].values\n#     seq = np.pad(seq, (0, MAX_LEN-len(seq)), 'constant', constant_values=\"[PAD]\")\n#     res[idx] = (seq, pred_tags[i:i+MAX_LEN])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tmp = dataset.drop_duplicates(subset=\"sentence_idx\", keep='first').copy()\n# tmp.drop([\"word\", \"pos\"], axis=1, inplace=True)\n# tmp[\"pred\"]=\"\"\n# tmp.reset_index(drop=True, inplace=True)\n\n# del dataset\n# gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i, row in tmp.iterrows():\n#     #print(row[0])\n#     idx = int(row[1])\n#     #cond = np.where(np.array(res[idx][1])=='o-dataset')[0]\n#     cond = np.array(res[idx][1])=='o-dataset'\n#     #pred = \" \".join(list(res[idx][0][cond]))\n#     pred = \" \".join(list(np.where(cond, res[idx][0], \"|\"))).strip(\" | \")\n#     tmp.at[i, 'pred'] = pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# tmp -> sample_submission　の変換","metadata":{}},{"cell_type":"code","source":"#tmp[\"pred\"][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tmp.groupby(\"sentence\").apply(lambda r: \"|\".join(r[\"pred\"]).strip(\"|\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(\"text\", axis=1, inplace=True)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test = pd.read_csv(\"../input/coleridgeinitiative-show-us-the-data/sample_submission.csv\")\n# df_test[\"PredictionString\"] = tmp.groupby(\"sentence\").apply(lambda r: \"|\".join(r[\"pred\"]).strip(\"|\")).values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}