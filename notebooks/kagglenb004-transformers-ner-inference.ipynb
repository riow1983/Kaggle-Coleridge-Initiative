{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import yaml\nfrom pprint import pprint\nwith open('../input/localnb001-transformers-ner/config.yml') as file:\n    CFG = yaml.load(file, Loader=yaml.FullLoader)\npprint(CFG)\n\nEPOCHS = CFG[\"epochs\"]\nBATCH_SIZE = CFG[\"test_batch_size\"]\nTRAIN = False\nMAX_LEN = CFG[\"max_len\"]\nUSE_POS = CFG[\"use_pos\"]\nDEBUG = CFG[\"debug\"]\nTEXT_LEN = CFG[\"text_len\"]\nTAGS_VALS = CFG[\"tags_vals\"]\nUSE_TPU = False","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:52:39.708693Z","iopub.execute_input":"2021-06-05T14:52:39.709047Z","iopub.status.idle":"2021-06-05T14:52:39.72604Z","shell.execute_reply.started":"2021-06-05T14:52:39.709015Z","shell.execute_reply":"2021-06-05T14:52:39.725291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if USE_TPU:\n    ##### PyTorch-XLA installation via internet connection\n\n    ##!cp -r ../input/localnb001-transformers-ner/* .\n    #!cp -r ../input/d/riow1983/localnb001-transformers-ner/* .\n    #!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev\n\n\n\n\n\n    ##### PyTorch-XLA installation via source code (offline installation)\n\n    # Credit to https://www.kaggle.com/joshi98kishan/foldtraining-pytorch-tpu-8-cores/data?scriptVersionId=48061653\n    #Copying all the required wheel files from the created kaggle dataset to the working dir.\n    !cp ../input/pytorch-xla-setup-script/torch-nightly-cp37-cp37m-linux_x86_64.whl ./torch-nightly-cp37-cp37m-linux_x86_64.whl\n    !cp ../input/pytorch-xla-setup-script/torch_xla-nightly-cp37-cp37m-linux_x86_64.whl ./torch_xla-nightly-cp37-cp37m-linux_x86_64.whl\n    !cp ../input/pytorch-xla-setup-script/torchvision-nightly-cp37-cp37m-linux_x86_64.whl ./torchvision-nightly-cp37-cp37m-linux_x86_64.whl\n\n    # This deb files are the dependencies, copying them to the working dir.\n    !cp ../input/pytorch-xla-setup-script/libgfortran4_7.5.0-3ubuntu1_18.04_amd64.deb ./libgfortran4_7.5.0-3ubuntu1_18.04_amd64.deb\n    !cp ../input/pytorch-xla-setup-script/libomp5_5.0.1-1_amd64.deb ./libomp5_5.0.1-1_amd64.deb\n    !cp ../input/pytorch-xla-setup-script/libopenblas-base_0.2.20ds-4_amd64.deb ./libopenblas-base_0.2.20ds-4_amd64.deb\n    !cp ../input/pytorch-xla-setup-script/libopenblas-dev_0.2.20ds-4_amd64.deb ./libopenblas-dev_0.2.20ds-4_amd64.deb\n\n    #installing pytorch-xla by running this script\n    !python ../input/pytorch-xla-setup-script/pytorch-xla-env-setup.py --version nightly\n\n    #Now, istalling depedencies\n    !dpkg -i ./libgfortran4_7.5.0-3ubuntu1_18.04_amd64.deb\n    !dpkg -i ./libomp5_5.0.1-1_amd64.deb\n    !dpkg -i ./libopenblas-base_0.2.20ds-4_amd64.deb\n    !dpkg -i ./libopenblas-dev_0.2.20ds-4_amd64.deb\n\n    # Removing wheel and deb files, as we don't need them now.\n    !rm torch-nightly-cp37-cp37m-linux_x86_64.whl \n    !rm torch_xla-nightly-cp37-cp37m-linux_x86_64.whl \n    !rm torchvision-nightly-cp37-cp37m-linux_x86_64.whl\n    !rm libgfortran4_7.5.0-3ubuntu1_18.04_amd64.deb \n    !rm libomp5_5.0.1-1_amd64.deb \n    !rm libopenblas-base_0.2.20ds-4_amd64.deb \n    !rm libopenblas-dev_0.2.20ds-4_amd64.deb","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:52:39.727403Z","iopub.execute_input":"2021-06-05T14:52:39.727647Z","iopub.status.idle":"2021-06-05T14:52:39.814044Z","shell.execute_reply.started":"2021-06-05T14:52:39.727624Z","shell.execute_reply":"2021-06-05T14:52:39.81314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing pytorch and the library for TPU execution\n\nimport torch\nif USE_TPU:\n    import torch_xla\n    import torch_xla.core.xla_model as xm","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:52:39.816264Z","iopub.execute_input":"2021-06-05T14:52:39.816806Z","iopub.status.idle":"2021-06-05T14:52:40.913999Z","shell.execute_reply.started":"2021-06-05T14:52:39.816769Z","shell.execute_reply":"2021-06-05T14:52:40.91313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertForTokenClassification, BertTokenizer, BertConfig, BertModel\n\nif USE_TPU:\n    # Preparing for TPU usage\n    dev = xm.xla_device()\nelse:\n    #dev = torch.device('cuda:0')\n    dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(dev)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:52:40.917281Z","iopub.execute_input":"2021-06-05T14:52:40.91755Z","iopub.status.idle":"2021-06-05T14:52:41.286026Z","shell.execute_reply.started":"2021-06-05T14:52:40.917524Z","shell.execute_reply":"2021-06-05T14:52:41.28511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining some key variables that will be used later on in the training\n# test = False\n# use_pos = False\n# CV = 1\n# MAX_LEN = 290\n# BATCH_SIZE = 4 #16\ntokenizer = BertTokenizer.from_pretrained('../input/localnb001-transformers-ner/bert-base-uncased-vocab.txt')\n#tokenizer = BertTokenizer.from_pretrained('bert-base-cased-vocab.txt')\n#tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n#tokenizer = BertTokenizer.from_pretrained(f'../input/localnb001-transformers-ner/bert-base-cased-ner-cv{CV}.pt')\n#tokenizer = BertTokenizer.from_pretrained(f'../input/localnb001-transformers-ner')\n#tokenizer = BertTokenizer.from_pretrained('../input/d/riow1983/localnb001-transformers-ner/bert-base-cased')","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:52:41.287257Z","iopub.execute_input":"2021-06-05T14:52:41.287742Z","iopub.status.idle":"2021-06-05T14:52:41.336125Z","shell.execute_reply.started":"2021-06-05T14:52:41.287704Z","shell.execute_reply":"2021-06-05T14:52:41.335254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp ../input/localnb001-transformers-ner/bridge.py ./bridge.py\n#from bridge import *\n\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm.notebook import tqdm\nimport gc\nimport re\nimport pickle\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nif USE_POS:\n    import spacy\n    nlp = spacy.load(\"en_core_web_sm\")","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:52:41.33731Z","iopub.execute_input":"2021-06-05T14:52:41.337802Z","iopub.status.idle":"2021-06-05T14:52:41.992756Z","shell.execute_reply.started":"2021-06-05T14:52:41.337765Z","shell.execute_reply":"2021-06-05T14:52:41.991727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"markdown","source":"Credit to: https://www.kaggle.com/c/coleridgeinitiative-show-us-the-data/discussion/230341","metadata":{}},{"cell_type":"code","source":"!python ./bridge.py {TRAIN} {MAX_LEN} {USE_POS} {DEBUG} {TEXT_LEN} {TAGS_VALS}","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:52:41.996299Z","iopub.execute_input":"2021-06-05T14:52:41.996575Z","iopub.status.idle":"2021-06-05T14:52:48.026529Z","shell.execute_reply.started":"2021-06-05T14:52:41.996546Z","shell.execute_reply":"2021-06-05T14:52:48.025521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = pickle.load(open(\"./sentences.pkl\", 'rb'))\nprint(sentences[0])","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:52:48.032478Z","iopub.execute_input":"2021-06-05T14:52:48.034657Z","iopub.status.idle":"2021-06-05T14:52:48.057967Z","shell.execute_reply.started":"2021-06-05T14:52:48.03459Z","shell.execute_reply":"2021-06-05T14:52:48.057144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_labels = len(TAGS_VALS.split())","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:52:48.062265Z","iopub.execute_input":"2021-06-05T14:52:48.0642Z","iopub.status.idle":"2021-06-05T14:52:48.069813Z","shell.execute_reply.started":"2021-06-05T14:52:48.064164Z","shell.execute_reply":"2021-06-05T14:52:48.068843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n# Deprecated from here","metadata":{}},{"cell_type":"code","source":"# def get_text(filename, test=test):\n#     if test:\n#         df = pd.read_json('../input/coleridgeinitiative-show-us-the-data/test/{}.json'.format(filename))\n#     else:\n#         df = pd.read_json('../input/coleridgeinitiative-show-us-the-data/train/{}.json'.format(filename))\n#     text = \" \".join(list(df['text']))\n#     return text\n\n\n# def clean_text(txt):\n#     return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n\n\n\n\n# def convert_tokens(x, m, max_len, train=False, verbose=False):\n#     \"\"\"\n#     Args:\n#         x: df row\n#         m: row index\n#     Returns:\n#         df\n    \n#     ex) convert_tokens(row,i, MAX_LEN)\n#     \"\"\"\n#     df = pd.DataFrame()\n#     if use_pos:\n#         text = x[\"tok\"]\n#         pos = x[\"pos\"]\n#     else:\n#         text = x['text'].replace('\\uf0b7','').split()\n\n#     if train:\n#         entity = x['dataset_label']\n\n#         ## main\n#         tokens=[]\n#         k=0\n#         for i,x in enumerate(text):\n\n#             if k==0:\n#                 if x==entity.split()[0]:\n#                     entity_len = len(entity.split())\n#                     if entity == ' '.join(text[i:i+entity_len]):\n#                         tokens.extend(['o-dataset']*len(entity.split()))\n#                         k = entity_len\n#                     else:\n#                         tokens.append('o')\n#                 else:\n#                     tokens.append('o')\n\n\n#             k = max(0,k-1)\n    \n\n#     k=0\n#     sentence_hash=[]\n#     for i in range(0,len(text),max_len):\n#         if verbose:\n#             print(\"Is length of text[i:i+max_len] 290?\", len(text[i:i+max_len]))\n#         sentence_hash.extend([f'sentence#{k}']* len(text[i:i+max_len]))\n#         k+=1\n\n#     #df['word']=list(map(str,text))\n#     df['word'] = text\n#     if use_pos:\n#         df['pos'] = pos\n#     else:\n#         df['pos'] = None\n#     df['sentence'] = f'sentence{m}'\n#     df['sentence#'] = sentence_hash\n#     if train:\n#         df['tag'] = tokens\n\n#     return df","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:52:48.073961Z","iopub.execute_input":"2021-06-05T14:52:48.07622Z","iopub.status.idle":"2021-06-05T14:52:48.08289Z","shell.execute_reply.started":"2021-06-05T14:52:48.076185Z","shell.execute_reply":"2021-06-05T14:52:48.08215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ## read data\n# if test:\n#     df_test = pd.read_csv(\"../input/coleridgeinitiative-show-us-the-data/sample_submission.csv\")\n# else:\n#     df_test = pd.read_csv(\"../input/coleridgeinitiative-show-us-the-data/train.csv\")\n# df_test['text'] = df_test['Id'].apply(get_text)\n\n# ## clean text\n# df_test[\"text\"] = df_test[\"text\"].apply(lambda x: clean_text(x))\n\n# df_test","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-06-05T14:52:48.086893Z","iopub.execute_input":"2021-06-05T14:52:48.089175Z","iopub.status.idle":"2021-06-05T14:52:48.094575Z","shell.execute_reply.started":"2021-06-05T14:52:48.08914Z","shell.execute_reply":"2021-06-05T14:52:48.093553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #num_labels = dataset[\"tag\"].nunique() + 1\n# num_labels = 3","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:52:48.098936Z","iopub.execute_input":"2021-06-05T14:52:48.101016Z","iopub.status.idle":"2021-06-05T14:52:48.106226Z","shell.execute_reply.started":"2021-06-05T14:52:48.100953Z","shell.execute_reply":"2021-06-05T14:52:48.105303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Creating a class to pull the words from the columns and create them into sentences\n\n# class SentenceGetter(object):\n    \n#     def __init__(self, dataset):\n#         self.n_sent = 1\n#         self.dataset = dataset\n#         self.empty = False\n#         if use_pos:\n#             agg_func = lambda s: [(w,p) for w,p in zip(s[\"word\"].values.tolist(),\n#                                                        s[\"pos\"].values.tolist())]\n#         else:\n#             agg_func = lambda s: [(w,) for w in s[\"word\"].values.tolist()]\n#         self.grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n#         self.sentences = [s for s in self.grouped]\n    \n#     def get_next(self):\n#         try:\n#             s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n#             self.n_sent += 1\n#             return s\n#         except:\n#             return None\n\n# #### RIOW\n# #getter = SentenceGetter(dataset)\n# #### RIOWRIOW","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:52:48.108674Z","iopub.execute_input":"2021-06-05T14:52:48.109845Z","iopub.status.idle":"2021-06-05T14:52:48.116905Z","shell.execute_reply.started":"2021-06-05T14:52:48.109809Z","shell.execute_reply":"2021-06-05T14:52:48.116085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deprecated above\n***","metadata":{}},{"cell_type":"markdown","source":"- [x] ToDo: [localnb001-transformers-ner.ipynb](https://github.com/riow1983/Kaggle-Coleridge-Initiative/blob/main/notebooks/localnb001-transformers-ner.ipynb)を参考にDataloader, pred関数を実装する \n- [x] ToDo: finalテーブルをベースに, pred entityを加えたsubmission.csvを作成する処理を実装する","metadata":{}},{"cell_type":"markdown","source":"# Dataloader","metadata":{}},{"cell_type":"code","source":"# example: https://huggingface.co/transformers/model_doc/bert.html\n'''\nfrom transformers import BertTokenizer, BertModel\nimport torch\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\n'''","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:52:48.118746Z","iopub.execute_input":"2021-06-05T14:52:48.120177Z","iopub.status.idle":"2021-06-05T14:52:48.137896Z","shell.execute_reply.started":"2021-06-05T14:52:48.120142Z","shell.execute_reply":"2021-06-05T14:52:48.13707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### RIOW\n# class CustomDataset(Dataset):\n#     #### RIOW\n# #     def __init__(self, tokenizer, sentences, poses, max_len):\n# #         self.len = len(sentences)\n# #         self.sentences = sentences\n# #         self.poses = poses\n# #         self.tokenizer = tokenizer\n# #         self.max_len = max_len\n#     def __init__(self, tokenizer, df, max_len):\n#         self.df = df\n#         #self.lengths = [len(d) for d in self.df[\"text\"]]\n#         self.lengths = [np.ceil(len(d.split(\" \"))/MAX_LEN).astype(int) for d in self.df[\"text\"]]\n#         self.tokenizer = tokenizer\n#         self.max_len = max_len\n#     #### RIOWRIOW\n    \n#     def __len__(self):\n#         #### RIOW\n#         #return self.len\n#         return sum(self.lengths)\n#         #return sum([np.ceil(l/MAX_LEN).astype(int) for l in self.lengths])\n#         #### RIOWRIOW\n        \n#     def index_converter(self, text_lengths, original_index, verbose=False):\n#         # reference:\n#         # [1] https://gist.github.com/Miladiouss/6ba0876f0e2b65d0178be7274f61ad2f\n#         accum = np.add.accumulate(text_lengths)\n#         sentence_index = len(np.argwhere(accum <= original_index))\n#         if verbose:\n#             print(f\"*********** original_index: {original_index} *************\")\n#             print(\"sentence_index:\", sentence_index)\n#         #index_wrt_sentence = (original_index - np.insert(accum, 0, 0)[sentence_index]) // MAX_LEN\n#         #index_wrt_sentence = np.floor((original_index - np.insert(accum, 0, 0)[sentence_index]) / MAX_LEN).astype(int)\n#         index_wrt_sentence = original_index - np.insert(accum, 0, 0)[sentence_index]\n#         if verbose:\n#             print(\"index_wrt_sentence:\", index_wrt_sentence)\n#             print()\n#         return sentence_index, index_wrt_sentence\n        \n#     def __getitem__(self, index):\n#         #### RIOW\n# #         sentence = str(self.sentences[index])\n# #         if use_pos:\n# #             pos = str(self.poses[index])\n# #         else:\n# #             pos = None\n        \n#         #df = self.df[index]\n#         #sentence_index, index_wrt_sentence = self.index_converter(self.lengths, index*MAX_LEN)\n#         sentence_index, index_wrt_sentence = self.index_converter(self.lengths, index, verbose=False)\n#         #df = self.df[converted_index]\n#         df = self.df[self.df.index==sentence_index]\n#         # POS tagging\n#         if use_pos:\n#             tok, pos = [], []\n#             #bar = tqdm(total = df.shape[0])\n#             for doc in nlp.pipe(df['text'].values, batch_size=50, n_process=-1):\n#                 if doc.is_parsed:\n#                     tok.append([n.text for n in doc])\n#                     pos.append([n.pos_ for n in doc])\n#                 else:\n#                     # We want to make sure that the lists of parsed results have the\n#                     # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n#                     tok.append(None)\n#                     pos.append(None)\n#                 #bar.update(1)\n#             df[\"tok\"] = tok\n#             df[\"pos\"] = pos\n#         else:\n#             df[\"tok\"] = None\n#             df[\"pos\"] = None\n        \n#         # process\n#         dataset = pd.DataFrame()\n#         #bar = tqdm(total = df.shape[0])\n#         for i,row in df.iterrows():\n#             _df = convert_tokens(row,i, MAX_LEN, verbose=False)\n#             dataset = dataset.append(_df,ignore_index=True)\n#             #bar.update(1)\n            \n#         dataset[\"sentence_idx\"] = dataset[\"sentence\"] + dataset[\"sentence#\"]\n#         #dataset = dataset[[\"sentence\", \"sentence_idx\", \"word\", \"pos\"]].copy()\n#         #dataset.rename(columns={\"token\":\"word\"}, inplace=True)\n        \n#         getter = SentenceGetter(dataset)\n        \n#         #print(\"getter.sentences:\", getter.sentences)\n#         #for sent in getter.sentences:\n#         #    print(\"lenght of sent:\", len(sent))\n            \n#         sentences = [' '.join([s[0] for s in sent]) for sent in getter.sentences]\n#         if use_pos:\n#             poses = [' '.join([s[1] for s in sent]) for sent in getter.sentences]\n#         else:\n#             poses = None\n            \n#         #print(\"len(sentences):\", len(sentences))\n#         #for sentence in sentences:\n#         #    print(\"len(sentence.split(\" \")):\", len(sentence.split(\" \")))\n#         #print(\"sentences:\", sentences)\n#         #print(\"index_wrt_sentence:\", index_wrt_sentence)\n        \n#         sentence = str(sentences[index_wrt_sentence])\n#         if use_pos:\n#             pos = str(poses[index_wrt_sentence])\n#         else:\n#             pos = None\n#         #### RIOWRIOW\n        \n    \n#         inputs = self.tokenizer.encode_plus(\n#             sentence,\n#             #### RIOW\n#             #None,\n#             pos,\n#             #### RIOWRIOW\n#             add_special_tokens=True,\n#             max_length=self.max_len,\n#             pad_to_max_length=True,\n#             return_token_type_ids=True\n#         )\n#         ids = inputs['input_ids']\n#         mask = inputs['attention_mask']\n\n#         return {\n#             'ids': torch.tensor(ids, dtype=torch.long),\n#             'mask': torch.tensor(mask, dtype=torch.long)\n#         } \n    \n    \nclass CustomDataset(Dataset):\n    def __init__(self, sentences, tokenizer, max_len):\n        self.len = len(sentences)\n        self.sentences = sentences\n        #self.poses = poses\n        #self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __getitem__(self, index):\n        tmp = self.sentences[index]\n        sentence = \" \".join(tmp[0])\n        \n        if USE_POS:\n            #pos = str(self.poses[index])\n            pos = \" \".join(tmp[1])\n        else:\n            #pos = self.poses # which is None\n            pos = tmp[1]\n        \n        inputs = self.tokenizer.encode_plus(\n            sentence,\n            pos,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            truncation=True,\n            pad_to_max_length=True, # future warning (deprecated)\n            #padding=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n\n        if TRAIN:\n            #label = self.labels[index]\n            label = list(tmp[2])\n            label.extend([2]*self.max_len) # tag2idx = {'o':0, 'o-dataset':1, 'pad':2}\n            label = label[:self.max_len]\n        \n            return {'ids': torch.tensor(ids, dtype=torch.long),\n                    'mask': torch.tensor(mask, dtype=torch.long),\n                    'tags': torch.tensor(label, dtype=torch.long)} \n        else:\n            return {'ids': torch.tensor(ids, dtype=torch.long),\n                    'mask': torch.tensor(mask, dtype=torch.long)} \n    \n    def __len__(self):\n        return self.len\n    \n    \n    \n#### RIOWRIOW","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:52:48.140542Z","iopub.execute_input":"2021-06-05T14:52:48.141988Z","iopub.status.idle":"2021-06-05T14:52:48.162991Z","shell.execute_reply.started":"2021-06-05T14:52:48.141952Z","shell.execute_reply":"2021-06-05T14:52:48.162116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### RIOW\n#testing_set = CustomDataset(tokenizer, sentences, poses, MAX_LEN)\n#testing_set = CustomDataset(tokenizer, df_test, MAX_LEN)\ntesting_set = CustomDataset(sentences, tokenizer, MAX_LEN)\n#### RIOWRIOW\n\n# cf. \n'''Warning:\nTruncation was not explicitly activated but `max_length` is provided a specific value, \nplease use `truncation=True` to explicitly truncate examples to max length. \n\n... or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n'''","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:52:48.167563Z","iopub.execute_input":"2021-06-05T14:52:48.170117Z","iopub.status.idle":"2021-06-05T14:52:48.179513Z","shell.execute_reply.started":"2021-06-05T14:52:48.17008Z","shell.execute_reply":"2021-06-05T14:52:48.178654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_params = {'batch_size': BATCH_SIZE,\n               'shuffle': False,\n               'num_workers': 8 #0\n                }\n\ntesting_loader = DataLoader(testing_set, **test_params)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:52:48.183729Z","iopub.execute_input":"2021-06-05T14:52:48.185703Z","iopub.status.idle":"2021-06-05T14:52:48.191693Z","shell.execute_reply.started":"2021-06-05T14:52:48.185641Z","shell.execute_reply":"2021-06-05T14:52:48.190826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the fine-tuned pre-trained BERT model","metadata":{}},{"cell_type":"code","source":"# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n\nclass BERTClass(torch.nn.Module):\n    def __init__(self):\n        super(BERTClass, self).__init__()\n        #self.l1 = transformers.BertForTokenClassification.from_pretrained('../input/localnb001-transformers-ner/bert-base-cased')\n        #self.l1 = transformers.BertForTokenClassification.from_pretrained(f'../input/localnb001-transformers-ner/bert-base-cased-ner-cv{CV}.pth', num_labels=num_labels)\n        #self.l1 = transformers.BertForTokenClassification.from_pretrained(f'../input/localnb001-transformers-ner/bert-base-cased-ner-cv{CV}.bin')\n        #self.l1 = transformers.BertForTokenClassification.from_pretrained('../input/d/riow1983/localnb001-transformers-ner', num_labels=num_labels)\n        self.l1 = transformers.BertForTokenClassification.from_pretrained('../input/localnb001-transformers-ner', num_labels=num_labels)\n        #self.l1 = transformers.BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=num_labels) # This requires internet connection.\n        \n        # self.l2 = torch.nn.Dropout(0.3)\n        # self.l3 = torch.nn.Linear(768, 200)\n    \n    def forward(self, ids, mask, labels):\n        output_1= self.l1(ids, mask, labels = labels)\n        # output_2 = self.l2(output_1[0])\n        # output = self.l3(output_2)\n        return output_1","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:52:48.195985Z","iopub.execute_input":"2021-06-05T14:52:48.198322Z","iopub.status.idle":"2021-06-05T14:52:48.206386Z","shell.execute_reply.started":"2021-06-05T14:52:48.198264Z","shell.execute_reply":"2021-06-05T14:52:48.205545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BERTClass()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:52:48.210683Z","iopub.execute_input":"2021-06-05T14:52:48.21304Z","iopub.status.idle":"2021-06-05T14:52:56.843513Z","shell.execute_reply.started":"2021-06-05T14:52:48.213004Z","shell.execute_reply":"2021-06-05T14:52:56.842657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print model's state_dict\nprint(\"Model's state_dict:\")\nfor param_tensor in model.state_dict():\n    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:52:56.844798Z","iopub.execute_input":"2021-06-05T14:52:56.845171Z","iopub.status.idle":"2021-06-05T14:52:57.147233Z","shell.execute_reply.started":"2021-06-05T14:52:56.845134Z","shell.execute_reply":"2021-06-05T14:52:57.14647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(USE_TPU)\nprint(dev)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:52:57.149901Z","iopub.execute_input":"2021-06-05T14:52:57.150139Z","iopub.status.idle":"2021-06-05T14:52:57.156551Z","shell.execute_reply.started":"2021-06-05T14:52:57.150115Z","shell.execute_reply":"2021-06-05T14:52:57.155452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save on CPU, Load on GPU (Example)\n```\n# Save\ntorch.save(net.state_dict(), PATH)\n\n# Load\ndevice = torch.device(\"cuda\")\nmodel = Net()\n# Choose whatever GPU device number you want\nmodel.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))\n# Make sure to call input = input.to(device) on any input tensors that you feed to the model\nmodel.to(device)\n```\nrefenrece: https://pytorch.org/tutorials/recipes/recipes/save_load_across_devices.html","metadata":{}},{"cell_type":"code","source":"CV = 1\nif USE_POS:\n    output_model = f\"../input/localnb001-transformers-ner/bert-base-uncased-ner-pad-cv{CV}-epochs{EPOCHS}.pth\"\nelse:\n    output_model = f\"../input/localnb001-transformers-ner/bert-base-uncased-ner-pad-nopos-cv{CV}-epochs{EPOCHS}.pth\"\n\nif USE_TPU:\n    checkpoint = torch.load(output_model, map_location='tpu')\nelse:\n    #checkpoint = torch.load(output_model, map_location=torch.device('cpu'))\n    checkpoint = torch.load(output_model, map_location=\"cuda:0\")\n\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.to(dev)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:52:57.157828Z","iopub.execute_input":"2021-06-05T14:52:57.158168Z","iopub.status.idle":"2021-06-05T14:53:12.36779Z","shell.execute_reply.started":"2021-06-05T14:52:57.158134Z","shell.execute_reply":"2021-06-05T14:53:12.366958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"~~ToDo: fine-tuned modelの保存ファイルの拡張子は.pthではなく.binにする~~  \n~~reference: https://github.com/huggingface/transformers/issues/1620~~","metadata":{}},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"#tags_vals = ['o-dataset', 'o']\n#tags_vals = ['o', 'o-dataset', 'pad'] # the order is the same as localnb001\ntags_vals = TAGS_VALS.split()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:12.372167Z","iopub.execute_input":"2021-06-05T14:53:12.37242Z","iopub.status.idle":"2021-06-05T14:53:12.37569Z","shell.execute_reply.started":"2021-06-05T14:53:12.372395Z","shell.execute_reply":"2021-06-05T14:53:12.374732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pred(model, testing_loader, verbose=False):\n    model.eval()\n    \n    predictions = []\n    with torch.no_grad():\n        for _, data in enumerate(testing_loader, 0):\n            ids = data['ids'].to(dev, dtype = torch.long)\n            print(_, ids.shape)\n            mask = data['mask'].to(dev, dtype = torch.long)\n\n            #output = model(ids, mask, mask)\n            output = model(ids, mask, labels=None)\n            #loss, logits = output[:2]\n            logits = output[0]\n            logits = logits.detach().cpu().numpy()\n            \n            predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n            if verbose:\n                #print(\"predictions:\", predictions)\n                print(\"length of predictions:\", len(predictions))\n                for p in predictions:\n                    print(\"length of p:\", len(p))\n                print()\n                \n        pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n        if verbose:\n            print(\"pred_tags:\", pred_tags)\n            print()\n        \n    return pred_tags","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:12.377698Z","iopub.execute_input":"2021-06-05T14:53:12.378229Z","iopub.status.idle":"2021-06-05T14:53:12.388225Z","shell.execute_reply.started":"2021-06-05T14:53:12.378192Z","shell.execute_reply":"2021-06-05T14:53:12.387509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To get the results on the validation set. This data is not seen by the model\n\npred_tags = pred(model, testing_loader, verbose=False)\n\n'''Warning:\nTruncation was not explicitly activated but `max_length` is provided a specific value, \nplease use `truncation=True` to explicitly truncate examples to max length. \nDefaulting to 'longest_first' truncation strategy. \n\nIf you encode pairs of sequences (GLUE-style) with the tokenizer \nyou can select this strategy more precisely by providing a specific strategy to `truncation`.\n\n\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: \nThe `pad_to_max_length` argument is deprecated and will be removed in a future version, \nuse `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, \nor use `padding='max_length'` to pad to a max length. \nIn this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or \nleave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n'''","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:12.38947Z","iopub.execute_input":"2021-06-05T14:53:12.389972Z","iopub.status.idle":"2021-06-05T14:53:44.107344Z","shell.execute_reply.started":"2021-06-05T14:53:12.389935Z","shell.execute_reply":"2021-06-05T14:53:44.106326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred_tags 内容確認\nnp.unique(pred_tags)\n# EPOCHS=1では全て'o'で予測してしまっていた\n# EPOCHS=5では逆に全て'o-dataset'で予測してしまっている","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.111711Z","iopub.execute_input":"2021-06-05T14:53:44.112243Z","iopub.status.idle":"2021-06-05T14:53:44.128662Z","shell.execute_reply.started":"2021-06-05T14:53:44.112202Z","shell.execute_reply":"2021-06-05T14:53:44.127619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(len(pred_tags), pred_tags[:10])\n# 175 * MAX_LEN(=290) = 50750","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.132437Z","iopub.execute_input":"2021-06-05T14:53:44.134541Z","iopub.status.idle":"2021-06-05T14:53:44.139473Z","shell.execute_reply.started":"2021-06-05T14:53:44.134505Z","shell.execute_reply":"2021-06-05T14:53:44.138688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"finalテーブルをベースに, pred entityを加えたsubmission.csvを作成する処理を実装する","metadata":{}},{"cell_type":"code","source":"# Id = \"2100032a-7c33-4bff-97ef-690822c43466\"\n# pstns = [tmp[-1]==Id for tmp in sentences]\n# positions_contracted = np.array(pstns)\n# positions_contracted","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.143895Z","iopub.execute_input":"2021-06-05T14:53:44.146483Z","iopub.status.idle":"2021-06-05T14:53:44.151027Z","shell.execute_reply.started":"2021-06-05T14:53:44.146448Z","shell.execute_reply":"2021-06-05T14:53:44.150218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Id = \"2100032a-7c33-4bff-97ef-690822c43466\"\n# pstns = [tmp[-1]==Id for tmp in sentences]\n# positions_contracted = np.array(pstns)\n# positions_dilated = np.array([p for p in positions for i in range(MAX_LEN)])\n# print(np.sort(np.where(positions_dilated==1)[0]))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.153902Z","iopub.execute_input":"2021-06-05T14:53:44.155076Z","iopub.status.idle":"2021-06-05T14:53:44.160715Z","shell.execute_reply.started":"2021-06-05T14:53:44.155039Z","shell.execute_reply":"2021-06-05T14:53:44.159946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#np.array(sentences)[positions_contracted]","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.162872Z","iopub.execute_input":"2021-06-05T14:53:44.164601Z","iopub.status.idle":"2021-06-05T14:53:44.170515Z","shell.execute_reply.started":"2021-06-05T14:53:44.164566Z","shell.execute_reply":"2021-06-05T14:53:44.16959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tmp = pd.read_json(f'../input/coleridgeinitiative-show-us-the-data/test/{Id}.json')\n# text = \" \".join(list(tmp['text']))\n# text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n# text[:100] #string","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.172502Z","iopub.execute_input":"2021-06-05T14:53:44.173506Z","iopub.status.idle":"2021-06-05T14:53:44.179682Z","shell.execute_reply.started":"2021-06-05T14:53:44.173472Z","shell.execute_reply":"2021-06-05T14:53:44.178937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# text = np.array(text.split())\n# text","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.181161Z","iopub.execute_input":"2021-06-05T14:53:44.182287Z","iopub.status.idle":"2021-06-05T14:53:44.188025Z","shell.execute_reply.started":"2021-06-05T14:53:44.18225Z","shell.execute_reply":"2021-06-05T14:53:44.187097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# np.ceil(len(text)/MAX_LEN)*MAX_LEN","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.189988Z","iopub.execute_input":"2021-06-05T14:53:44.191219Z","iopub.status.idle":"2021-06-05T14:53:44.198873Z","shell.execute_reply.started":"2021-06-05T14:53:44.19118Z","shell.execute_reply":"2021-06-05T14:53:44.198014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#text = np.pad(text, (0, int(np.ceil(len(text)/MAX_LEN)*MAX_LEN)-len(text)), 'constant', constant_values=\"[PAD]\")","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.200474Z","iopub.execute_input":"2021-06-05T14:53:44.20115Z","iopub.status.idle":"2021-06-05T14:53:44.206902Z","shell.execute_reply.started":"2021-06-05T14:53:44.201115Z","shell.execute_reply":"2021-06-05T14:53:44.206097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#\"|\".join(text[np.array([0,1,2])]).strip(\"|\")","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.208901Z","iopub.execute_input":"2021-06-05T14:53:44.209917Z","iopub.status.idle":"2021-06-05T14:53:44.215712Z","shell.execute_reply.started":"2021-06-05T14:53:44.209881Z","shell.execute_reply":"2021-06-05T14:53:44.214948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Id = \"2f392438-e215-4169-bebf-21ac4ff253e1\"\n# positions = [tmp[-1]==Id for tmp in sentences]\n# positions = np.array([p for p in positions for i in range(MAX_LEN)])\n# print(np.sort(np.where(positions==1)[0]))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.217537Z","iopub.execute_input":"2021-06-05T14:53:44.219069Z","iopub.status.idle":"2021-06-05T14:53:44.224777Z","shell.execute_reply.started":"2021-06-05T14:53:44.219033Z","shell.execute_reply":"2021-06-05T14:53:44.223975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Id = \"3f316b38-1a24-45a9-8d8c-4e05a42257c6\"\n# positions = [tmp[-1]==Id for tmp in sentences]\n# positions = np.array([p for p in positions for i in range(MAX_LEN)])\n# print(np.sort(np.where(positions==1)[0]))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.226679Z","iopub.execute_input":"2021-06-05T14:53:44.228017Z","iopub.status.idle":"2021-06-05T14:53:44.233535Z","shell.execute_reply.started":"2021-06-05T14:53:44.227978Z","shell.execute_reply":"2021-06-05T14:53:44.232772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Id = \"8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60\"\n# positions = [tmp[-1]==Id for tmp in sentences]\n# positions = np.array([p for p in positions for i in range(MAX_LEN)])\n# print(np.sort(np.where(positions==1)[0]))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.235545Z","iopub.execute_input":"2021-06-05T14:53:44.236992Z","iopub.status.idle":"2021-06-05T14:53:44.242267Z","shell.execute_reply.started":"2021-06-05T14:53:44.236956Z","shell.execute_reply":"2021-06-05T14:53:44.241334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#np.array(pred_tags)[positions]","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.244931Z","iopub.execute_input":"2021-06-05T14:53:44.246007Z","iopub.status.idle":"2021-06-05T14:53:44.25197Z","shell.execute_reply.started":"2021-06-05T14:53:44.245968Z","shell.execute_reply":"2021-06-05T14:53:44.250948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#np.array(pred_tags)[np.array([0,1,2])]","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.254547Z","iopub.execute_input":"2021-06-05T14:53:44.255136Z","iopub.status.idle":"2021-06-05T14:53:44.260964Z","shell.execute_reply.started":"2021-06-05T14:53:44.255102Z","shell.execute_reply":"2021-06-05T14:53:44.259956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_submission_file(df, sentences, pred_tags, finding_tag=\"o-dataset\", verbose=False):\n    \"\"\"\n    Args:\n        df: pd.DataFrame\n        sentences: List[tuple[np.array, ..., str]]\n        pred_tagas: List[str]\n    Returns:\n        df: pd.DataFrame\n    \"\"\"\n    #df[\"PredictionString\"] = \"\"\n    for i,row in tqdm(df.iterrows()):\n        Id = row[\"Id\"]\n        pstns = [tmp[-1]==Id for tmp in sentences]\n        #positions_contracted = np.array(pstns)\n        positions_dilated = np.array([p for p in pstns for i in range(MAX_LEN)])\n        \n        pred_tags_per_pub = np.array(pred_tags)[positions_dilated]\n        \n        tmp = pd.read_json(f'../input/coleridgeinitiative-show-us-the-data/test/{Id}.json')\n        text = \" \".join(list(tmp['text']))\n        text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip() #clean_text\n        \n        text = np.array(text.split()) # to numpy array\n        \n        if TEXT_LEN > 0:\n            text = text[:TEXT_LEN]\n        \n        desired_len = int(np.ceil(len(text)/MAX_LEN)*MAX_LEN) # padding\n        text = np.pad(text, (0, desired_len-len(text)), 'constant', constant_values=\"[PAD]\")\n        \n        if verbose:\n            print(\"len(pred_tags_per_pub): \", len(pred_tags_per_pub))\n            print(\"len(text): \", len(text))\n        \n        assert len(pred_tags_per_pub) == len(text)\n        cond = np.where(pred_tags_per_pub==finding_tag)[0] # extract finding tokens\n        pred_string = \"|\".join(text[cond]).strip(\"|\")\n        #pred_string = \" \".join(np.where(pred_tags_per_pub==finding_tag, text, \"|\")).strip(\"|\")\n        \n        if verbose:\n            print(\"row#: \", i)\n            print(\"pred_string:\\n\", pred_string, type(pred_string))\n        \n        df.loc[i, \"PredictionString\"] = pred_string\n        #df.at[i, \"PredictionString\"] = pred_string\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.263422Z","iopub.execute_input":"2021-06-05T14:53:44.26539Z","iopub.status.idle":"2021-06-05T14:53:44.294384Z","shell.execute_reply.started":"2021-06-05T14:53:44.265355Z","shell.execute_reply":"2021-06-05T14:53:44.29347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/coleridgeinitiative-show-us-the-data/sample_submission.csv\")\ndf = make_submission_file(df, sentences, pred_tags, finding_tag=\"o-dataset\", verbose=False)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.295997Z","iopub.execute_input":"2021-06-05T14:53:44.296566Z","iopub.status.idle":"2021-06-05T14:53:44.654564Z","shell.execute_reply.started":"2021-06-05T14:53:44.296532Z","shell.execute_reply":"2021-06-05T14:53:44.65382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.655926Z","iopub.execute_input":"2021-06-05T14:53:44.656263Z","iopub.status.idle":"2021-06-05T14:53:44.66444Z","shell.execute_reply.started":"2021-06-05T14:53:44.656228Z","shell.execute_reply":"2021-06-05T14:53:44.663569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Id = \"8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60\"\n# tmp = pd.read_json(f'../input/coleridgeinitiative-show-us-the-data/test/{Id}.json')\n# text = \" \".join(list(tmp['text']))\n# text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n# text[:100] #string","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.666039Z","iopub.execute_input":"2021-06-05T14:53:44.666426Z","iopub.status.idle":"2021-06-05T14:53:44.671607Z","shell.execute_reply.started":"2021-06-05T14:53:44.66639Z","shell.execute_reply":"2021-06-05T14:53:44.670528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def extractor(df, pred_tags, max_len):\n#     \"\"\"\n#     Args:\n#         df: pd.DataFrame\n#         pred_tags: List of Str\n#         max_len: Int\n#     Returns:\n#         df: pd.DataFrame\n#     \"\"\"\n#     df[\"PredictionString\"] = \"\"\n#     total_length = 0\n#     for i, row in df.iterrows():\n#         wordlist = row[\"text\"].split(\" \")\n#         units = len(wordlist) // max_len\n#         resid = len(wordlist) - (units*max_len)\n        \n#         first = wordlist[:units*max_len]\n#         if resid > 0:\n#             second = wordlist[-resid:]\n#             second = np.pad(second, (0, max_len-len(second)), 'constant', constant_values=\"[PAD]\")\n#             first = np.append(first, second)\n#         first = np.array(first)\n#         length = len(first)\n#         sub_pred_tags = np.array(pred_tags[total_length:total_length+length])\n        \n#         datalist = first[np.where(sub_pred_tags=='o-dataset')[0]]\n#         datastring = \"|\".join(datalist).strip(\"|\")\n#         df.at[i, \"PredictionString\"] = datastring\n#         total_length += length\n#     return df","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.673311Z","iopub.execute_input":"2021-06-05T14:53:44.673699Z","iopub.status.idle":"2021-06-05T14:53:44.679988Z","shell.execute_reply.started":"2021-06-05T14:53:44.673663Z","shell.execute_reply":"2021-06-05T14:53:44.679103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = extractor(df_test, pred_tags, MAX_LEN)\n# df","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.681071Z","iopub.execute_input":"2021-06-05T14:53:44.681346Z","iopub.status.idle":"2021-06-05T14:53:44.687515Z","shell.execute_reply.started":"2021-06-05T14:53:44.681318Z","shell.execute_reply":"2021-06-05T14:53:44.686608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del df_test, pred_tags\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.688649Z","iopub.execute_input":"2021-06-05T14:53:44.688968Z","iopub.status.idle":"2021-06-05T14:53:44.695242Z","shell.execute_reply.started":"2021-06-05T14:53:44.688945Z","shell.execute_reply":"2021-06-05T14:53:44.694262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:53:44.69651Z","iopub.execute_input":"2021-06-05T14:53:44.697147Z","iopub.status.idle":"2021-06-05T14:53:44.703441Z","shell.execute_reply.started":"2021-06-05T14:53:44.697111Z","shell.execute_reply":"2021-06-05T14:53:44.702719Z"},"trusted":true},"execution_count":null,"outputs":[]}]}