{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook simply uses matching if a dataset is in the document, it \"predicts\" the title.  It uses the 180 dataset list from the train data and adds some hand curated govt dataset titles.","metadata":{}},{"cell_type":"code","source":"# huggingface related scripts are writen between #### HF and #### HFHF\n# all other scripts by Ryosuke Horiuchi will be written between #### RIOW and #### RIOWRIOW\n\n# huggingface related scripts are copied from:\n# https://github.com/riow1983/Kaggle-Coleridge-Initiative/blob/main/notebooks/kagglenb008-pytorch-bert-for-ner-inference.ipynb\n\n\n\n#### HF\nMAX_SAMPLE = None # set a small number for experimentation, set None for production.\n\n!pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n!pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n!pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl\n#### HFHF","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:07:23.862158Z","iopub.execute_input":"2021-06-21T00:07:23.862525Z","iopub.status.idle":"2021-06-21T00:08:51.013693Z","shell.execute_reply.started":"2021-06-21T00:07:23.86245Z","shell.execute_reply":"2021-06-21T00:08:51.012677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport json\nimport time\nimport datetime\nimport random\nimport glob\nimport importlib\nimport pickle\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#### RIOW\n# import torch \n# if torch.cuda.is_available():\n#     import cupy\n#### RIOWRIOW","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-21T00:08:51.015787Z","iopub.execute_input":"2021-06-21T00:08:51.016156Z","iopub.status.idle":"2021-06-21T00:08:51.80784Z","shell.execute_reply.started":"2021-06-21T00:08:51.016118Z","shell.execute_reply":"2021-06-21T00:08:51.807052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### RIOW\nrandom.seed(123)\nnp.random.seed(456)\n#### RIOWRIOW","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:08:51.811148Z","iopub.execute_input":"2021-06-21T00:08:51.811428Z","iopub.status.idle":"2021-06-21T00:08:51.817572Z","shell.execute_reply.started":"2021-06-21T00:08:51.811401Z","shell.execute_reply":"2021-06-21T00:08:51.816639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ndef get_count_tp_fp_fn(prediction, verbose=True):\n    preds = prediction.split(\" \")\n    if verbose:\n        print(preds)\n    tpc = 0\n    fpc = 0\n    fnc = 0\n    for pred in preds:\n        if pred == \"TP\":\n            tpc = tpc + 1\n        elif pred == \"FP\":\n            fpc = fpc + 1\n        elif pred == \"FN\":\n            fnc = fnc + 1\n    return [tpc, fpc, fnc]\n\ndef make_col_tp_fp_fn(df, col):\n    df['TP'] = df[col].apply(lambda x : x[0])\n    df['FP'] = df[col].apply(lambda x : x[1])\n    df['FN'] = df[col].apply(lambda x : x[2])\n    return df\n\ndef get_precision_recall(tp, fp, fn):\n    precision = tp / (tp+fp)\n    recall = tp / (tp + fn)\n    return precision, recall\n\ndef fbeta_score(precision, recall, beta):\n    fbeta = (1+(beta*beta))*((precision*recall)/( (beta*beta*precision) + recall))\n    return fbeta\n\ndef coleridge_initiative_jaccard(ground_truth, prediction, verbose=True):\n    gts = ground_truth.split('|')\n    pds = sorted(prediction.split('|'))\n    if verbose:\n        print(\"Ground truth : \" , gts)\n        print(\"Prediction : \", pds)\n        \n    js_scores = []\n    cf_matrix = []\n    \n    #### Counting True Positives (TP) and False Positives (FP)\n\n    for pd in pds:\n        if len(pd)>0:\n            score = -1\n            for gt in gts:\n                js = jaccard(pd, gt)\n                if js > score:\n                    score = js\n            if score >= 0.5:\n                js_scores.append(score)\n                cf_matrix.append(\"TP\")\n            else:\n                js_scores.append(score)\n                cf_matrix.append(\"FP\")\n\n    \n    #### Counting False Negatives (FN)\n    \n    for gt in gts:\n        score = -1\n        for pd in pds:\n            js = jaccard(gt, pd)\n            if js > score:\n                score = js\n        if score == 0:\n            js_scores.append(score)\n            cf_matrix.append(\"FN\")\n            \n    return js_scores, \" \".join(cf_matrix)\n    \n\ndef score_df_coleridge_initiative(output, gt_col, pred_col, beta=0.5, verbose=True):\n    \n    '''\n    This function will calculate the FBeta score for Coleridge Initiative competition \n    if given appropriate arguments\n    \n    Arguments - \n    output - Your submission dataframe that has both ground truth and prediction columns.\n    gt_col - This is the column name of ground truth column.\n    pred_col - This is the column name of predictions column.\n    beta - Beta value to calculate FBeta score.\n    \n    Returns - \n    This function will return the FBeta (beta=0.5) score.\n    \n    ## Set verbose = True to print logs    \n    '''\n    \n    ### Jaccard Similarity\n    output['evaluation'] = output.apply(lambda x: coleridge_initiative_jaccard(x[gt_col], x[pred_col], verbose=False), axis=1)\n    output['js_scores'] = output['evaluation'].apply(lambda x : x[0])\n    output['pred_type'] = output['evaluation'].apply(lambda x : x[1])\n    \n    ### TP, FP and FN \n    output['tp_fp_fn'] = output['pred_type'].apply(lambda x : get_count_tp_fp_fn(x, verbose=False))\n    output = make_col_tp_fp_fn(output, 'tp_fp_fn')\n    \n    tp = sum(output['TP'])\n    fp = sum(output['FP'])\n    fn = sum(output['FN'])\n    precision, recall = get_precision_recall(tp, fp, fn)\n    fbeta = fbeta_score(precision, recall, 0.5)\n    \n    if verbose:\n\n        print(\"TP_FP_FN : \", tp,fp,fn)\n\n    return fbeta","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:08:51.818966Z","iopub.execute_input":"2021-06-21T00:08:51.819314Z","iopub.status.idle":"2021-06-21T00:08:51.843962Z","shell.execute_reply.started":"2021-06-21T00:08:51.819278Z","shell.execute_reply":"2021-06-21T00:08:51.843168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### RIOW\nsample_sub = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\ntrain_data_path = '../input/coleridgeinitiative-show-us-the-data/train'\ntest_data_path = '../input/coleridgeinitiative-show-us-the-data/test'\ntrain_df = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/train.csv')\n#### RIOWRIOW\n\n#### HF\ntrain_path = '../input/coleridgeinitiative-show-us-the-data/train.csv'\ntrain = pd.read_csv(train_path)\ntrain = train[:MAX_SAMPLE]\n\npaper_train_folder = '../input/coleridgeinitiative-show-us-the-data/train'\npapers = {}\nfor paper_id in train['Id'].unique():\n    with open(f'{paper_train_folder}/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper\n        \n\nsample_submission_path = '../input/coleridgeinitiative-show-us-the-data/sample_submission.csv'\nsample_submission = pd.read_csv(sample_submission_path)\n\npaper_test_folder = '../input/coleridgeinitiative-show-us-the-data/test'\nfor paper_id in sample_submission['Id']:\n    with open(f'{paper_test_folder}/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper\n\n\nall_labels = set()\nfor label_1, label_2, label_3 in train[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\n    all_labels.add(str(label_1).lower())\n    all_labels.add(str(label_2).lower())\n    all_labels.add(str(label_3).lower())\n    \nprint(f'No. different labels: {len(all_labels)}')\n\n#### HFHF","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:08:51.84762Z","iopub.execute_input":"2021-06-21T00:08:51.847882Z","iopub.status.idle":"2021-06-21T00:09:45.308074Z","shell.execute_reply.started":"2021-06-21T00:08:51.847856Z","shell.execute_reply":"2021-06-21T00:09:45.307317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_json_pub(filename, train_data_path=train_data_path, output='text'):\n    json_path = os.path.join(train_data_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:09:45.311673Z","iopub.execute_input":"2021-06-21T00:09:45.311935Z","iopub.status.idle":"2021-06-21T00:09:45.318255Z","shell.execute_reply.started":"2021-06-21T00:09:45.311909Z","shell.execute_reply":"2021-06-21T00:09:45.317411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n    text = re.sub(' +', ' ', text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text\n","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:09:45.319428Z","iopub.execute_input":"2021-06-21T00:09:45.319805Z","iopub.status.idle":"2021-06-21T00:09:45.335603Z","shell.execute_reply.started":"2021-06-21T00:09:45.319769Z","shell.execute_reply":"2021-06-21T00:09:45.334893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:09:45.337113Z","iopub.execute_input":"2021-06-21T00:09:45.337555Z","iopub.status.idle":"2021-06-21T00:09:45.344334Z","shell.execute_reply.started":"2021-06-21T00:09:45.337517Z","shell.execute_reply":"2021-06-21T00:09:45.343587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### HF\n# def clean_text(txt):\n#     return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\ndef totally_clean_text(txt):\n    txt = clean_text(txt)\n    txt = re.sub(' +', ' ', txt)\n    return txt\n#### HFHF","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:09:45.34588Z","iopub.execute_input":"2021-06-21T00:09:45.346371Z","iopub.status.idle":"2021-06-21T00:09:45.354291Z","shell.execute_reply.started":"2021-06-21T00:09:45.346334Z","shell.execute_reply":"2021-06-21T00:09:45.353423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### HF\nliteral_preds = []\nfor paper_id in sample_submission['Id']:\n    paper = papers[paper_id]\n    text_1 = '. '.join(section['text'] for section in paper).lower()\n    text_2 = totally_clean_text(text_1)\n    \n    labels = set()\n    for label in all_labels:\n        if label in text_1 or label in text_2:\n            labels.add(clean_text(label))\n    \n    literal_preds.append('|'.join(labels))\n#### HFHF","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:09:45.355575Z","iopub.execute_input":"2021-06-21T00:09:45.355995Z","iopub.status.idle":"2021-06-21T00:09:45.464179Z","shell.execute_reply.started":"2021-06-21T00:09:45.35596Z","shell.execute_reply":"2021-06-21T00:09:45.463319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### HF\nMAX_LENGTH = 64 # max no. words for each sentence.\nOVERLAP = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\nPREDICT_BATCH = 64000 \n\n\ninputfile = \"nb005-pytorch-bert-for-ner-fold-2\"\nPRETRAINED_PATH = f'../input/{inputfile}'\n\nTEST_INPUT_SAVE_PATH = './input_data'\nTEST_NER_DATA_FILE = 'test_ner_input.json'\n\n# train_ner.json, valid_ner.jsonはオリジナルデータセット(!=inputfile)に格納\nTRAIN_PATH = f'../input/nb005-pytorch-bert-for-ner/fold_3_train_ner.json'\nVAL_PATH = f'../input/nb005-pytorch-bert-for-ner/fold_3_valid_ner.json'\n\nPREDICTION_SAVE_PATH = './pred'\nPREDICTION_FILE = 'test_predictions.txt'\n\n\ntrain = train.groupby('Id').agg({\n    'pub_title': 'first',\n    'dataset_title': '|'.join,\n    'dataset_label': '|'.join,\n    'cleaned_label': '|'.join\n}).reset_index()\n\nprint(f'No. grouped training rows: {len(train)}')\n\n\n\n\n\n\n#### HFHF","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:09:45.46582Z","iopub.execute_input":"2021-06-21T00:09:45.466201Z","iopub.status.idle":"2021-06-21T00:09:45.817642Z","shell.execute_reply.started":"2021-06-21T00:09:45.466164Z","shell.execute_reply":"2021-06-21T00:09:45.816757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### HF\ndef clean_training_text(txt):\n    \"\"\"\n    similar to the default clean_text function but without lowercasing.\n    \"\"\"\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n\ndef shorten_sentences(sentences):\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > MAX_LENGTH:\n            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences\n#### HFHF","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:09:45.819025Z","iopub.execute_input":"2021-06-21T00:09:45.819551Z","iopub.status.idle":"2021-06-21T00:09:45.827346Z","shell.execute_reply.started":"2021-06-21T00:09:45.81951Z","shell.execute_reply":"2021-06-21T00:09:45.826397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### HF\ntest_rows = [] # test data in NER format\npaper_length = [] # store the number of sentences each paper has\n\nfor paper_id in sample_submission['Id']:\n    # load paper\n    paper = papers[paper_id]\n    \n    # extract sentences\n    sentences = [clean_training_text(sentence) for section in paper \n                 for sentence in section['text'].split('.')\n                ]\n    sentences = shorten_sentences(sentences) # make sentences short\n    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n    sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n        \n    # collect all sentences in json\n    for sentence in sentences:\n        sentence_words = sentence.split()\n        dummy_tags = ['O']*len(sentence_words)\n        test_rows.append({'tokens' : sentence_words, 'tags' : dummy_tags})\n    \n    # track which sentence belongs to which data point\n    paper_length.append(len(sentences))\n    \nprint(f'total number of sentences: {len(test_rows)}')\n#### HFHF","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:09:45.828537Z","iopub.execute_input":"2021-06-21T00:09:45.829233Z","iopub.status.idle":"2021-06-21T00:09:45.874739Z","shell.execute_reply.started":"2021-06-21T00:09:45.829192Z","shell.execute_reply":"2021-06-21T00:09:45.873912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### HF\nos.environ[\"MODEL_PATH\"] = f\"{PRETRAINED_PATH}\"\nos.environ[\"TRAIN_FILE\"] = f\"{TRAIN_PATH}\"\nos.environ[\"VALIDATION_FILE\"] = f\"{VAL_PATH}\"\nos.environ[\"TEST_FILE\"] = f\"{TEST_INPUT_SAVE_PATH}/{TEST_NER_DATA_FILE}\"\nos.environ[\"OUTPUT_DIR\"] = f\"{PREDICTION_SAVE_PATH}\"\n\n\n\n# copy my_seqeval.py to the working directory because the input directory is non-writable\n!cp /kaggle/input/coleridge-packages/my_seqeval.py ./\n\n# make necessart directories and files\nos.makedirs(TEST_INPUT_SAVE_PATH, exist_ok=True)\n\n\ndef bert_predict():\n    !python ../input/kaggle-ner-utils/kaggle_run_ner.py \\\n    --model_name_or_path $MODEL_PATH \\\n    --train_file $TRAIN_FILE \\\n    --validation_file $VALIDATION_FILE \\\n    --test_file $TEST_FILE \\\n    --output_dir $OUTPUT_DIR \\\n    --report_to 'none' \\\n    --seed 123 \\\n    --do_predict\n#### HFHF","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:09:45.875785Z","iopub.execute_input":"2021-06-21T00:09:45.876141Z","iopub.status.idle":"2021-06-21T00:09:46.544891Z","shell.execute_reply.started":"2021-06-21T00:09:45.876107Z","shell.execute_reply":"2021-06-21T00:09:46.543954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### HF\nbert_outputs = []\nfor batch_begin in range(0, len(test_rows), PREDICT_BATCH):\n    # write data rows to input file\n    with open(f'{TEST_INPUT_SAVE_PATH}/{TEST_NER_DATA_FILE}', 'w') as f:\n        for row in test_rows[batch_begin:batch_begin+PREDICT_BATCH]:\n            json.dump(row, f)\n            f.write('\\n')\n    \n    # remove output dir\n    !rm -r $OUTPUT_DIR\n    \n    # do predict\n    bert_predict()\n    \n    # read predictions\n    with open(f'{PREDICTION_SAVE_PATH}/{PREDICTION_FILE}') as f:\n        this_preds = f.read().split('\\n')[:-1]\n        bert_outputs += [pred.split() for pred in this_preds]\n\n\n#### HFHF","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:09:46.548354Z","iopub.execute_input":"2021-06-21T00:09:46.548629Z","iopub.status.idle":"2021-06-21T00:11:00.915755Z","shell.execute_reply.started":"2021-06-21T00:09:46.548601Z","shell.execute_reply":"2021-06-21T00:11:00.914638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### HF\n# get test sentences\ntest_sentences = [row['tokens'] for row in test_rows]\ndel test_rows\n\n\n\nbert_dataset_labels = [] # store all dataset labels for each publication\nfor length in paper_length:\n    labels = set()\n    for sentence, pred in zip(test_sentences[:length], bert_outputs[:length]):\n        curr_phrase = ''\n        for word, tag in zip(sentence, pred):\n            if tag == 'B': # start a new phrase\n                if curr_phrase:\n                    labels.add(curr_phrase)\n                    curr_phrase = ''\n                curr_phrase = word\n            elif tag == 'I' and curr_phrase: # continue the phrase\n                curr_phrase += ' ' + word\n            else: # end last phrase (if any)\n                if curr_phrase:\n                    labels.add(curr_phrase)\n                    curr_phrase = ''\n        # check if the label is the suffix of the sentence\n        if curr_phrase:\n            labels.add(curr_phrase)\n            curr_phrase = ''\n    \n    # record dataset labels for this publication\n    bert_dataset_labels.append(labels)\n    \n    del test_sentences[:length], bert_outputs[:length]\n#### HFHF","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:11:00.918636Z","iopub.execute_input":"2021-06-21T00:11:00.918988Z","iopub.status.idle":"2021-06-21T00:11:00.938006Z","shell.execute_reply.started":"2021-06-21T00:11:00.918952Z","shell.execute_reply":"2021-06-21T00:11:00.936872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2=pd.read_csv('../input/bigger-govt-dataset-list/data_set_800.csv')\n#df2=pd.read_csv(\"../input/coleridge-additional-gov-datasets-22000popular/additional_gov_datasets_22000popular.csv\")\n#df2=pd.read_csv(\"../input/add-dataset-coloridge/data_set_800_with2000popular.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:11:00.94248Z","iopub.execute_input":"2021-06-21T00:11:00.945025Z","iopub.status.idle":"2021-06-21T00:11:00.968741Z","shell.execute_reply.started":"2021-06-21T00:11:00.944983Z","shell.execute_reply":"2021-06-21T00:11:00.9678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\n\n\n#### remove >.5 jaccard matches from predicitons\ndef jaccard_similarity(s1, s2):\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) / union\n\n\n#### HF\n# def jaccard_similarity(s1, s2):\n#     l1 = s1.split(\" \")\n#     l2 = s2.split(\" \")    \n#     intersection = len(list(set(l1).intersection(l2)))\n#     union = (len(l1) + len(l2)) - intersection\n#     return float(intersection) / union\n#### HFHF\n\n#############################\n#path=train_data_path\npath=test_data_path\n\n#for training use train_sample\n\n#for submission use sample_sub\n\n#############\n\ncolumn_names = [\"Id\", \"PredictionString\"]\n\nsubmission = pd.DataFrame(columns = column_names)\nfn_list=[]\nfn_text=[]\nall_list=[]\nall_text=[]\nto_append=[]\nfor index, row in sample_sub.iterrows():\n#for index, row in tqdm(train_df.iterrows()):\n    to_append=[row['Id'],'']\n    large_string = str(read_json_pub(row['Id'],path))\n    clean_string=text_cleaning(large_string)\n    for index, row2 in df2.iterrows():\n        query_string = str(row2['title'])\n        if query_string in clean_string:\n            if to_append[1]!='' and clean_text(query_string) not in to_append[1]:\n                to_append[1]=to_append[1]+'|'+clean_text(query_string)\n            if to_append[1]=='':\n                to_append[1]=clean_text(query_string)\n\n                \n    if to_append[1]=='':\n        fn_list+=[row['Id']]\n        fn_text+=[large_string]\n    all_list+=[row['Id']]\n    all_text+=[large_string]\n\n\n    df_length = len(submission)\n    submission.loc[df_length] = to_append\nsubmission.to_csv('submission.csv', index = False)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nsubmission\n","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:11:00.973338Z","iopub.execute_input":"2021-06-21T00:11:00.975503Z","iopub.status.idle":"2021-06-21T00:11:02.150028Z","shell.execute_reply.started":"2021-06-21T00:11:00.975457Z","shell.execute_reply":"2021-06-21T00:11:02.149094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#%%time\n#!pip uninstall fastai en-core-web-sm en-core-web-lg spacy -y -q\n#!pip install ../input/spacy3/catalogue-2.0.3-py3-none-any.whl ../input/spacy3/typer-0.3.2-py3-none-any.whl ../input/spacy3/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl ../input/spacy3/pathy-0.5.2-py3-none-any.whl ../input/spacy3/smart_open-3.0.0-py3-none-any.whl ../input/spacy3/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl ../input/spacy3/thinc-8.0.3-cp37-cp37m-manylinux2014_x86_64.whl ../input/spacy3/spacy-3.0.6-cp37-cp37m-manylinux2014_x86_64.whl ../input/spacy3/spacy_legacy-3.0.5-py2.py3-none-any.whl -q\n#!pip install ../input/spacy3/en_core_web_lg-3.0.0-py3-none-any.whl ../input/spacy3/en_core_web_md-3.0.0-py3-none-any.whl ../input/spacy3/en_core_web_sm-3.0.0-py3-none-any.whl -q\n#!pip install ../input/spacy3/spacy_alignments-0.8.3-cp37-cp37m-manylinux2014_x86_64.whl ../input/spacy3/spacy_transformers-1.0.2-py2.py3-none-any.whl ../input/spacy3/en_core_web_trf-3.0.0-py3-none-any.whl -q\n#import spacy\n#assert spacy.__version__ == '3.0.6'\n#import en_core_web_trf\n#import torch \n#if torch.cuda.is_available():\n#    spacy.prefer_gpu()\n#nlp = spacy.load(\"../input/spacy-cv-4-model/output/model-best\") #load the best model\n#nlp2 = spacy.load(\"../input/spacy-train-set/cv0-model-best\") #load the best model","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:11:02.151342Z","iopub.execute_input":"2021-06-21T00:11:02.151692Z","iopub.status.idle":"2021-06-21T00:11:02.156506Z","shell.execute_reply.started":"2021-06-21T00:11:02.151664Z","shell.execute_reply":"2021-06-21T00:11:02.155198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n%%time\n\n\nexisting_labels = set(df2[\"title\"])\ndef nlp_label_cv(Id,text,existing_labels,nlp_list):\n    c_label=[]\n    for nlp_er0 in nlp_list:\n        doc = nlp_er0(text)\n        ent_d=set([doc.ents[i].text  for i in range(len(doc.ents)) if (doc.ents[i].label_ == 'DB_label') & (clean_text(doc.ents[i].text) != \"\")] )\n       \n\n        for ent in ent_d:\n            j_val=[jaccard(clean_text(ent.lower()), clean_text(list(existing_labels)[i]))>0.7  for i in range(len(existing_labels)) ]\n            #c_label+=set(pd.Series(list(existing_labels))[j_val] )\n            #j_val=[jaccard(clean_text(ent.lower()), clean_text(list(existing_labels)[i]))  for i in range(len(existing_labels)) ]\n            #if np.max(j_val) > 0.7:\n            #    c_label+=set(pd.Series(list(existing_labels)).iloc[np.argmax(j_val)] )\n            if sum(j_val)==0:\n                c_label+=[clean_text(str(ent).lower())]\n                #if nlp_qa0(question=\"dataset?\", context=str(ent))[\"score\"] > 0.7:\n                #    c_label+=[clean_text(nlp_qa0(question=\"dataset?\", context=str(ent))['answer'].lower())  ]\n\n\n\n        del nlp_er0\n    #del nlp_qa0\n\n    \n    return [\"|\".join(list(set(c_label)))]\n    \"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:11:02.157845Z","iopub.execute_input":"2021-06-21T00:11:02.158317Z","iopub.status.idle":"2021-06-21T00:11:02.17027Z","shell.execute_reply.started":"2021-06-21T00:11:02.158279Z","shell.execute_reply":"2021-06-21T00:11:02.169263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"######################################################################################\n#############   NER推論部分_pred_nerがサブミッションファイルと同じ形式になる   ##############\n######################################################################################\n\n\"\"\"\npred_ner=pd.DataFrame(columns=[\"Id\",'PredictionString'])#\ntex_df=pd.DataFrame({\"Id\":fn_list,\"raw_text\":fn_text}).drop_duplicates()#train\n#tex_df=sample_submission_df[[\"Id\",\"raw_text\"]].drop_duplicates()#test\nId_list=[]\npred_list=[]\nfor Id in tqdm(fn_list):\n    if torch.cuda.is_available():\n        spacy.prefer_gpu()\n        torch.cuda.empty_cache()\n        cupy.get_default_memory_pool().free_all_blocks()\n    nlp_er = nlp\n    nlp_er2 = nlp2\n    #nlp_qa0=nlp_qa\n    #nlp_er.get_pipe(\"transformer\").model.attrs[\"flush_cache_chance\"] = 1\n    text = tex_df.set_index(\"Id\").loc[Id,\"raw_text\"]\n    if len(text) > 200_000:\n        text=text[0:200_000]\n    Id_list+=[Id]\n    #pred_list+=[\"|\".join(set([clean_text(doc.ents[i].text)  for i in range(len(doc.ents)) if doc.ents[i].label_ == 'DB_label' ] ))]\n    #pred_ner=pd.concat([pred_ner,nlp_lable(Id,text,existing_labels,nlp_er)],axis=0)\n    pred_list+=nlp_label_cv(Id,text,existing_labels,[nlp_er,nlp_er2])\n\n\npred_ner=pd.DataFrame({\"Id\":Id_list,'PredictionString':pred_list})   \nsum(pred_ner[\"PredictionString\"]==\"\")\n\"\"\"\n\n\n#### HF\n    \n# def jaccard_similarity(s1, s2):\n#     l1 = s1.split(\" \")\n#     l2 = s2.split(\" \")    \n#     intersection = len(list(set(l1).intersection(l2)))\n#     union = (len(l1) + len(l2)) - intersection\n#     return float(intersection) / union\n\nfiltered_bert_labels = []\nfor labels in bert_dataset_labels:\n    filtered = []\n    \n    for label in sorted(labels, key=len):\n        label = clean_text(label)\n        if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):\n            filtered.append(label)\n    \n    filtered_bert_labels.append('|'.join(filtered))\n    \n\n#### RIOW\nfinal_predictions = []\n# for literal_match, bert_pred in zip(literal_preds, filtered_bert_labels):\n#     if literal_match:\n#         final_predictions.append(literal_match)\n#     else:\n#         final_predictions.append(bert_pred)\nfor bert_pred in filtered_bert_labels:\n    final_predictions.append(bert_pred)\n#### RIOWRIOW\n        \nsample_submission['PredictionString'] = final_predictions\n#### RIOW\npred_ner = sample_submission.copy()\n#### RIOWRIOW\n#### HFHF","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:11:02.171856Z","iopub.execute_input":"2021-06-21T00:11:02.172254Z","iopub.status.idle":"2021-06-21T00:11:02.183854Z","shell.execute_reply.started":"2021-06-21T00:11:02.172216Z","shell.execute_reply":"2021-06-21T00:11:02.183152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"name=pd.Series(pred_ner[\"PredictionString\"].str.split(\"|\").sum()).value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:11:02.185339Z","iopub.execute_input":"2021-06-21T00:11:02.185852Z","iopub.status.idle":"2021-06-21T00:11:02.200001Z","shell.execute_reply.started":"2021-06-21T00:11:02.185815Z","shell.execute_reply":"2021-06-21T00:11:02.199093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tex_df=pd.DataFrame({\"Id\":all_list,\"raw_text\":all_text}).drop_duplicates()\nuse_name=name[name>100].index\ncolumn_names = [\"Id\", \"PredictionString\"]\npred_match = pd.DataFrame(columns = column_names)\nto_append=[]\nfor Id in tqdm(all_list):\n#for index, row in tqdm(train_df.iterrows()):\n    to_append=[Id,'']\n    large_string = str(tex_df.set_index(\"Id\").loc[Id,\"raw_text\"])\n    clean_string=text_cleaning(large_string)\n    for row2 in use_name:\n        query_string = str(row2)\n        if query_string in clean_string:\n            if to_append[1]!='' and clean_text(query_string) not in to_append[1]:\n                to_append[1]=to_append[1]+'|'+clean_text(query_string)\n            if to_append[1]=='':\n                to_append[1]=clean_text(query_string)\n    #pred_match+=to_append\n    df_length = len(pred_match)\n    pred_match.loc[df_length] = to_append","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:11:02.201468Z","iopub.execute_input":"2021-06-21T00:11:02.201842Z","iopub.status.idle":"2021-06-21T00:11:02.313119Z","shell.execute_reply.started":"2021-06-21T00:11:02.201808Z","shell.execute_reply":"2021-06-21T00:11:02.31045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub=pd.concat([submission,pred_match])\nsub[\"PredictionString\"]=sub[\"PredictionString\"].str.split(\"|\")\nsub=sub.groupby(\"Id\").sum()\n#### RIOW\n#sub[\"PredictionString\"]=[\"|\".join(list(set(sub[\"PredictionString\"][i]))) for i in range(sub.shape[0]) ]\nsub[\"PredictionString\"] = sub[\"PredictionString\"].apply(lambda x: \"|\".join(list(set(x))).strip(\"|\"))\n#### RIOWRIOW\nsub=sub.reset_index()\nsub","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:11:02.31467Z","iopub.execute_input":"2021-06-21T00:11:02.315018Z","iopub.status.idle":"2021-06-21T00:11:02.339181Z","shell.execute_reply.started":"2021-06-21T00:11:02.314982Z","shell.execute_reply":"2021-06-21T00:11:02.338328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### RIOW\n# for i in range(4):\n#     print(sub.loc[i, \"PredictionString\"])\n#     print()\n#### RIOWRIOW","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:11:02.340451Z","iopub.execute_input":"2021-06-21T00:11:02.340785Z","iopub.status.idle":"2021-06-21T00:11:02.347038Z","shell.execute_reply.started":"2021-06-21T00:11:02.34075Z","shell.execute_reply":"2021-06-21T00:11:02.346064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T00:11:02.348338Z","iopub.execute_input":"2021-06-21T00:11:02.3487Z","iopub.status.idle":"2021-06-21T00:11:02.35598Z","shell.execute_reply.started":"2021-06-21T00:11:02.348651Z","shell.execute_reply":"2021-06-21T00:11:02.355054Z"},"trusted":true},"execution_count":null,"outputs":[]}]}