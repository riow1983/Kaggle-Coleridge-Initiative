{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook simply uses matching if a dataset is in the document, it \"predicts\" the title.  It uses the 180 dataset list from the train data and adds some hand curated govt dataset titles.","metadata":{}},{"cell_type":"code","source":"# huggingface related scripts are writen between #### HF and #### HFHF\n# all other scripts by Ryosuke Horiuchi will be written between #### RIOW and #### RIOWRIOW\n\n# huggingface related scripts are copied from:\n# https://github.com/riow1983/Kaggle-Coleridge-Initiative/blob/main/notebooks/kagglenb008-pytorch-bert-for-ner-inference.ipynb\n\n\n\n#### HF\nMAX_SAMPLE = None # set a small number for experimentation, set None for production.\n\n!pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n!pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n!pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl\n#### HFHF","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport json\nimport time\nimport datetime\nimport random\nimport glob\nimport importlib\nimport pickle\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch \nif torch.cuda.is_available():\n    import cupy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-15T05:39:40.781408Z","iopub.execute_input":"2021-06-15T05:39:40.78198Z","iopub.status.idle":"2021-06-15T05:39:40.788918Z","shell.execute_reply.started":"2021-06-15T05:39:40.781942Z","shell.execute_reply":"2021-06-15T05:39:40.78812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### RIOW\nrandom.seed(123)\nnp.random.seed(456)\n#### RIOWRIOW","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ndef get_count_tp_fp_fn(prediction, verbose=True):\n    preds = prediction.split(\" \")\n    if verbose:\n        print(preds)\n    tpc = 0\n    fpc = 0\n    fnc = 0\n    for pred in preds:\n        if pred == \"TP\":\n            tpc = tpc + 1\n        elif pred == \"FP\":\n            fpc = fpc + 1\n        elif pred == \"FN\":\n            fnc = fnc + 1\n    return [tpc, fpc, fnc]\n\ndef make_col_tp_fp_fn(df, col):\n    df['TP'] = df[col].apply(lambda x : x[0])\n    df['FP'] = df[col].apply(lambda x : x[1])\n    df['FN'] = df[col].apply(lambda x : x[2])\n    return df\n\ndef get_precision_recall(tp, fp, fn):\n    precision = tp / (tp+fp)\n    recall = tp / (tp + fn)\n    return precision, recall\n\ndef fbeta_score(precision, recall, beta):\n    fbeta = (1+(beta*beta))*((precision*recall)/( (beta*beta*precision) + recall))\n    return fbeta\n\ndef coleridge_initiative_jaccard(ground_truth, prediction, verbose=True):\n    gts = ground_truth.split('|')\n    pds = sorted(prediction.split('|'))\n    if verbose:\n        print(\"Ground truth : \" , gts)\n        print(\"Prediction : \", pds)\n        \n    js_scores = []\n    cf_matrix = []\n    \n    #### Counting True Positives (TP) and False Positives (FP)\n\n    for pd in pds:\n        if len(pd)>0:\n            score = -1\n            for gt in gts:\n                js = jaccard(pd, gt)\n                if js > score:\n                    score = js\n            if score >= 0.5:\n                js_scores.append(score)\n                cf_matrix.append(\"TP\")\n            else:\n                js_scores.append(score)\n                cf_matrix.append(\"FP\")\n\n    \n    #### Counting False Negatives (FN)\n    \n    for gt in gts:\n        score = -1\n        for pd in pds:\n            js = jaccard(gt, pd)\n            if js > score:\n                score = js\n        if score == 0:\n            js_scores.append(score)\n            cf_matrix.append(\"FN\")\n            \n    return js_scores, \" \".join(cf_matrix)\n    \n\ndef score_df_coleridge_initiative(output, gt_col, pred_col, beta=0.5, verbose=True):\n    \n    '''\n    This function will calculate the FBeta score for Coleridge Initiative competition \n    if given appropriate arguments\n    \n    Arguments - \n    output - Your submission dataframe that has both ground truth and prediction columns.\n    gt_col - This is the column name of ground truth column.\n    pred_col - This is the column name of predictions column.\n    beta - Beta value to calculate FBeta score.\n    \n    Returns - \n    This function will return the FBeta (beta=0.5) score.\n    \n    ## Set verbose = True to print logs    \n    '''\n    \n    ### Jaccard Similarity\n    output['evaluation'] = output.apply(lambda x: coleridge_initiative_jaccard(x[gt_col], x[pred_col], verbose=False), axis=1)\n    output['js_scores'] = output['evaluation'].apply(lambda x : x[0])\n    output['pred_type'] = output['evaluation'].apply(lambda x : x[1])\n    \n    ### TP, FP and FN \n    output['tp_fp_fn'] = output['pred_type'].apply(lambda x : get_count_tp_fp_fn(x, verbose=False))\n    output = make_col_tp_fp_fn(output, 'tp_fp_fn')\n    \n    tp = sum(output['TP'])\n    fp = sum(output['FP'])\n    fn = sum(output['FN'])\n    precision, recall = get_precision_recall(tp, fp, fn)\n    fbeta = fbeta_score(precision, recall, 0.5)\n    \n    if verbose:\n\n        print(\"TP_FP_FN : \", tp,fp,fn)\n\n    return fbeta","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:39:40.790561Z","iopub.execute_input":"2021-06-15T05:39:40.791233Z","iopub.status.idle":"2021-06-15T05:39:40.816986Z","shell.execute_reply.started":"2021-06-15T05:39:40.791167Z","shell.execute_reply":"2021-06-15T05:39:40.816134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\ntrain_data_path = '../input/coleridgeinitiative-show-us-the-data/train'\ntest_data_path = '../input/coleridgeinitiative-show-us-the-data/test'\ntrain_df = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/train.csv')\n\n\n#### HF\ntrain_path = '../input/coleridgeinitiative-show-us-the-data/train.csv'\ntrain = pd.read_csv(train_path)\ntrain = train[:MAX_SAMPLE]\n\npaper_train_folder = '../input/coleridgeinitiative-show-us-the-data/train'\npapers = {}\nfor paper_id in train['Id'].unique():\n    with open(f'{paper_train_folder}/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper\n        \n\nsample_submission_path = '../input/coleridgeinitiative-show-us-the-data/sample_submission.csv'\nsample_submission = pd.read_csv(sample_submission_path)\n\npaper_test_folder = '../input/coleridgeinitiative-show-us-the-data/test'\nfor paper_id in sample_submission['Id']:\n    with open(f'{paper_test_folder}/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper\n\n#### HFHF","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_json_pub(filename, train_data_path=train_data_path, output='text'):\n    json_path = os.path.join(train_data_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:39:41.021434Z","iopub.execute_input":"2021-06-15T05:39:41.022008Z","iopub.status.idle":"2021-06-15T05:39:41.029361Z","shell.execute_reply.started":"2021-06-15T05:39:41.021971Z","shell.execute_reply":"2021-06-15T05:39:41.028651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n    text = re.sub(' +', ' ', text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text\n","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:39:41.030523Z","iopub.execute_input":"2021-06-15T05:39:41.031022Z","iopub.status.idle":"2021-06-15T05:39:41.043308Z","shell.execute_reply.started":"2021-06-15T05:39:41.030987Z","shell.execute_reply":"2021-06-15T05:39:41.042127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:39:41.046532Z","iopub.execute_input":"2021-06-15T05:39:41.047013Z","iopub.status.idle":"2021-06-15T05:39:41.055004Z","shell.execute_reply.started":"2021-06-15T05:39:41.046967Z","shell.execute_reply":"2021-06-15T05:39:41.054131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2=pd.read_csv('../input/bigger-govt-dataset-list/data_set_800.csv')\n#df2=pd.read_csv(\"../input/coleridge-additional-gov-datasets-22000popular/additional_gov_datasets_22000popular.csv\")\n#df2=pd.read_csv(\"../input/add-dataset-coloridge/data_set_800_with2000popular.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:39:41.057206Z","iopub.execute_input":"2021-06-15T05:39:41.057659Z","iopub.status.idle":"2021-06-15T05:39:41.093693Z","shell.execute_reply.started":"2021-06-15T05:39:41.057591Z","shell.execute_reply":"2021-06-15T05:39:41.092754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\n\n\n#### remove >.5 jaccard matches from predicitons\ndef jaccard_similarity(s1, s2):\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) / union\n\n#############################\n#path=train_data_path\npath=test_data_path\n\n#for training use train_sample\n\n#for submission use sample_sub\n\n#############\n\ncolumn_names = [\"Id\", \"PredictionString\"]\n\nsubmission = pd.DataFrame(columns = column_names)\nfn_list=[]\nfn_text=[]\nall_list=[]\nall_text=[]\nto_append=[]\nfor index, row in sample_sub.iterrows():\n#for index, row in tqdm(train_df.iterrows()):\n    to_append=[row['Id'],'']\n    large_string = str(read_json_pub(row['Id'],path))\n    clean_string=text_cleaning(large_string)\n    for index, row2 in df2.iterrows():\n        query_string = str(row2['title'])\n        if query_string in clean_string:\n            if to_append[1]!='' and clean_text(query_string) not in to_append[1]:\n                to_append[1]=to_append[1]+'|'+clean_text(query_string)\n            if to_append[1]=='':\n                to_append[1]=clean_text(query_string)\n\n                \n    if to_append[1]=='':\n        fn_list+=[row['Id']]\n        fn_text+=[large_string]\n    all_list+=[row['Id']]\n    all_text+=[large_string]\n\n\n    df_length = len(submission)\n    submission.loc[df_length] = to_append\nsubmission.to_csv('submission.csv', index = False)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nsubmission\n","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:39:41.095059Z","iopub.execute_input":"2021-06-15T05:39:41.095341Z","iopub.status.idle":"2021-06-15T05:39:43.362893Z","shell.execute_reply.started":"2021-06-15T05:39:41.095313Z","shell.execute_reply":"2021-06-15T05:39:43.362175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:39:43.363991Z","iopub.execute_input":"2021-06-15T05:39:43.364385Z","iopub.status.idle":"2021-06-15T05:39:43.370263Z","shell.execute_reply.started":"2021-06-15T05:39:43.364355Z","shell.execute_reply":"2021-06-15T05:39:43.369166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:39:43.371877Z","iopub.execute_input":"2021-06-15T05:39:43.372283Z","iopub.status.idle":"2021-06-15T05:39:43.382147Z","shell.execute_reply.started":"2021-06-15T05:39:43.372243Z","shell.execute_reply":"2021-06-15T05:39:43.381368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#!pip uninstall fastai en-core-web-sm en-core-web-lg spacy -y -q\n#!pip install ../input/spacy3/catalogue-2.0.3-py3-none-any.whl ../input/spacy3/typer-0.3.2-py3-none-any.whl ../input/spacy3/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl ../input/spacy3/pathy-0.5.2-py3-none-any.whl ../input/spacy3/smart_open-3.0.0-py3-none-any.whl ../input/spacy3/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl ../input/spacy3/thinc-8.0.3-cp37-cp37m-manylinux2014_x86_64.whl ../input/spacy3/spacy-3.0.6-cp37-cp37m-manylinux2014_x86_64.whl ../input/spacy3/spacy_legacy-3.0.5-py2.py3-none-any.whl -q\n#!pip install ../input/spacy3/en_core_web_lg-3.0.0-py3-none-any.whl ../input/spacy3/en_core_web_md-3.0.0-py3-none-any.whl ../input/spacy3/en_core_web_sm-3.0.0-py3-none-any.whl -q\n#!pip install ../input/spacy3/spacy_alignments-0.8.3-cp37-cp37m-manylinux2014_x86_64.whl ../input/spacy3/spacy_transformers-1.0.2-py2.py3-none-any.whl ../input/spacy3/en_core_web_trf-3.0.0-py3-none-any.whl -q\n#import spacy\n#assert spacy.__version__ == '3.0.6'\n#import en_core_web_trf\n#import torch \n#if torch.cuda.is_available():\n#    spacy.prefer_gpu()\n#nlp = spacy.load(\"../input/spacy-cv-4-model/output/model-best\") #load the best model\n#nlp2 = spacy.load(\"../input/spacy-train-set/cv0-model-best\") #load the best model","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:39:43.383356Z","iopub.execute_input":"2021-06-15T05:39:43.38394Z","iopub.status.idle":"2021-06-15T05:41:17.321316Z","shell.execute_reply.started":"2021-06-15T05:39:43.3839Z","shell.execute_reply":"2021-06-15T05:41:17.320119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n%%time\n\n\nexisting_labels = set(df2[\"title\"])\ndef nlp_label_cv(Id,text,existing_labels,nlp_list):\n    c_label=[]\n    for nlp_er0 in nlp_list:\n        doc = nlp_er0(text)\n        ent_d=set([doc.ents[i].text  for i in range(len(doc.ents)) if (doc.ents[i].label_ == 'DB_label') & (clean_text(doc.ents[i].text) != \"\")] )\n       \n\n        for ent in ent_d:\n            j_val=[jaccard(clean_text(ent.lower()), clean_text(list(existing_labels)[i]))>0.7  for i in range(len(existing_labels)) ]\n            #c_label+=set(pd.Series(list(existing_labels))[j_val] )\n            #j_val=[jaccard(clean_text(ent.lower()), clean_text(list(existing_labels)[i]))  for i in range(len(existing_labels)) ]\n            #if np.max(j_val) > 0.7:\n            #    c_label+=set(pd.Series(list(existing_labels)).iloc[np.argmax(j_val)] )\n            if sum(j_val)==0:\n                c_label+=[clean_text(str(ent).lower())]\n                #if nlp_qa0(question=\"dataset?\", context=str(ent))[\"score\"] > 0.7:\n                #    c_label+=[clean_text(nlp_qa0(question=\"dataset?\", context=str(ent))['answer'].lower())  ]\n\n\n\n        del nlp_er0\n    #del nlp_qa0\n\n    \n    return [\"|\".join(list(set(c_label)))]\n    \"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-06-15T06:18:04.000352Z","iopub.execute_input":"2021-06-15T06:18:04.000781Z","iopub.status.idle":"2021-06-15T06:18:04.011986Z","shell.execute_reply.started":"2021-06-15T06:18:04.000738Z","shell.execute_reply":"2021-06-15T06:18:04.010224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"######################################################################################\n#############   NER推論部分_pred_nerがサブミッションファイルと同じ形式になる   ##############\n######################################################################################\n\n\"\"\"\npred_ner=pd.DataFrame(columns=[\"Id\",'PredictionString'])#\ntex_df=pd.DataFrame({\"Id\":fn_list,\"raw_text\":fn_text}).drop_duplicates()#train\n#tex_df=sample_submission_df[[\"Id\",\"raw_text\"]].drop_duplicates()#test\nId_list=[]\npred_list=[]\nfor Id in tqdm(fn_list):\n    if torch.cuda.is_available():\n        spacy.prefer_gpu()\n        torch.cuda.empty_cache()\n        cupy.get_default_memory_pool().free_all_blocks()\n    nlp_er = nlp\n    nlp_er2 = nlp2\n    #nlp_qa0=nlp_qa\n    #nlp_er.get_pipe(\"transformer\").model.attrs[\"flush_cache_chance\"] = 1\n    text = tex_df.set_index(\"Id\").loc[Id,\"raw_text\"]\n    if len(text) > 200_000:\n        text=text[0:200_000]\n    Id_list+=[Id]\n    #pred_list+=[\"|\".join(set([clean_text(doc.ents[i].text)  for i in range(len(doc.ents)) if doc.ents[i].label_ == 'DB_label' ] ))]\n    #pred_ner=pd.concat([pred_ner,nlp_lable(Id,text,existing_labels,nlp_er)],axis=0)\n    pred_list+=nlp_label_cv(Id,text,existing_labels,[nlp_er,nlp_er2])\n\n\npred_ner=pd.DataFrame({\"Id\":Id_list,'PredictionString':pred_list})   \nsum(pred_ner[\"PredictionString\"]==\"\")\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-06-15T06:18:04.419592Z","iopub.execute_input":"2021-06-15T06:18:04.420093Z","iopub.status.idle":"2021-06-15T06:22:40.688088Z","shell.execute_reply.started":"2021-06-15T06:18:04.420055Z","shell.execute_reply":"2021-06-15T06:22:40.687038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"name=pd.Series(pred_ner[\"PredictionString\"].str.split(\"|\").sum()).value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T06:23:21.500331Z","iopub.execute_input":"2021-06-15T06:23:21.500712Z","iopub.status.idle":"2021-06-15T06:23:21.51036Z","shell.execute_reply.started":"2021-06-15T06:23:21.500678Z","shell.execute_reply":"2021-06-15T06:23:21.509265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tex_df=pd.DataFrame({\"Id\":all_list,\"raw_text\":all_text}).drop_duplicates()\nuse_name=name[name>100].index\ncolumn_names = [\"Id\", \"PredictionString\"]\npred_match = pd.DataFrame(columns = column_names)\nto_append=[]\nfor Id in tqdm(all_list):\n#for index, row in tqdm(train_df.iterrows()):\n    to_append=[Id,'']\n    large_string = str(tex_df.set_index(\"Id\").loc[Id,\"raw_text\"])\n    clean_string=text_cleaning(large_string)\n    for row2 in use_name:\n        query_string = str(row2)\n        if query_string in clean_string:\n            if to_append[1]!='' and clean_text(query_string) not in to_append[1]:\n                to_append[1]=to_append[1]+'|'+clean_text(query_string)\n            if to_append[1]=='':\n                to_append[1]=clean_text(query_string)\n    #pred_match+=to_append\n    df_length = len(pred_match)\n    pred_match.loc[df_length] = to_append","metadata":{"execution":{"iopub.status.busy":"2021-06-15T06:23:21.658915Z","iopub.execute_input":"2021-06-15T06:23:21.659439Z","iopub.status.idle":"2021-06-15T06:23:21.811006Z","shell.execute_reply.started":"2021-06-15T06:23:21.65939Z","shell.execute_reply":"2021-06-15T06:23:21.809915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nsub=pd.concat([submission,pred_match])\nsub[\"PredictionString\"]=sub[\"PredictionString\"].str.split(\"|\")\nsub=sub.groupby(\"Id\").sum()\nsub[\"PredictionString\"]=[\"|\".join(list(set(sub[\"PredictionString\"][i]))) for i in range(sub.shape[0]) ]\nsub=sub.reset_index()\nsub","metadata":{"execution":{"iopub.status.busy":"2021-06-15T06:23:21.98882Z","iopub.execute_input":"2021-06-15T06:23:21.989204Z","iopub.status.idle":"2021-06-15T06:23:22.023866Z","shell.execute_reply.started":"2021-06-15T06:23:21.989175Z","shell.execute_reply":"2021-06-15T06:23:22.023134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T06:23:22.239531Z","iopub.execute_input":"2021-06-15T06:23:22.240082Z","iopub.status.idle":"2021-06-15T06:23:22.246356Z","shell.execute_reply.started":"2021-06-15T06:23:22.240041Z","shell.execute_reply":"2021-06-15T06:23:22.245147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:46:05.787718Z","iopub.status.idle":"2021-06-15T05:46:05.788137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}