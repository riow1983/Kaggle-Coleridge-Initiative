{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"伊藤氏作成の[\nEX_Data_patern+Spacy3_TR(0.604) ver.2](https://www.kaggle.com/ti110106/ex-data-patern-spacy3-tr-0-604?scriptVersionId=66122583) のコピー.<br>\ninputデータはlocalnb004-spacy-trainのoutput.\n\n\nThis notebook simply uses matching if a dataset is in the document, it \"predicts\" the title.  It uses the 180 dataset list from the train data and adds some hand curated govt dataset titles.","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport json\nimport time\nimport datetime\nimport random\nimport glob\nimport importlib\nimport pickle\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch \nif torch.cuda.is_available():\n    import cupy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-14T09:20:03.166781Z","iopub.execute_input":"2021-07-14T09:20:03.16713Z","iopub.status.idle":"2021-07-14T09:20:06.428393Z","shell.execute_reply.started":"2021-07-14T09:20:03.16703Z","shell.execute_reply":"2021-07-14T09:20:06.426922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ndef get_count_tp_fp_fn(prediction, verbose=True):\n    preds = prediction.split(\" \")\n    if verbose:\n        print(preds)\n    tpc = 0\n    fpc = 0\n    fnc = 0\n    for pred in preds:\n        if pred == \"TP\":\n            tpc = tpc + 1\n        elif pred == \"FP\":\n            fpc = fpc + 1\n        elif pred == \"FN\":\n            fnc = fnc + 1\n    return [tpc, fpc, fnc]\n\ndef make_col_tp_fp_fn(df, col):\n    df['TP'] = df[col].apply(lambda x : x[0])\n    df['FP'] = df[col].apply(lambda x : x[1])\n    df['FN'] = df[col].apply(lambda x : x[2])\n    return df\n\ndef get_precision_recall(tp, fp, fn):\n    precision = tp / (tp+fp)\n    recall = tp / (tp + fn)\n    return precision, recall\n\ndef fbeta_score(precision, recall, beta):\n    fbeta = (1+(beta*beta))*((precision*recall)/( (beta*beta*precision) + recall))\n    return fbeta\n\ndef coleridge_initiative_jaccard(ground_truth, prediction, verbose=True):\n    gts = ground_truth.split('|')\n    pds = sorted(prediction.split('|'))\n    if verbose:\n        print(\"Ground truth : \" , gts)\n        print(\"Prediction : \", pds)\n        \n    js_scores = []\n    cf_matrix = []\n    \n    #### Counting True Positives (TP) and False Positives (FP)\n\n    for pd in pds:\n        if len(pd)>0:\n            score = -1\n            for gt in gts:\n                js = jaccard(pd, gt)\n                if js > score:\n                    score = js\n            if score >= 0.5:\n                js_scores.append(score)\n                cf_matrix.append(\"TP\")\n            else:\n                js_scores.append(score)\n                cf_matrix.append(\"FP\")\n\n    \n    #### Counting False Negatives (FN)\n    \n    for gt in gts:\n        score = -1\n        for pd in pds:\n            js = jaccard(gt, pd)\n            if js > score:\n                score = js\n        if score == 0:\n            js_scores.append(score)\n            cf_matrix.append(\"FN\")\n            \n    return js_scores, \" \".join(cf_matrix)\n    \n\ndef score_df_coleridge_initiative(output, gt_col, pred_col, beta=0.5, verbose=True):\n    \n    '''\n    This function will calculate the FBeta score for Coleridge Initiative competition \n    if given appropriate arguments\n    \n    Arguments - \n    output - Your submission dataframe that has both ground truth and prediction columns.\n    gt_col - This is the column name of ground truth column.\n    pred_col - This is the column name of predictions column.\n    beta - Beta value to calculate FBeta score.\n    \n    Returns - \n    This function will return the FBeta (beta=0.5) score.\n    \n    ## Set verbose = True to print logs    \n    '''\n    \n    ### Jaccard Similarity\n    output['evaluation'] = output.apply(lambda x: coleridge_initiative_jaccard(x[gt_col], x[pred_col], verbose=False), axis=1)\n    output['js_scores'] = output['evaluation'].apply(lambda x : x[0])\n    output['pred_type'] = output['evaluation'].apply(lambda x : x[1])\n    \n    ### TP, FP and FN \n    output['tp_fp_fn'] = output['pred_type'].apply(lambda x : get_count_tp_fp_fn(x, verbose=False))\n    output = make_col_tp_fp_fn(output, 'tp_fp_fn')\n    \n    tp = sum(output['TP'])\n    fp = sum(output['FP'])\n    fn = sum(output['FN'])\n    precision, recall = get_precision_recall(tp, fp, fn)\n    fbeta = fbeta_score(precision, recall, 0.5)\n    \n    if verbose:\n        #print(\"True Positives (TP) : \", tp)\n        #print(\"False Positives (FP) : \", fp)\n        #print(\"False Negatives (FN) : \", fn)\n        #print(\"Precision : \", precision)\n        #print(\"Recall : \", recall)\n        #print(\"FBeta Score : \", fbeta)\n        #display(output.head())\n        print(\"TP_FP_FN : \", tp,fp,fn)\n\n    return fbeta","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:20:06.432753Z","iopub.execute_input":"2021-07-14T09:20:06.433374Z","iopub.status.idle":"2021-07-14T09:20:06.46223Z","shell.execute_reply.started":"2021-07-14T09:20:06.433335Z","shell.execute_reply":"2021-07-14T09:20:06.461373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n\nsample_sub","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:20:06.466531Z","iopub.execute_input":"2021-07-14T09:20:06.468279Z","iopub.status.idle":"2021-07-14T09:20:06.512762Z","shell.execute_reply.started":"2021-07-14T09:20:06.468235Z","shell.execute_reply":"2021-07-14T09:20:06.511667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_path = '../input/coleridgeinitiative-show-us-the-data/train'\ntest_data_path = '../input/coleridgeinitiative-show-us-the-data/test'","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:20:06.514472Z","iopub.execute_input":"2021-07-14T09:20:06.514843Z","iopub.status.idle":"2021-07-14T09:20:06.521344Z","shell.execute_reply.started":"2021-07-14T09:20:06.514803Z","shell.execute_reply":"2021-07-14T09:20:06.520478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:20:06.522788Z","iopub.execute_input":"2021-07-14T09:20:06.523184Z","iopub.status.idle":"2021-07-14T09:20:06.682151Z","shell.execute_reply.started":"2021-07-14T09:20:06.523142Z","shell.execute_reply":"2021-07-14T09:20:06.681213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_json_pub(filename, train_data_path=train_data_path, output='text'):\n    json_path = os.path.join(train_data_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:20:06.684354Z","iopub.execute_input":"2021-07-14T09:20:06.684633Z","iopub.status.idle":"2021-07-14T09:20:06.694955Z","shell.execute_reply.started":"2021-07-14T09:20:06.684606Z","shell.execute_reply":"2021-07-14T09:20:06.693136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n    text = re.sub(' +', ' ', text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text\n","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:20:06.696913Z","iopub.execute_input":"2021-07-14T09:20:06.697301Z","iopub.status.idle":"2021-07-14T09:20:06.704079Z","shell.execute_reply.started":"2021-07-14T09:20:06.697265Z","shell.execute_reply":"2021-07-14T09:20:06.702986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:20:06.706927Z","iopub.execute_input":"2021-07-14T09:20:06.707444Z","iopub.status.idle":"2021-07-14T09:20:06.714397Z","shell.execute_reply.started":"2021-07-14T09:20:06.707406Z","shell.execute_reply":"2021-07-14T09:20:06.713501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2=pd.read_csv('../input/bigger-govt-dataset-list/data_set_800.csv')\n#df2=pd.read_csv(\"../input/coleridge-additional-gov-datasets-22000popular/additional_gov_datasets_22000popular.csv\")\n#df2=pd.read_csv(\"../input/add-dataset-coloridge/data_set_800_with2000popular.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:20:06.718294Z","iopub.execute_input":"2021-07-14T09:20:06.718604Z","iopub.status.idle":"2021-07-14T09:20:06.737116Z","shell.execute_reply.started":"2021-07-14T09:20:06.718568Z","shell.execute_reply":"2021-07-14T09:20:06.736317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\n\n\n#### remove >.5 jaccard matches from predicitons\ndef jaccard_similarity(s1, s2):\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) / union\n\n#############################\n#path=train_data_path\npath=test_data_path\n\n#for training use train_sample\n\n#for submission use sample_sub\n\n#############\n\ncolumn_names = [\"Id\", \"PredictionString\"]\n\nsubmission = pd.DataFrame(columns = column_names)\nfn_list=[]\nfn_text=[]\nall_list=[]\nall_text=[]\nto_append=[]\nfor index, row in sample_sub.iterrows():\n#for index, row in tqdm(train_df.iterrows()):\n    to_append=[row['Id'],'']\n    large_string = str(read_json_pub(row['Id'],path))\n    clean_string=text_cleaning(large_string)\n    for index, row2 in df2.iterrows():\n        query_string = str(row2['title'])\n        if query_string in clean_string:\n            if to_append[1]!='' and clean_text(query_string) not in to_append[1]:\n                to_append[1]=to_append[1]+'|'+clean_text(query_string)\n            if to_append[1]=='':\n                to_append[1]=clean_text(query_string)\n\n                \n    if to_append[1]=='':\n        fn_list+=[row['Id']]\n        fn_text+=[large_string]\n    all_list+=[row['Id']]\n    all_text+=[large_string]\n\n    ###### remove similar jaccard\n    #got_label=to_append[1].split('|')\n    #filtered=[]\n    #filtered_labels = ''\n    #for label in sorted(got_label, key=len):\n        #label = clean_text(label)\n        #if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 1.0 for got_label in filtered):\n            #filtered.append(label)\n            #if filtered_labels!='':\n                #filtered_labels=filtered_labels+'|'+label\n            #if filtered_labels=='':\n                #filtered_labels=label\n    #to_append[1] = filtered_labels         \n    #print ('################')\n    #print (to_append)\n    #print (large_string)\n    #print ('################')\n    ###### remove similar jaccard\n    df_length = len(submission)\n    submission.loc[df_length] = to_append\nsubmission.to_csv('submission.csv', index = False)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nsubmission\n","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:20:06.738492Z","iopub.execute_input":"2021-07-14T09:20:06.739031Z","iopub.status.idle":"2021-07-14T09:20:08.239413Z","shell.execute_reply.started":"2021-07-14T09:20:06.738993Z","shell.execute_reply":"2021-07-14T09:20:08.237526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#name=pd.Series(submission[\"PredictionString\"].str.split(\"|\").sum()).value_counts()\n#use_name=name[name>200].index\n#df3=pd.DataFrame({\"title\":use_name})\n\"\"\"\nstart_time = time.time()\n\n\ncolumn_names = [\"Id\", \"PredictionString\"]\n\nsubmission = pd.DataFrame(columns = column_names)\nfn_list=[]\nfn_text=[]\nto_append=[]\nfor index, row in sample_sub.iterrows():\n#for index, row in tqdm(train_df.iterrows()):\n    to_append=[row['Id'],'']\n    large_string = str(read_json_pub(row['Id'],path))\n    clean_string=text_cleaning(large_string)\n    for index, row2 in df3.iterrows():\n        query_string = str(row2['title'])\n        if query_string in clean_string:\n            if to_append[1]!='' and clean_text(query_string) not in to_append[1]:\n                to_append[1]=to_append[1]+'|'+clean_text(query_string)\n            if to_append[1]=='':\n                to_append[1]=clean_text(query_string)\n\n                \n    if to_append[1]=='':\n        fn_list+=[row['Id']]\n        fn_text+=[large_string]\n\n\n\n    ###### remove similar jaccard\n    df_length = len(submission)\n    submission.loc[df_length] = to_append\nsubmission.to_csv('submission.csv', index = False)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nsubmission\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:20:08.24398Z","iopub.execute_input":"2021-07-14T09:20:08.245489Z","iopub.status.idle":"2021-07-14T09:20:08.261028Z","shell.execute_reply.started":"2021-07-14T09:20:08.245355Z","shell.execute_reply":"2021-07-14T09:20:08.260014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df3[\"title\"].sort_values().drop_duplicates()\n#a=[len(df2[\"title\"].loc[i].split())==1 for i in range(df2.shape[0])]\n#df2[a]","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:20:08.264631Z","iopub.execute_input":"2021-07-14T09:20:08.265262Z","iopub.status.idle":"2021-07-14T09:20:08.270629Z","shell.execute_reply.started":"2021-07-14T09:20:08.265224Z","shell.execute_reply":"2021-07-14T09:20:08.268829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n!pip uninstall fastai en-core-web-sm en-core-web-lg spacy -y -q\n!pip install ../input/spacy3/catalogue-2.0.3-py3-none-any.whl ../input/spacy3/typer-0.3.2-py3-none-any.whl ../input/spacy3/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl ../input/spacy3/pathy-0.5.2-py3-none-any.whl ../input/spacy3/smart_open-3.0.0-py3-none-any.whl ../input/spacy3/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl ../input/spacy3/thinc-8.0.3-cp37-cp37m-manylinux2014_x86_64.whl ../input/spacy3/spacy-3.0.6-cp37-cp37m-manylinux2014_x86_64.whl ../input/spacy3/spacy_legacy-3.0.5-py2.py3-none-any.whl -q\n!pip install ../input/spacy3/en_core_web_lg-3.0.0-py3-none-any.whl ../input/spacy3/en_core_web_md-3.0.0-py3-none-any.whl ../input/spacy3/en_core_web_sm-3.0.0-py3-none-any.whl -q\n!pip install ../input/spacy3/spacy_alignments-0.8.3-cp37-cp37m-manylinux2014_x86_64.whl ../input/spacy3/spacy_transformers-1.0.2-py2.py3-none-any.whl ../input/spacy3/en_core_web_trf-3.0.0-py3-none-any.whl -q","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:20:08.272009Z","iopub.execute_input":"2021-07-14T09:20:08.272611Z","iopub.status.idle":"2021-07-14T09:22:14.628195Z","shell.execute_reply.started":"2021-07-14T09:20:08.272576Z","shell.execute_reply":"2021-07-14T09:22:14.62724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nassert spacy.__version__ == '3.0.6'\nimport en_core_web_trf\nimport torch \nif torch.cuda.is_available():\n    spacy.prefer_gpu()\n    \n#### RIOW\n\n#nlp = spacy.load(\"../input/spacy-train-set/cv4-3s-model-best\")\n#nlp2 = spacy.load(\"../input/spacy-train-set/cv0-model-best\") #load the best model\n\nnlp = spacy.load(\"../input/localnb004-spacy-train/model-best\")\n\n#### RIOWRIOW","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:22:14.629751Z","iopub.execute_input":"2021-07-14T09:22:14.630137Z","iopub.status.idle":"2021-07-14T09:22:40.333272Z","shell.execute_reply.started":"2021-07-14T09:22:14.630097Z","shell.execute_reply":"2021-07-14T09:22:40.332417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:22:40.334533Z","iopub.execute_input":"2021-07-14T09:22:40.334855Z","iopub.status.idle":"2021-07-14T09:22:40.346965Z","shell.execute_reply.started":"2021-07-14T09:22:40.334822Z","shell.execute_reply":"2021-07-14T09:22:40.345905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fn_list=list(train_df[\"Id\"].iloc[0:10])\n#fn_text=[str(read_json_pub(str(fn_list[i]),train_data_path)) for i in range(10)]","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:22:40.348432Z","iopub.execute_input":"2021-07-14T09:22:40.348923Z","iopub.status.idle":"2021-07-14T09:22:40.354501Z","shell.execute_reply.started":"2021-07-14T09:22:40.348867Z","shell.execute_reply":"2021-07-14T09:22:40.353779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from transformers import pipeline\n#if torch.cuda.is_available():\n#    nlp_qa = pipeline(\"question-answering\",device=0)\n#else:\n#    nlp_qa = pipeline(\"question-answering\")","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:22:40.355757Z","iopub.execute_input":"2021-07-14T09:22:40.356169Z","iopub.status.idle":"2021-07-14T09:22:40.362849Z","shell.execute_reply.started":"2021-07-14T09:22:40.356136Z","shell.execute_reply":"2021-07-14T09:22:40.36201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#with open(\"../input/spacy-train-set/nlp_qa_model.pkl\",\"rb\") as f:\n#    nlp_qa=pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:22:40.365436Z","iopub.execute_input":"2021-07-14T09:22:40.365675Z","iopub.status.idle":"2021-07-14T09:22:40.371518Z","shell.execute_reply.started":"2021-07-14T09:22:40.365653Z","shell.execute_reply":"2021-07-14T09:22:40.370613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n\nexisting_labels = set(df2[\"title\"])\ndef nlp_label(Id,text,existing_labels,nlp_er):\n    doc = nlp_er(text)\n    ent_d=set([doc.ents[i].text  for i in range(len(doc.ents)) if (doc.ents[i].label_ == 'DB_label') & (clean_text(doc.ents[i].text) != \"\")] )\n    c_label=[]\n\n    for ent in ent_d:\n        j_val=[jaccard(clean_text(ent.lower()), clean_text(list(existing_labels)[i]))>0.7  for i in range(len(existing_labels)) ]\n        #c_label+=set(pd.Series(list(existing_labels))[j_val] )\n        #j_val=[jaccard(clean_text(ent.lower()), clean_text(list(existing_labels)[i]))  for i in range(len(existing_labels)) ]\n        #if np.max(j_val) > 0.7:\n        #    c_label+=set(pd.Series(list(existing_labels)).iloc[np.argmax(j_val)] )\n        if sum(j_val)==0:\n            c_label+=[clean_text(str(ent).lower())]\n            #if nlp_qa0(question=\"dataset?\", context=str(ent))[\"score\"] > 0.7:\n            #    c_label+=[clean_text(nlp_qa0(question=\"dataset?\", context=str(ent))['answer'].lower())  ]\n    \n    \n    \n    del nlp_er\n    #del nlp_qa0\n\n    \n    return [\"|\".join(c_label)]","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:22:40.37287Z","iopub.execute_input":"2021-07-14T09:22:40.373226Z","iopub.status.idle":"2021-07-14T09:22:40.383968Z","shell.execute_reply.started":"2021-07-14T09:22:40.373193Z","shell.execute_reply":"2021-07-14T09:22:40.383222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n\nexisting_labels = set(df2[\"title\"])\ndef nlp_label_cv(Id,text,existing_labels,nlp_list):\n    c_label=[]\n    for nlp_er0 in nlp_list:\n        doc = nlp_er0(text)\n        ent_d=set([doc.ents[i].text  for i in range(len(doc.ents)) if (doc.ents[i].label_ == 'DB_label') & (clean_text(doc.ents[i].text) != \"\")] )\n       \n\n        for ent in ent_d:\n            #j_val=[jaccard(clean_text(ent.lower()), clean_text(list(existing_labels)[i]))>0.7  for i in range(len(existing_labels)) ]\n            #c_label+=set(pd.Series(list(existing_labels))[j_val] )\n            #j_val=[jaccard(clean_text(ent.lower()), clean_text(list(existing_labels)[i]))  for i in range(len(existing_labels)) ]\n            #if np.max(j_val) > 0.7:\n            #    c_label+=set(pd.Series(list(existing_labels)).iloc[np.argmax(j_val)] )\n            #if sum(j_val)==0:\n            c_label+=[clean_text(str(ent).lower())]\n                #if nlp_qa0(question=\"dataset?\", context=str(ent))[\"score\"] > 0.7:\n                #    c_label+=[clean_text(nlp_qa0(question=\"dataset?\", context=str(ent))['answer'].lower())  ]\n\n\n\n        del nlp_er0\n    #del nlp_qa0\n\n    \n    return [\"|\".join(list(set(c_label)))]","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:22:40.385208Z","iopub.execute_input":"2021-07-14T09:22:40.385583Z","iopub.status.idle":"2021-07-14T09:22:40.399195Z","shell.execute_reply.started":"2021-07-14T09:22:40.385539Z","shell.execute_reply":"2021-07-14T09:22:40.398219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\npred_ner=pd.DataFrame(columns=[\"Id\",'PredictionString'])#\ntex_df=pd.DataFrame({\"Id\":fn_list,\"raw_text\":fn_text}).drop_duplicates()#train\n\nId_list=[]\npred_list=[]\nfor Id in tqdm(fn_list):\n    if torch.cuda.is_available():\n        spacy.prefer_gpu()\n        torch.cuda.empty_cache()\n        cupy.get_default_memory_pool().free_all_blocks()\n    nlp_er = nlp\n    #### RIOW\n    #nlp_er2 = nlp2\n    #### RIOWRIOW\n    text = tex_df.set_index(\"Id\").loc[Id,\"raw_text\"]\n    if len(text) > 200_000:\n        text=text[0:200_000]\n    Id_list+=[Id]\n    #### RIOW\n    #pred_list+=nlp_label_cv(Id,text,existing_labels,[nlp_er,nlp_er2])\n    pred_list+=nlp_label_cv(Id,text,existing_labels,[nlp_er])\n    #### RIOWRIOW\n\n\npred_ner=pd.DataFrame({\"Id\":Id_list,'PredictionString':pred_list})   \nsum(pred_ner[\"PredictionString\"]==\"\")\n","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:22:40.40051Z","iopub.execute_input":"2021-07-14T09:22:40.400995Z","iopub.status.idle":"2021-07-14T09:22:40.431834Z","shell.execute_reply.started":"2021-07-14T09:22:40.40096Z","shell.execute_reply":"2021-07-14T09:22:40.431036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#score_df_coleridge_initiative(train_df.merge(pred_ner,left_on=\"Id\",right_on=\"Id\",how=\"right\"), gt_col=\"cleaned_label\", pred_col=\"PredictionString\", beta=0.5, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:22:40.433085Z","iopub.execute_input":"2021-07-14T09:22:40.433577Z","iopub.status.idle":"2021-07-14T09:22:40.437368Z","shell.execute_reply.started":"2021-07-14T09:22:40.433542Z","shell.execute_reply":"2021-07-14T09:22:40.436236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_ner","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:22:40.441156Z","iopub.execute_input":"2021-07-14T09:22:40.441585Z","iopub.status.idle":"2021-07-14T09:22:40.454647Z","shell.execute_reply.started":"2021-07-14T09:22:40.441551Z","shell.execute_reply":"2021-07-14T09:22:40.453533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"name=pd.Series(pred_ner[\"PredictionString\"].str.split(\"|\").sum()).value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:22:40.456631Z","iopub.execute_input":"2021-07-14T09:22:40.457181Z","iopub.status.idle":"2021-07-14T09:22:40.941985Z","shell.execute_reply.started":"2021-07-14T09:22:40.457139Z","shell.execute_reply":"2021-07-14T09:22:40.939927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tex_df=pd.DataFrame({\"Id\":all_list,\"raw_text\":all_text}).drop_duplicates()\nuse_name=name[name>100].index\ncolumn_names = [\"Id\", \"PredictionString\"]\npred_match = pd.DataFrame(columns = column_names)\nto_append=[]\nfor Id in tqdm(all_list):\n#for index, row in tqdm(train_df.iterrows()):\n    to_append=[Id,'']\n    large_string = str(tex_df.set_index(\"Id\").loc[Id,\"raw_text\"])\n    clean_string=text_cleaning(large_string)\n    for row2 in use_name:\n        query_string = str(row2)\n        if query_string in clean_string:\n            if to_append[1]!='' and clean_text(query_string) not in to_append[1]:\n                to_append[1]=to_append[1]+'|'+clean_text(query_string)\n            if to_append[1]=='':\n                to_append[1]=clean_text(query_string)\n    #pred_match+=to_append\n    df_length = len(pred_match)\n    pred_match.loc[df_length] = to_append","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:22:40.943351Z","iopub.status.idle":"2021-07-14T09:22:40.943913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sub=pd.concat([submission,pred_ner])\nsub=pd.concat([submission,pred_match])\nsub[\"PredictionString\"]=sub[\"PredictionString\"].str.split(\"|\")\nsub=sub.groupby(\"Id\").sum()\nsub[\"PredictionString\"]=[\"|\".join(list(set(sub[\"PredictionString\"][i]))) for i in range(sub.shape[0]) ]\nsub=sub.reset_index()\nsub","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:22:40.945502Z","iopub.status.idle":"2021-07-14T09:22:40.946081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:22:40.947431Z","iopub.status.idle":"2021-07-14T09:22:40.94798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#score_df_coleridge_initiative(train_df.merge(submission,left_on=\"Id\",right_on=\"Id\"), gt_col=\"cleaned_label\", pred_col=\"PredictionString\", beta=0.5, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T09:22:40.949367Z","iopub.status.idle":"2021-07-14T09:22:40.949921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}