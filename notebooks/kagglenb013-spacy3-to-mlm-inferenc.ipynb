{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook simply uses matching if a dataset is in the document, it \"predicts\" the title.  It uses the 180 dataset list from the train data and adds some hand curated govt dataset titles.","metadata":{}},{"cell_type":"code","source":"# CFG\n\nSEED = 42","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:13:47.818363Z","iopub.execute_input":"2021-06-21T15:13:47.818775Z","iopub.status.idle":"2021-06-21T15:13:47.823812Z","shell.execute_reply.started":"2021-06-21T15:13:47.81865Z","shell.execute_reply":"2021-06-21T15:13:47.822657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MLM related scripts are writen between #### MLM and #### MLMMLM\n# all other scripts by Ryosuke Horiuchi will be written between #### RIOW and #### RIOWRIOW\n\n# MLM related scripts are copied from:\n# https://www.kaggle.com/chienhsianghung/external-datasets-matching-mlmv4\n\n\n#### MLM\n!pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n!pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n!pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl\n#### MLMMLM","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:13:47.825851Z","iopub.execute_input":"2021-06-21T15:13:47.826451Z","iopub.status.idle":"2021-06-21T15:15:15.018295Z","shell.execute_reply.started":"2021-06-21T15:13:47.82641Z","shell.execute_reply":"2021-06-21T15:15:15.017465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport json\nimport time\nimport random\nimport glob\nimport importlib\nimport numpy as np\nimport pandas as pd\nfrom tqdm.autonotebook import tqdm\n\n#### MLM\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorForLanguageModeling, \\\nAutoModelForMaskedLM, Trainer, TrainingArguments, pipeline\n\nfrom typing import List\nimport string\nfrom functools import partial\nimport warnings\nwarnings.filterwarnings(\"ignore\", 'This pattern has match groups')\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n#### MLMMLM\n\nsample_submission = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:15.02038Z","iopub.execute_input":"2021-06-21T15:15:15.020706Z","iopub.status.idle":"2021-06-21T15:15:22.963236Z","shell.execute_reply.started":"2021-06-21T15:15:15.020672Z","shell.execute_reply":"2021-06-21T15:15:22.962463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ndef get_count_tp_fp_fn(prediction, verbose=True):\n    preds = prediction.split(\" \")\n    if verbose:\n        print(preds)\n    tpc = 0\n    fpc = 0\n    fnc = 0\n    for pred in preds:\n        if pred == \"TP\":\n            tpc = tpc + 1\n        elif pred == \"FP\":\n            fpc = fpc + 1\n        elif pred == \"FN\":\n            fnc = fnc + 1\n    return [tpc, fpc, fnc]\n\ndef make_col_tp_fp_fn(df, col):\n    df['TP'] = df[col].apply(lambda x : x[0])\n    df['FP'] = df[col].apply(lambda x : x[1])\n    df['FN'] = df[col].apply(lambda x : x[2])\n    return df\n\ndef get_precision_recall(tp, fp, fn):\n    precision = tp / (tp+fp)\n    recall = tp / (tp + fn)\n    return precision, recall\n\ndef fbeta_score(precision, recall, beta):\n    fbeta = (1+(beta*beta))*((precision*recall)/( (beta*beta*precision) + recall))\n    return fbeta\n\ndef coleridge_initiative_jaccard(ground_truth, prediction, verbose=True):\n    gts = ground_truth.split('|')\n    pds = sorted(prediction.split('|'))\n    if verbose:\n        print(\"Ground truth : \" , gts)\n        print(\"Prediction : \", pds)\n        \n    js_scores = []\n    cf_matrix = []\n    \n    #### Counting True Positives (TP) and False Positives (FP)\n\n    for pd in pds:\n        if len(pd)>0:\n            score = -1\n            for gt in gts:\n                js = jaccard(pd, gt)\n                if js > score:\n                    score = js\n            if score >= 0.5:\n                js_scores.append(score)\n                cf_matrix.append(\"TP\")\n            else:\n                js_scores.append(score)\n                cf_matrix.append(\"FP\")\n\n    \n    #### Counting False Negatives (FN)\n    \n    for gt in gts:\n        score = -1\n        for pd in pds:\n            js = jaccard(gt, pd)\n            if js > score:\n                score = js\n        if score == 0:\n            js_scores.append(score)\n            cf_matrix.append(\"FN\")\n            \n    return js_scores, \" \".join(cf_matrix)\n    \n\ndef score_df_coleridge_initiative(output, gt_col, pred_col, beta=0.5, verbose=True):\n    \n    '''\n    This function will calculate the FBeta score for Coleridge Initiative competition \n    if given appropriate arguments\n    \n    Arguments - \n    output - Your submission dataframe that has both ground truth and prediction columns.\n    gt_col - This is the column name of ground truth column.\n    pred_col - This is the column name of predictions column.\n    beta - Beta value to calculate FBeta score.\n    \n    Returns - \n    This function will return the FBeta (beta=0.5) score.\n    \n    ## Set verbose = True to print logs    \n    '''\n    \n    ### Jaccard Similarity\n    output['evaluation'] = output.apply(lambda x: coleridge_initiative_jaccard(x[gt_col], x[pred_col], verbose=False), axis=1)\n    output['js_scores'] = output['evaluation'].apply(lambda x : x[0])\n    output['pred_type'] = output['evaluation'].apply(lambda x : x[1])\n    \n    ### TP, FP and FN \n    output['tp_fp_fn'] = output['pred_type'].apply(lambda x : get_count_tp_fp_fn(x, verbose=False))\n    output = make_col_tp_fp_fn(output, 'tp_fp_fn')\n    \n    tp = sum(output['TP'])\n    fp = sum(output['FP'])\n    fn = sum(output['FN'])\n    precision, recall = get_precision_recall(tp, fp, fn)\n    fbeta = fbeta_score(precision, recall, 0.5)\n    \n    if verbose:\n\n        print(\"TP_FP_FN : \", tp,fp,fn)\n\n    return fbeta","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:22.965057Z","iopub.execute_input":"2021-06-21T15:15:22.965315Z","iopub.status.idle":"2021-06-21T15:15:22.986237Z","shell.execute_reply.started":"2021-06-21T15:15:22.96529Z","shell.execute_reply":"2021-06-21T15:15:22.985435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### RIOW\nsample_sub = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\ntrain_data_path = '../input/coleridgeinitiative-show-us-the-data/train'\ntest_data_path = '../input/coleridgeinitiative-show-us-the-data/test'\ntrain_df = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/train.csv')\n#### RIOWRIOW\n\n#### MLM\ntrain_path = '../input/coleridgeinitiative-show-us-the-data/train.csv'\ntrain_files_path = '../input/coleridgeinitiative-show-us-the-data/train'\ntrain = pd.read_csv(train_path)\n\n\nsample_submission = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\npaper_test_folder = '../input/coleridgeinitiative-show-us-the-data/test'\ntest_files_path = paper_test_folder\n\n\npapers = {}\nfor paper_id in tqdm(sample_submission['Id']):\n    with open(f'{paper_test_folder}/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper\n\n\nall_labels = set()\nfor label_1, label_2, label_3 in train[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\n    all_labels.add(str(label_1).lower())\n    all_labels.add(str(label_2).lower())\n    all_labels.add(str(label_3).lower())\n    \nprint(f'No. different labels: {len(all_labels)}')\n#### MLMMLM","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:22.987686Z","iopub.execute_input":"2021-06-21T15:15:22.988339Z","iopub.status.idle":"2021-06-21T15:15:23.252716Z","shell.execute_reply.started":"2021-06-21T15:15:22.988298Z","shell.execute_reply":"2021-06-21T15:15:23.251865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_json_pub(filename, train_data_path=train_data_path, output='text'):\n    json_path = os.path.join(train_data_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:23.25422Z","iopub.execute_input":"2021-06-21T15:15:23.254576Z","iopub.status.idle":"2021-06-21T15:15:23.263765Z","shell.execute_reply.started":"2021-06-21T15:15:23.254539Z","shell.execute_reply":"2021-06-21T15:15:23.262836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n    text = re.sub(' +', ' ', text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:23.265083Z","iopub.execute_input":"2021-06-21T15:15:23.265579Z","iopub.status.idle":"2021-06-21T15:15:23.274162Z","shell.execute_reply.started":"2021-06-21T15:15:23.265541Z","shell.execute_reply":"2021-06-21T15:15:23.273325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:23.275706Z","iopub.execute_input":"2021-06-21T15:15:23.276205Z","iopub.status.idle":"2021-06-21T15:15:23.288156Z","shell.execute_reply.started":"2021-06-21T15:15:23.276068Z","shell.execute_reply":"2021-06-21T15:15:23.287276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### MLM\n# def clean_text(txt):\n#     return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\ndef totally_clean_text(txt):\n    txt = clean_text(txt)\n    txt = re.sub(' +', ' ', txt)\n    return txt\n#### MLMMLM","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:23.289568Z","iopub.execute_input":"2021-06-21T15:15:23.290014Z","iopub.status.idle":"2021-06-21T15:15:23.297845Z","shell.execute_reply.started":"2021-06-21T15:15:23.289976Z","shell.execute_reply":"2021-06-21T15:15:23.297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### MLM\nliteral_preds = []\nfor paper_id in sample_submission['Id']:\n    paper = papers[paper_id]\n    text_1 = '. '.join(section['text'] for section in paper).lower()\n    text_2 = totally_clean_text(text_1)\n    \n    labels = set()\n    for label in all_labels:\n        if label in text_1 or label in text_2:\n            labels.add(clean_text(label))\n    \n    literal_preds.append('|'.join(labels))\n#### MLMMLM","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:23.299272Z","iopub.execute_input":"2021-06-21T15:15:23.299729Z","iopub.status.idle":"2021-06-21T15:15:23.410191Z","shell.execute_reply.started":"2021-06-21T15:15:23.299642Z","shell.execute_reply":"2021-06-21T15:15:23.409486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### MLM\nPRETRAINED_PATH = '../input/coleridge-bert-mlmv4/output-mlm/checkpoint-48000'\nTOKENIZER_PATH = '../input/coleridge-bert-mlmv4/model_tokenizer'\n\nMAX_LENGTH = 64\nOVERLAP = 20\n\nPREDICT_BATCH = 32 # a higher value requires higher GPU memory usage\n\nDATASET_SYMBOL = '$' # this symbol represents a dataset name\nNONDATA_SYMBOL = '#' # this symbol represents a non-dataset name\n#### MLMMLM","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:23.411575Z","iopub.execute_input":"2021-06-21T15:15:23.412025Z","iopub.status.idle":"2021-06-21T15:15:23.417405Z","shell.execute_reply.started":"2021-06-21T15:15:23.411986Z","shell.execute_reply":"2021-06-21T15:15:23.416446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### MLM\ntokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=True)\nmodel = AutoModelForMaskedLM.from_pretrained(PRETRAINED_PATH)\n\nmlm = pipeline(\n    'fill-mask', \n    model=model,\n    tokenizer=tokenizer,\n    device=0 if torch.cuda.is_available() else -1\n)\n#### MLMMLM","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:23.421818Z","iopub.execute_input":"2021-06-21T15:15:23.422571Z","iopub.status.idle":"2021-06-21T15:15:37.825512Z","shell.execute_reply.started":"2021-06-21T15:15:23.42253Z","shell.execute_reply":"2021-06-21T15:15:37.824764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### MLM\ndef jaccard_similarity(s1, s2):\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) / union\n\ndef clean_paper_sentence(s):\n    \"\"\"\n    This function is essentially clean_text without lowercasing.\n    \"\"\"\n    s = re.sub('[^A-Za-z0-9]+', ' ', str(s)).strip()\n    s = re.sub(' +', ' ', s)\n    return s\n\ndef shorten_sentences(sentences):\n    \"\"\"\n    Sentences that have more than MAX_LENGTH words will be split\n    into multiple sentences with overlappings.\n    \"\"\"\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > MAX_LENGTH:\n            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences\n\nconnection_tokens = {'s', 'of', 'and', 'in', 'on', 'for', 'data', 'dataset'}\ndef find_mask_candidates(sentence):\n    \"\"\"\n    Extract masking candidates for Masked Dataset Modeling from a given $sentence.\n    A candidate should be a continuous sequence of at least 2 words, \n    each of these words either has the first letter in uppercase or is one of\n    the connection words ($connection_tokens). Furthermore, the connection \n    tokens are not allowed to appear at the beginning and the end of the\n    sequence.\n    \"\"\"\n    def candidate_qualified(words):\n        while len(words) and words[0].lower() in connection_tokens:\n            words = words[1:]\n        while len(words) and words[-1].lower() in connection_tokens:\n            words = words[:-1]\n        \n        return len(words) >= 2\n    \n    candidates = []\n    \n    phrase_start, phrase_end = -1, -1\n    for id in range(1, len(sentence)):\n        word = sentence[id]\n        if word[0].isupper() or word in connection_tokens:\n            if phrase_start == -1:\n                phrase_start = phrase_end = id\n            else:\n                phrase_end = id\n        else:\n            if phrase_start != -1:\n                if candidate_qualified(sentence[phrase_start:phrase_end+1]):\n                    candidates.append((phrase_start, phrase_end))\n                phrase_start = phrase_end = -1\n    \n    if phrase_start != -1:\n        if candidate_qualified(sentence[phrase_start:phrase_end+1]):\n            candidates.append((phrase_start, phrase_end))\n    \n    return candidates","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:37.826955Z","iopub.execute_input":"2021-06-21T15:15:37.82727Z","iopub.status.idle":"2021-06-21T15:15:37.840471Z","shell.execute_reply.started":"2021-06-21T15:15:37.827235Z","shell.execute_reply":"2021-06-21T15:15:37.839527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### MLM\nmask = mlm.tokenizer.mask_token\n\nall_test_data = []\nfor paper_id in tqdm(sample_submission['Id']):\n    # load paper\n    paper = papers[paper_id]\n\n    # extract sentences\n    sentences = set([clean_paper_sentence(sentence) for section in paper \n                     for sentence in section['text'].split('.')\n                    ])\n    sentences = shorten_sentences(sentences) # make sentences short\n    sentences = [sentence for sentence in sentences if len(sentence) > 1] # only accept sentences with length > 1 chars\n    sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n    sentences = [sentence.split() for sentence in sentences] # sentence = list of words\n\n    # mask\n    test_data = []\n    for sentence in sentences:\n        for phrase_start, phrase_end in find_mask_candidates(sentence):\n            dt_point = sentence[:phrase_start] + [mask] + sentence[phrase_end+1:]\n            test_data.append((' '.join(dt_point), ' '.join(sentence[phrase_start:phrase_end+1]))) # (masked text, phrase)\n\n    all_test_data.append(test_data)\n#### MLMMLM","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:37.841881Z","iopub.execute_input":"2021-06-21T15:15:37.842447Z","iopub.status.idle":"2021-06-21T15:15:37.963129Z","shell.execute_reply.started":"2021-06-21T15:15:37.842408Z","shell.execute_reply":"2021-06-21T15:15:37.962154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### MLM\npred_mlm_labels = []\nfor test_data in tqdm(all_test_data):\n    pred_bag = set()\n\n    if len(test_data):\n        texts, phrases = list(zip(*test_data))\n        mlm_pred = []\n        for p_id in range(0, len(texts), PREDICT_BATCH):\n            batch_texts = texts[p_id:p_id+PREDICT_BATCH]\n            batch_pred = mlm(list(batch_texts), targets=[f' {DATASET_SYMBOL}', f' {NONDATA_SYMBOL}'])\n\n            if len(batch_texts) == 1:\n                batch_pred = [batch_pred]\n\n            mlm_pred.extend(batch_pred)\n\n        for (result1, result2), phrase in zip(mlm_pred, phrases):\n            if (result1['score'] > result2['score']*2 and result1['token_str'] == DATASET_SYMBOL) or\\\n               (result2['score'] > result1['score']*2 and result2['token_str'] == NONDATA_SYMBOL):\n                pred_bag.add(clean_text(phrase))\n\n    # filter labels by jaccard score \n    filtered_labels = []\n\n    for label in sorted(pred_bag, key=len, reverse=True):\n        if len(filtered_labels) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered_labels):\n            filtered_labels.append(label)\n\n    pred_mlm_labels.append('|'.join(filtered_labels))\n    \nfinal_predictions = pred_mlm_labels\nsample_submission['PredictionString'] = final_predictions\n#### MLMMLM","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:37.964864Z","iopub.execute_input":"2021-06-21T15:15:37.965249Z","iopub.status.idle":"2021-06-21T15:15:40.867147Z","shell.execute_reply.started":"2021-06-21T15:15:37.965202Z","shell.execute_reply":"2021-06-21T15:15:40.866466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2=pd.read_csv('../input/bigger-govt-dataset-list/data_set_800.csv')\n#df2=pd.read_csv(\"../input/coleridge-additional-gov-datasets-22000popular/additional_gov_datasets_22000popular.csv\")\n#df2=pd.read_csv(\"../input/add-dataset-coloridge/data_set_800_with2000popular.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:40.868351Z","iopub.execute_input":"2021-06-21T15:15:40.868669Z","iopub.status.idle":"2021-06-21T15:15:40.883181Z","shell.execute_reply.started":"2021-06-21T15:15:40.868641Z","shell.execute_reply":"2021-06-21T15:15:40.882417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\n\n\n#### remove >.5 jaccard matches from predicitons\ndef jaccard_similarity(s1, s2):\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) / union\n\n\n#############################\n#path=train_data_path\npath=test_data_path\n\n#for training use train_sample\n\n#for submission use sample_sub\n\n#############\n\ncolumn_names = [\"Id\", \"PredictionString\"]\n\nsubmission = pd.DataFrame(columns = column_names)\nfn_list=[]\nfn_text=[]\nall_list=[]\nall_text=[]\nto_append=[]\nfor index, row in sample_sub.iterrows():\n#for index, row in tqdm(train_df.iterrows()):\n    to_append=[row['Id'],'']\n    large_string = str(read_json_pub(row['Id'],path))\n    clean_string=text_cleaning(large_string)\n    for index, row2 in df2.iterrows():\n        query_string = str(row2['title'])\n        if query_string in clean_string:\n            if to_append[1]!='' and clean_text(query_string) not in to_append[1]:\n                to_append[1]=to_append[1]+'|'+clean_text(query_string)\n            if to_append[1]=='':\n                to_append[1]=clean_text(query_string)\n\n                \n    if to_append[1]=='':\n        fn_list+=[row['Id']]\n        fn_text+=[large_string]\n    all_list+=[row['Id']]\n    all_text+=[large_string]\n\n\n    df_length = len(submission)\n    submission.loc[df_length] = to_append\nsubmission.to_csv('submission.csv', index = False)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nsubmission","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:40.884522Z","iopub.execute_input":"2021-06-21T15:15:40.884875Z","iopub.status.idle":"2021-06-21T15:15:41.975554Z","shell.execute_reply.started":"2021-06-21T15:15:40.884841Z","shell.execute_reply":"2021-06-21T15:15:41.974735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%time\n#!pip uninstall fastai en-core-web-sm en-core-web-lg spacy -y -q\n#!pip install ../input/spacy3/catalogue-2.0.3-py3-none-any.whl ../input/spacy3/typer-0.3.2-py3-none-any.whl ../input/spacy3/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl ../input/spacy3/pathy-0.5.2-py3-none-any.whl ../input/spacy3/smart_open-3.0.0-py3-none-any.whl ../input/spacy3/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl ../input/spacy3/thinc-8.0.3-cp37-cp37m-manylinux2014_x86_64.whl ../input/spacy3/spacy-3.0.6-cp37-cp37m-manylinux2014_x86_64.whl ../input/spacy3/spacy_legacy-3.0.5-py2.py3-none-any.whl -q\n#!pip install ../input/spacy3/en_core_web_lg-3.0.0-py3-none-any.whl ../input/spacy3/en_core_web_md-3.0.0-py3-none-any.whl ../input/spacy3/en_core_web_sm-3.0.0-py3-none-any.whl -q\n#!pip install ../input/spacy3/spacy_alignments-0.8.3-cp37-cp37m-manylinux2014_x86_64.whl ../input/spacy3/spacy_transformers-1.0.2-py2.py3-none-any.whl ../input/spacy3/en_core_web_trf-3.0.0-py3-none-any.whl -q\n#import spacy\n#assert spacy.__version__ == '3.0.6'\n#import en_core_web_trf\n#import torch \n#if torch.cuda.is_available():\n#    spacy.prefer_gpu()\n#nlp = spacy.load(\"../input/spacy-cv-4-model/output/model-best\") #load the best model\n#nlp2 = spacy.load(\"../input/spacy-train-set/cv0-model-best\") #load the best model","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:41.97691Z","iopub.execute_input":"2021-06-21T15:15:41.97726Z","iopub.status.idle":"2021-06-21T15:15:41.981913Z","shell.execute_reply.started":"2021-06-21T15:15:41.977222Z","shell.execute_reply":"2021-06-21T15:15:41.980824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n%%time\n\n\nexisting_labels = set(df2[\"title\"])\ndef nlp_label_cv(Id,text,existing_labels,nlp_list):\n    c_label=[]\n    for nlp_er0 in nlp_list:\n        doc = nlp_er0(text)\n        ent_d=set([doc.ents[i].text  for i in range(len(doc.ents)) if (doc.ents[i].label_ == 'DB_label') & (clean_text(doc.ents[i].text) != \"\")] )\n       \n\n        for ent in ent_d:\n            j_val=[jaccard(clean_text(ent.lower()), clean_text(list(existing_labels)[i]))>0.7  for i in range(len(existing_labels)) ]\n            #c_label+=set(pd.Series(list(existing_labels))[j_val] )\n            #j_val=[jaccard(clean_text(ent.lower()), clean_text(list(existing_labels)[i]))  for i in range(len(existing_labels)) ]\n            #if np.max(j_val) > 0.7:\n            #    c_label+=set(pd.Series(list(existing_labels)).iloc[np.argmax(j_val)] )\n            if sum(j_val)==0:\n                c_label+=[clean_text(str(ent).lower())]\n                #if nlp_qa0(question=\"dataset?\", context=str(ent))[\"score\"] > 0.7:\n                #    c_label+=[clean_text(nlp_qa0(question=\"dataset?\", context=str(ent))['answer'].lower())  ]\n\n\n\n        del nlp_er0\n    #del nlp_qa0\n\n    \n    return [\"|\".join(list(set(c_label)))]\n    \"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:41.983372Z","iopub.execute_input":"2021-06-21T15:15:41.983724Z","iopub.status.idle":"2021-06-21T15:15:41.997724Z","shell.execute_reply.started":"2021-06-21T15:15:41.983688Z","shell.execute_reply":"2021-06-21T15:15:41.996663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"######################################################################################\n#############   NER推論部分_pred_nerがサブミッションファイルと同じ形式になる   ##############\n######################################################################################\n\n\"\"\"\npred_ner=pd.DataFrame(columns=[\"Id\",'PredictionString'])#\ntex_df=pd.DataFrame({\"Id\":fn_list,\"raw_text\":fn_text}).drop_duplicates()#train\n#tex_df=sample_submission_df[[\"Id\",\"raw_text\"]].drop_duplicates()#test\nId_list=[]\npred_list=[]\nfor Id in tqdm(fn_list):\n    if torch.cuda.is_available():\n        spacy.prefer_gpu()\n        torch.cuda.empty_cache()\n        cupy.get_default_memory_pool().free_all_blocks()\n    nlp_er = nlp\n    nlp_er2 = nlp2\n    #nlp_qa0=nlp_qa\n    #nlp_er.get_pipe(\"transformer\").model.attrs[\"flush_cache_chance\"] = 1\n    text = tex_df.set_index(\"Id\").loc[Id,\"raw_text\"]\n    if len(text) > 200_000:\n        text=text[0:200_000]\n    Id_list+=[Id]\n    #pred_list+=[\"|\".join(set([clean_text(doc.ents[i].text)  for i in range(len(doc.ents)) if doc.ents[i].label_ == 'DB_label' ] ))]\n    #pred_ner=pd.concat([pred_ner,nlp_lable(Id,text,existing_labels,nlp_er)],axis=0)\n    pred_list+=nlp_label_cv(Id,text,existing_labels,[nlp_er,nlp_er2])\n\n\npred_ner=pd.DataFrame({\"Id\":Id_list,'PredictionString':pred_list})   \nsum(pred_ner[\"PredictionString\"]==\"\")\n\"\"\"\n\n\n\n#### RIOW\npred_ner = sample_submission.copy()\n#### RIOWRIOW","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:41.999248Z","iopub.execute_input":"2021-06-21T15:15:41.999769Z","iopub.status.idle":"2021-06-21T15:15:42.010133Z","shell.execute_reply.started":"2021-06-21T15:15:41.999712Z","shell.execute_reply":"2021-06-21T15:15:42.009193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### RIOW\n# for i in range(4):\n#     print(pred_ner.loc[i, \"PredictionString\"])\n#     print()\n#### RIOWRIOW","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:42.011497Z","iopub.execute_input":"2021-06-21T15:15:42.011916Z","iopub.status.idle":"2021-06-21T15:15:42.028511Z","shell.execute_reply.started":"2021-06-21T15:15:42.011873Z","shell.execute_reply":"2021-06-21T15:15:42.027793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"name=pd.Series(pred_ner[\"PredictionString\"].str.split(\"|\").sum()).value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:42.030928Z","iopub.execute_input":"2021-06-21T15:15:42.031446Z","iopub.status.idle":"2021-06-21T15:15:42.03957Z","shell.execute_reply.started":"2021-06-21T15:15:42.031403Z","shell.execute_reply":"2021-06-21T15:15:42.038608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tex_df=pd.DataFrame({\"Id\":all_list,\"raw_text\":all_text}).drop_duplicates()\nuse_name=name[name>100].index\ncolumn_names = [\"Id\", \"PredictionString\"]\npred_match = pd.DataFrame(columns = column_names)\nto_append=[]\nfor Id in tqdm(all_list):\n#for index, row in tqdm(train_df.iterrows()):\n    to_append=[Id,'']\n    large_string = str(tex_df.set_index(\"Id\").loc[Id,\"raw_text\"])\n    clean_string=text_cleaning(large_string)\n    for row2 in use_name:\n        query_string = str(row2)\n        if query_string in clean_string:\n            if to_append[1]!='' and clean_text(query_string) not in to_append[1]:\n                to_append[1]=to_append[1]+'|'+clean_text(query_string)\n            if to_append[1]=='':\n                to_append[1]=clean_text(query_string)\n    #pred_match+=to_append\n    df_length = len(pred_match)\n    pred_match.loc[df_length] = to_append","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:42.041078Z","iopub.execute_input":"2021-06-21T15:15:42.041665Z","iopub.status.idle":"2021-06-21T15:15:42.170592Z","shell.execute_reply.started":"2021-06-21T15:15:42.041622Z","shell.execute_reply":"2021-06-21T15:15:42.169485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub=pd.concat([submission,pred_match])\nsub[\"PredictionString\"]=sub[\"PredictionString\"].str.split(\"|\")\nsub=sub.groupby(\"Id\").sum()\n#### RIOW\n#sub[\"PredictionString\"]=[\"|\".join(list(set(sub[\"PredictionString\"][i]))) for i in range(sub.shape[0]) ]\nsub[\"PredictionString\"] = sub[\"PredictionString\"].apply(lambda x: \"|\".join(list(set(x))).strip(\"|\"))\n#### RIOWRIOW\nsub=sub.reset_index()\nsub","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:42.172067Z","iopub.execute_input":"2021-06-21T15:15:42.172408Z","iopub.status.idle":"2021-06-21T15:15:42.198867Z","shell.execute_reply.started":"2021-06-21T15:15:42.17237Z","shell.execute_reply":"2021-06-21T15:15:42.198042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### RIOW\n# for i in range(4):\n#     print(sub.loc[i, \"PredictionString\"])\n#     print()\n#### RIOWRIOW","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:42.200451Z","iopub.execute_input":"2021-06-21T15:15:42.20093Z","iopub.status.idle":"2021-06-21T15:15:42.207169Z","shell.execute_reply.started":"2021-06-21T15:15:42.200889Z","shell.execute_reply":"2021-06-21T15:15:42.206011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:15:42.208787Z","iopub.execute_input":"2021-06-21T15:15:42.209276Z","iopub.status.idle":"2021-06-21T15:15:42.21673Z","shell.execute_reply.started":"2021-06-21T15:15:42.209237Z","shell.execute_reply":"2021-06-21T15:15:42.2158Z"},"trusted":true},"execution_count":null,"outputs":[]}]}