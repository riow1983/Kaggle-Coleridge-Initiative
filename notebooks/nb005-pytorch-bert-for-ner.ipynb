{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"nb005-pytorch-bert-for-ner.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xbVABiahs17b","executionInfo":{"status":"ok","timestamp":1620198657036,"user_tz":-540,"elapsed":965,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"bb6dc9d1-c578-4c12-d38e-09d375f51882"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Wed May  5 07:10:56 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bdaoU9CrtClz","executionInfo":{"status":"ok","timestamp":1620198686037,"user_tz":-540,"elapsed":26563,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"096e9031-4fef-4326-9dc7-99a9a780113f"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"94vP8PHQtQL-","executionInfo":{"status":"ok","timestamp":1620198690243,"user_tz":-540,"elapsed":1931,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"0720f3ae-c3d0-451f-9865-9a998e04b9fc"},"source":["%cd /content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/notebooks"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/notebooks\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dHugIDq0sALu"},"source":["This notebook shows how to fine-tune a BERT model (from huggingface) for our dataset recognition task.\n","\n","Note that internet is needed during the training phase (for downloading the bert-base-cased model). Internet can be turned off during prediction."]},{"cell_type":"markdown","metadata":{"id":"kTPntd9esAL4"},"source":["## Install packages"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5TpzFYS6tlxC","executionInfo":{"status":"ok","timestamp":1620198705085,"user_tz":-540,"elapsed":12781,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"61251d39-f75f-4ef8-9abc-6956457f8cfa"},"source":["#!pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n","!pip install datasets --no-index --find-links=file:///content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/input/coleridge-packages/packages/datasets"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Looking in links: file:///content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/input/coleridge-packages/packages/datasets\n","Processing /content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/input/coleridge-packages/packages/datasets/datasets-1.5.0-py3-none-any.whl\n","Processing /content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/input/coleridge-packages/packages/datasets/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl\n","Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.10.1)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n","Processing /content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/input/coleridge-packages/packages/datasets/huggingface_hub-0.0.7-py3-none-any.whl\n","Processing /content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/input/coleridge-packages/packages/datasets/fsspec-0.8.7-py3-none-any.whl\n","Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: xxhash, huggingface-hub, fsspec, datasets\n","Successfully installed datasets-1.5.0 fsspec-0.8.7 huggingface-hub-0.0.7 xxhash-2.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"jVuAv21HsAL4","executionInfo":{"status":"ok","timestamp":1620198722027,"user_tz":-540,"elapsed":27039,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"6e47f5fd-df6c-494a-d069-6704e22423ba"},"source":["!pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n","!pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n","!pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Processing /content/drive/My Drive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval==1.2.2) (1.19.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval==1.2.2) (0.22.2.post1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.0.1)\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n","Processing /content/drive/My Drive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n","Installing collected packages: tokenizers\n","Successfully installed tokenizers-0.10.1\n","Processing /content/drive/My Drive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (0.10.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (3.0.12)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (3.10.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (20.9)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 11.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (2019.12.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (3.0.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0.dev0) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0.dev0) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.0.dev0) (2.4.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0.dev0) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0.dev0) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0.dev0) (1.15.0)\n","Installing collected packages: sacremoses, transformers\n","Successfully installed sacremoses-0.0.45 transformers-4.5.0.dev0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DDBDvo4csAL5"},"source":["# Import"]},{"cell_type":"code","metadata":{"trusted":true,"id":"jfEdYkccsAL6","executionInfo":{"status":"ok","timestamp":1620198722867,"user_tz":-540,"elapsed":24180,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["import os\n","import re\n","import json\n","import time\n","import datetime\n","import random\n","import glob\n","import importlib\n","\n","import numpy as np\n","import pandas as pd\n","\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import gc\n","\n","random.seed(123)\n","np.random.seed(456)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"9QIuiG0asAL6","executionInfo":{"status":"ok","timestamp":1620198723270,"user_tz":-540,"elapsed":21196,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["# copy my_seqeval.py to the working directory because the input directory is non-writable\n","#!cp /kaggle/input/coleridge-packages/my_seqeval.py ./\n","!cp ../input/coleridge-packages/my_seqeval.py ./"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3cCwQ5rhuPGy","executionInfo":{"status":"ok","timestamp":1620198723898,"user_tz":-540,"elapsed":20631,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"aa5b3a9b-3f53-4803-c9a1-20704d6f8e9b"},"source":["!ls"],"execution_count":8,"outputs":[{"output_type":"stream","text":["eda-coleridge-initiative.ipynb\n","kagglenb003-annotation-data.ipynb\n","kagglenb005-pytorch-bert-for-ner.ipynb\n","l2knb001-transformers-ner\n","localnb001-transformers-ner\n","localnb001-transformers-ner.ipynb\n","my_seqeval.py\n","my_seqeval.py.lock\n","nb003-annotation-data\n","nb003-annotation-data.ipynb\n","nb005-pytorch-bert-for-ner-512\n","nb005-pytorch-bert-for-ner.ipynb\n","NERDA\n","output_folder\n","pytorch-bert-for-named-entity-recognition.ipynb\n","pytorch-xla-env-setup.py\n","setup_kaggle.ipynb\n","torch-nightly+20200515-cp37-cp37m-linux_x86_64.whl\n","torchvision-nightly+20200515-cp37-cp37m-linux_x86_64.whl\n","torch_xla-nightly+20200515-cp37-cp37m-linux_x86_64.whl\n","train_ner.json\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"y1TvCs3NsAL7"},"source":["# Hyper-parameters"]},{"cell_type":"code","metadata":{"trusted":true,"id":"RhW7ZDKRsAL7","executionInfo":{"status":"ok","timestamp":1620198724753,"user_tz":-540,"elapsed":842,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["MAX_LENGTH = 512 #64 # max no. words for each sentence.\n","OVERLAP = 170 #20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n","\n","MAX_SAMPLE = None # set a small number for experimentation, set None for production."],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_XwNllYFsAL7"},"source":["# Load data"]},{"cell_type":"code","metadata":{"trusted":true,"id":"eY9RqsJHsAL8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620198727062,"user_tz":-540,"elapsed":3128,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"6838d8d6-f95f-467d-ec08-cfcdb0f0e231"},"source":["train_path = '../input/coleridgeinitiative-show-us-the-data/train.csv'\n","paper_train_folder = '../input/coleridgeinitiative-show-us-the-data/train'\n","\n","train = pd.read_csv(train_path)\n","train = train[:MAX_SAMPLE]\n","print(f'No. raw training rows: {len(train)}')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["No. raw training rows: 19661\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LToNKh_isAL8"},"source":["Group by publication, training labels should have the same form as expected output."]},{"cell_type":"code","metadata":{"trusted":true,"id":"yzOAtA9CsAL8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620198727471,"user_tz":-540,"elapsed":3524,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"0afd0376-6cd3-40b2-d3d1-7a5b886129b0"},"source":["train = train.groupby('Id').agg({\n","    'pub_title': 'first',\n","    'dataset_title': '|'.join,\n","    'dataset_label': '|'.join,\n","    'cleaned_label': '|'.join\n","}).reset_index()\n","\n","print(f'No. grouped training rows: {len(train)}')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["No. grouped training rows: 14316\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"trusted":true,"id":"UeYZ7gvVsAL9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620198750209,"user_tz":-540,"elapsed":21626,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"d5a198a6-57d8-465a-a3a8-88b20c61a317"},"source":["#papers = {}\n","#for paper_id in train['Id'].unique():\n","#    with open(f'{paper_train_folder}/{paper_id}.json', 'r') as f:\n","#        paper = json.load(f)\n","#        papers[paper_id] = paper\n","\n","#!cp -r {paper_train_folder} /content\n","tmp = pd.read_pickle(\"../input/kagglenb007-get-text/df_train.pkl\")\n","tmp.drop_duplicates(subset=[\"Id\"], keep='first', ignore_index=True, inplace=True)\n","papers = {k:v for k,v in zip(tmp[\"Id\"], tmp[\"text\"])}\n","\n","del tmp\n","gc.collect()"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"QxEkA-UhsAL9"},"source":["# Transform data to NER format"]},{"cell_type":"code","metadata":{"trusted":true,"id":"HfqzpqX4sAL9","executionInfo":{"status":"ok","timestamp":1620198750211,"user_tz":-540,"elapsed":2635,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def clean_training_text(txt):\n","    \"\"\"\n","    similar to the default clean_text function but without lowercasing.\n","    \"\"\"\n","    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n","\n","def shorten_sentences(sentences):\n","    short_sentences = []\n","    for sentence in sentences:\n","        words = sentence.split()\n","        if len(words) > MAX_LENGTH:\n","            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n","                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n","        else:\n","            short_sentences.append(sentence)\n","    return short_sentences\n","\n","def find_sublist(big_list, small_list):\n","    all_positions = []\n","    for i in range(len(big_list) - len(small_list) + 1):\n","        if small_list == big_list[i:i+len(small_list)]:\n","            all_positions.append(i)\n","    \n","    return all_positions\n","\n","def tag_sentence(sentence, labels): # requirement: both sentence and labels are already cleaned\n","    sentence_words = sentence.split()\n","    \n","    if labels is not None and any(re.findall(f'\\\\b{label}\\\\b', sentence)\n","                                  for label in labels): # positive sample\n","        nes = ['O'] * len(sentence_words)\n","        for label in labels:\n","            label_words = label.split()\n","\n","            all_pos = find_sublist(sentence_words, label_words)\n","            for pos in all_pos:\n","                nes[pos] = 'B'\n","                for i in range(pos+1, pos+len(label_words)):\n","                    nes[i] = 'I'\n","\n","        return True, list(zip(sentence_words, nes))\n","        \n","    else: # negative sample\n","        nes = ['O'] * len(sentence_words)\n","        return False, list(zip(sentence_words, nes))"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y3Ocy5y6ZJ8P","executionInfo":{"status":"ok","timestamp":1620198751444,"user_tz":-540,"elapsed":649,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"fe69b7af-142a-4320-a1d6-4aeb233ee53a"},"source":["papers['d0fa7568-7d8e-4db9-870f-f9c6f668c17b']"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'section_title': 'What is this study about?',\n","  'text': 'This study used data from the National Education Longitudinal Study (NELS:88) to examine the effects of dual enrollment programs for high school students on college degree attainment. The study also reported whether the impacts of dual enrollment programs were different for first generation college students versus students whose parents had attended at least some college. In addition, a supplemental analysis reports on the impact of different amounts of dual enrollment course-taking and college degree attainment.\\nDual enrollment programs offer college-level learning experiences for high school students. The programs offer college courses and/or the opportunity to earn college credits for students while still in high school.\\nThe intervention group in the study was comprised of NELS participants who attended a postsecondary school and who participated in a dual enrollment program while in high school (n = 880). The study author used propensity score matching methods to create a comparison group of NELS participants who also attended a postsecondary school but who did not participate in a dual enrollment program in high school (n = 7,920).'},\n"," {'section_title': 'Features of Dual Enrollment Programs',\n","  'text': \"Dual enrollment programs allow high school students to take college courses and earn college credits while still in high school. These programs are intended to improve college attainment, especially among low-income students, by helping students prepare academically for the rigors of college coursework and enabling students to accumulate college credits toward a degree.\\nThe study reported program impacts on two outcomes: attainment of any college degree and attainment of a Bachelor's degree. These impacts were examined for various subgroups of students, which are described below.\"},\n"," {'section_title': 'WWC Single Study Review',\n","  'text': \"What did the study find?\\nThe study reported, and the WWC confirmed, that dual enrollment programs significantly increased the likelihood of attaining (a) any college degree and (b) a bachelor's degree.\\nThe study reported on the impact of dual enrollment programs for first generation college students, students whose parents had some college, students whose parents had a Bachelor's degree, and students with post-Bachelor's degree parents. Although point estimates differed for the subgroups of students with different parental educational backgrounds, the subgroups were not significantly different from each other. The overall impact of dual enrollment programs is, therefore, the WWC's best estimate of effectiveness for these subgroups (see Appendix D for more information).\\nThe study author also examined whether students who earned more dual enrollment credits prior to attending college achieved greater benefits from the dual enrollment programs. The impact of dual enrollment was significantly different for students earning three dual enrollment credits versus those earning six or more credits. Students who earned three credits (i.e., had one dual enrollment course) were not more likely to attain a college degree than comparison group students. However, students who earned six credits (i.e., two courses) and students who earned seven or more credits were significantly more likely to attain any college degree or a bachelor's degree than comparison students (see Appendix D for more information).\\nA sample of these respondents was then resurveyed through four follow-ups in 1990, 1992, 1994, and 2000 . The fourth follow-up (2000) was used for this study.\\nAppendix B: Outcome measures for the degree attainment domain\\n\\nStudy Notes: A correction for multiple comparisons was needed but did not affect significance levels. The p-values presented here were reported in the original study. The mean differences reported in the table are covariate-adjusted mean differences and were taken from \"},\n"," {'section_title': 'WWC Rating',\n","  'text': 'The research described in this report meets WWC evidence standards with reservations Cautions: Although the study matched students who participated in dual enrollment programs to those who did not, students who self-selected to participate in dual enrollment programs may have been different from students in general high school programs in ways that were unobserved in the study data. Study sample A nationally representative sample of eighth graders was first surveyed in the spring of 1988.'},\n"," {'section_title': 'Intervention group',\n","  'text': 'The intervention group was comprised of those individuals in the NELS sample who attended college and participated in dual enrollment programs in high school. No other information about the nature or characteristics of the dual enrollment programs was provided.'},\n"," {'section_title': 'Comparison group',\n","  'text': 'The comparison group was comprised of NELS participants who attended college but did not participate in a dual enrollment program in high school. The comparison students participated in other high school programs (e.g., traditional high school programs or Advanced Placement programs).'},\n"," {'section_title': 'Outcomes and measurement',\n","  'text': \"There were two outcomes examined in the study: any college degree attainment and bachelor's degree attainment. Both outcomes were derived from the fourth follow-up of the NELS collected in 2000. For a more detailed description of these outcome measures, see Appendix B.\"},\n"," {'section_title': 'Support for implementation',\n","  'text': 'The study was based on secondary data from a nationally representative survey of students. No information about the nature of the training for dual enrollment providers or implemenation was provided.'},\n"," {'section_title': 'Reason for review',\n","  'text': 'This study was identified for review by receiving media attention.'},\n"," {'section_title': 'Degree attainment',\n","  'text': 'Any college degree attainment The study author collected information on college degree attainment from the fourth follow-up of the National Education Longitudinal Study collected in 2000. Only students who attended college were included in the study.'},\n"," {'section_title': \"Bachelor's degree attainment\",\n","  'text': \"The study author collected information on bachelor's degree attainment from the fourth follow-up of the National Education Longitudinal Study collected in 2000. Only students who attended college were included in the study. The effect size is a standardized measure of the effect of an intervention on student outcomes, representing the change (measured in standard deviations) in an average student's outcome that can be expected if the student is given the intervention. The improvement index is an alternate presentation of the effect size, reflecting the change in an average student's percentile rank that can be expected if the student is given the intervention. The WWC-computed average effect size is a simple average rounded to two decimal places; the average improvement index is calculated from the average effect size. The statistical significance of the study's domain average was determined by the WWC; the study is characterized as having a statistically significant positive effect because univariate statistical tests are reported for each outcome measure and both effects are positive and statistically significant, accounting for multiple comparisons. nr = not reported.\"},\n"," {'section_title': 'Study Notes:',\n","  'text': \"A correction for multiple comparisons was needed, but did not affect the statistical significance of the outcomes. The p-values presented here were reported in the original study. Effect sizes were computed using the covariate-adjusted mean difference and standard errors reported in Table 1 of the manuscript. This quasi-experimental study used propensity score matching to create a comparison group; therefore, the WWC required that baseline equivalence be established to determine whether the study met standards with reservations. Because the two outcomes reported above did not have pretests, two key covariates were selected by the WWC from among those used by the study author in the propensity score matching: family income and prior achievement. In response to a request by the WWC, the author provided baseline means and standard deviations for the analytic sample on these two covariates. The baseline effect sizes for family income and prior achievement were 0.02 and 0.00, respectively. The study therefore meets WWC standards with reservations because baseline equivalence was met on these two covariates. Table Notes : Positive results for mean difference, effect size, and improvement index favor the intervention group; negative results favor the comparison group. The effect size is a standardized measure of the effect of an intervention on student outcomes, representing the change (measured in standard deviations) in an average student's outcome that can be expected if the student is given the intervention. The improvement index is an alternate presentation of the effect size, reflecting the change in an average student's percentile rank that can be expected if the student is given the intervention. nr = not reported.\"},\n"," {'section_title': 'Glossary of Terms',\n","  'text': 'Attrition Attrition occurs when an outcome variable is not available for all participants initially assigned to the intervention and comparison groups. The WWC considers the total attrition rate and the difference in attrition rates across groups within a study.\\nClustering adjustment If intervention assignment is made at a cluster level and the analysis is conducted at the student level, the WWC will adjust the statistical significance to account for this mismatch, if necessary.\\nConfounding factor A confounding factor is a component of a study that is completely aligned with one of the study conditions, making it impossible to separate how much of the observed effect was due to the intervention and how much was due to the factor.\\nDesign The design of a study is the method by which intervention and comparison groups were assigned.\\nDomain A domain is a group of closely related outcomes.\\nEffect size The effect size is a measure of the magnitude of an effect. The WWC uses a standardized measure to facilitate comparisons across studies and outcomes.\\nEligibility A study is eligible for review if it falls within the scope of the review protocol and uses either an experimental or matched comparison group design.\\nEquivalence A demonstration that the analysis sample groups are similar on observed characteristics defined in the review area protocol.\\nImprovement index Along a percentile distribution of students, the improvement index represents the gain or loss of the average student due to the intervention. As the average student starts at the 50th percentile, the measure ranges from -50 to +50.'},\n"," {'section_title': 'Multiple comparison adjustment',\n","  'text': 'When a study includes multiple outcomes or comparison groups, the WWC will adjust the statistical significance to account for the multiple comparisons, if necessary.'},\n"," {'section_title': 'Quasi-experimental design (QED)',\n","  'text': 'A quasi-experimental design (QED) is a research design in which subjects are assigned to intervention and comparison groups through a process that is not random.'},\n"," {'section_title': 'Randomized controlled trial (RCT)',\n","  'text': 'A randomized controlled trial (RCT) is an experiment in which investigators randomly assign eligible participants into intervention and comparison groups.'},\n"," {'section_title': 'Single-case design (SCD)',\n","  'text': 'A research approach in which an outcome variable is measured repeatedly within and across different conditions that are defined by the presence or absence of an intervention.'},\n"," {'section_title': 'Standard deviation',\n","  'text': 'The standard deviation of a measure shows how much variation exists across observations in the sample. A low standard deviation indicates that the observations in the sample tend to be very close to the mean; a high standard deviation indicates that the observations in the sample are spread out over a large range of values.\\nStatistical significance Statistical significance is the probability that the difference between groups is a result of chance rather than a real difference between the groups. The WWC labels a finding statistically significant if the likelihood that the difference is due to chance is less than 5% (p < 0.05).\\nSubstantively important A substantively important finding is one that has an effect size of 0.25 or greater, regardless of statistical significance.\\nPlease see the WWC Procedures and Standards Handbook (version 2.1) for additional details.'}]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"trusted":true,"id":"cD7k0HejsAL-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620198920989,"user_tz":-540,"elapsed":165024,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"d7fdef84-069a-4c4a-c016-247fade4f3c1"},"source":["cnt_pos, cnt_neg = 0, 0 # number of sentences that contain/not contain labels\n","ner_data = []\n","\n","pbar = tqdm(total=len(train))\n","for i, id, dataset_label in train[['Id', 'dataset_label']].itertuples():\n","    # paper\n","    paper = papers[id]\n","    \n","    # labels\n","    labels = dataset_label.split('|')\n","    labels = [clean_training_text(label) for label in labels]\n","    \n","    # sentences\n","    sentences = set([clean_training_text(sentence) for section in paper \n","                 for sentence in section['text'].split('.') \n","                ])\n","    sentences = shorten_sentences(sentences) # make sentences short\n","    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n","    \n","    # positive sample\n","    for sentence in sentences:\n","        is_positive, tags = tag_sentence(sentence, labels)\n","        if is_positive:\n","            cnt_pos += 1\n","            ner_data.append(tags)\n","        elif any(word in sentence.lower() for word in ['data', 'study']): \n","            ner_data.append(tags)\n","            cnt_neg += 1\n","    \n","    # process bar\n","    pbar.update(1)\n","    pbar.set_description(f\"Training data size: {cnt_pos} positives + {cnt_neg} negatives\")\n","\n","# shuffling\n","random.shuffle(ner_data)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Training data size: 47202 positives + 514263 negatives: 100%|██████████| 14316/14316 [02:43<00:00, 95.66it/s]"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"k3M6GYyQsAL_"},"source":["write data to file."]},{"cell_type":"code","metadata":{"trusted":true,"id":"UQYQOhdrsAL_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620198951117,"user_tz":-540,"elapsed":191522,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"6dcbf35b-eb4d-4899-abd3-e6ef650ddc42"},"source":["with open('train_ner.json', 'w') as f:\n","    for row in ner_data:\n","        words, nes = list(zip(*row))\n","        row_json = {'tokens' : words, 'tags' : nes}\n","        json.dump(row_json, f)\n","        f.write('\\n')"],"execution_count":16,"outputs":[{"output_type":"stream","text":["\rTraining data size: 47202 positives + 514263 negatives: 100%|██████████| 14316/14316 [03:00<00:00, 95.66it/s]"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"an-PfAp_sAMA"},"source":["# Fine-tune a BERT model for NER"]},{"cell_type":"code","metadata":{"trusted":true,"id":"7bFPFmUZsAMA","executionInfo":{"status":"ok","timestamp":1620123658617,"user_tz":-540,"elapsed":176804,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["output_folder = \"nb005-pytorch-bert-for-ner\"\n","!rm -r {output_folder}\n","!mkdir {output_folder}\n","os.environ[\"OUTPUT_DIR\"] = output_folder\n","\n","def train(resume_training=False, num_checkpoint=None):\n","    if resume_training:\n","        os.environ[\"MODEL_PATH\"] = f\"./{output_folder}/checkpoint-{num_checkpoint}\"\n","        !python ../input/kaggle-ner-utils/kaggle_run_ner.py \\\n","        --model_name_or_path $MODEL_PATH \\\n","        --train_file './train_ner.json' \\\n","        --validation_file './train_ner.json' \\\n","        --num_train_epochs 5 \\\n","        --per_device_train_batch_size 8 \\\n","        --per_device_eval_batch_size 8 \\\n","        --save_steps 15000 \\\n","        --output_dir $OUTPUT_DIR \\\n","        --report_to 'none' \\\n","        --seed 123 \\\n","        --do_train \n","    else:\n","        !python ../input/kaggle-ner-utils/kaggle_run_ner.py \\\n","        --model_name_or_path 'bert-base-cased' \\\n","        --train_file './train_ner.json' \\\n","        --validation_file './train_ner.json' \\\n","        --num_train_epochs 5 \\\n","        --per_device_train_batch_size 8 \\\n","        --per_device_eval_batch_size 8 \\\n","        --save_steps 15000 \\\n","        --output_dir $OUTPUT_DIR \\\n","        --report_to 'none' \\\n","        --seed 123 \\\n","        --do_train "],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1n31gu8HKoAf","executionInfo":{"status":"ok","timestamp":1620165555091,"user_tz":-540,"elapsed":3181207,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"40c36428-6bd6-494d-f2b3-959202edcf53"},"source":[" train(resume_training=False)\n","#train(resume_training=True, num_checkpoint=45000)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["2021-05-04 10:21:03.639834: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","05/04/2021 10:21:05 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","05/04/2021 10:21:05 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=nb005-pytorch-bert-for-ner, overwrite_output_dir=False, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/May04_10-21-05_1696736c7213, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=15000, save_total_limit=None, no_cuda=False, seed=123, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=nb005-pytorch-bert-for-ner, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1)\n","05/04/2021 10:21:05 - WARNING - datasets.builder -   Using custom data configuration default-c9ea33a7b7ceb850\n","Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-c9ea33a7b7ceb850/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-c9ea33a7b7ceb850/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n","[INFO|file_utils.py:1402] 2021-05-04 10:21:25,600 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpygzioj5c\n","Downloading: 100% 570/570 [00:00<00:00, 459kB/s]\n","[INFO|file_utils.py:1406] 2021-05-04 10:21:25,871 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|file_utils.py:1409] 2021-05-04 10:21:25,871 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:472] 2021-05-04 10:21:25,871 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:508] 2021-05-04 10:21:25,872 >> Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"ner\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.5.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:472] 2021-05-04 10:21:26,141 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:508] 2021-05-04 10:21:26,141 >> Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.5.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|file_utils.py:1402] 2021-05-04 10:21:26,411 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp30ceo434\n","Downloading: 100% 213k/213k [00:00<00:00, 634kB/s]\n","[INFO|file_utils.py:1406] 2021-05-04 10:21:27,017 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|file_utils.py:1409] 2021-05-04 10:21:27,017 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|file_utils.py:1402] 2021-05-04 10:21:27,286 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpb2h3ovz1\n","Downloading: 100% 436k/436k [00:00<00:00, 1.03MB/s]\n","[INFO|file_utils.py:1406] 2021-05-04 10:21:27,979 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|file_utils.py:1409] 2021-05-04 10:21:27,979 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|file_utils.py:1402] 2021-05-04 10:21:28,786 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6s7l6anx\n","Downloading: 100% 29.0/29.0 [00:00<00:00, 22.9kB/s]\n","[INFO|file_utils.py:1406] 2021-05-04 10:21:29,055 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|file_utils.py:1409] 2021-05-04 10:21:29,055 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|tokenization_utils_base.py:1702] 2021-05-04 10:21:29,055 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1702] 2021-05-04 10:21:29,055 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1702] 2021-05-04 10:21:29,055 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1702] 2021-05-04 10:21:29,055 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1702] 2021-05-04 10:21:29,055 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|file_utils.py:1402] 2021-05-04 10:21:29,349 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp74xs5trf\n","Downloading: 100% 436M/436M [00:06<00:00, 67.3MB/s]\n","[INFO|file_utils.py:1406] 2021-05-04 10:21:35,888 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[INFO|file_utils.py:1409] 2021-05-04 10:21:35,888 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[INFO|modeling_utils.py:1051] 2021-05-04 10:21:35,888 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[WARNING|modeling_utils.py:1159] 2021-05-04 10:21:39,563 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:1170] 2021-05-04 10:21:39,564 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100% 557/557 [01:29<00:00,  6.22ba/s]\n","[INFO|trainer.py:485] 2021-05-04 10:23:16,455 >> The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags.\n","[INFO|trainer.py:988] 2021-05-04 10:23:16,666 >> ***** Running training *****\n","[INFO|trainer.py:989] 2021-05-04 10:23:16,666 >>   Num examples = 556688\n","[INFO|trainer.py:990] 2021-05-04 10:23:16,666 >>   Num Epochs = 5\n","[INFO|trainer.py:991] 2021-05-04 10:23:16,666 >>   Instantaneous batch size per device = 8\n","[INFO|trainer.py:992] 2021-05-04 10:23:16,666 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:993] 2021-05-04 10:23:16,666 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:994] 2021-05-04 10:23:16,666 >>   Total optimization steps = 347930\n","{'loss': 0.0289, 'learning_rate': 4.9928146466243214e-05, 'epoch': 0.01}\n","{'loss': 0.0137, 'learning_rate': 4.985629293248642e-05, 'epoch': 0.01}\n","{'loss': 0.0084, 'learning_rate': 4.978443939872963e-05, 'epoch': 0.02}\n","{'loss': 0.0096, 'learning_rate': 4.971258586497284e-05, 'epoch': 0.03}\n","{'loss': 0.0088, 'learning_rate': 4.964073233121605e-05, 'epoch': 0.04}\n","{'loss': 0.0074, 'learning_rate': 4.956887879745926e-05, 'epoch': 0.04}\n","{'loss': 0.0075, 'learning_rate': 4.949702526370247e-05, 'epoch': 0.05}\n","{'loss': 0.0058, 'learning_rate': 4.942517172994568e-05, 'epoch': 0.06}\n","{'loss': 0.0064, 'learning_rate': 4.9353318196188894e-05, 'epoch': 0.06}\n","{'loss': 0.0063, 'learning_rate': 4.9281464662432105e-05, 'epoch': 0.07}\n","{'loss': 0.0074, 'learning_rate': 4.920961112867531e-05, 'epoch': 0.08}\n","{'loss': 0.0053, 'learning_rate': 4.913775759491852e-05, 'epoch': 0.09}\n","{'loss': 0.0061, 'learning_rate': 4.906590406116173e-05, 'epoch': 0.09}\n","{'loss': 0.0069, 'learning_rate': 4.899405052740494e-05, 'epoch': 0.1}\n","{'loss': 0.0073, 'learning_rate': 4.892219699364815e-05, 'epoch': 0.11}\n","{'loss': 0.0047, 'learning_rate': 4.885034345989136e-05, 'epoch': 0.11}\n","{'loss': 0.0041, 'learning_rate': 4.8778489926134566e-05, 'epoch': 0.12}\n","{'loss': 0.0058, 'learning_rate': 4.870663639237778e-05, 'epoch': 0.13}\n","{'loss': 0.0048, 'learning_rate': 4.863478285862099e-05, 'epoch': 0.14}\n","{'loss': 0.0055, 'learning_rate': 4.85629293248642e-05, 'epoch': 0.14}\n","{'loss': 0.0061, 'learning_rate': 4.849107579110741e-05, 'epoch': 0.15}\n","{'loss': 0.0058, 'learning_rate': 4.8419222257350624e-05, 'epoch': 0.16}\n","{'loss': 0.0058, 'learning_rate': 4.834736872359383e-05, 'epoch': 0.17}\n","{'loss': 0.0056, 'learning_rate': 4.827551518983704e-05, 'epoch': 0.17}\n","{'loss': 0.0061, 'learning_rate': 4.8203661656080246e-05, 'epoch': 0.18}\n","{'loss': 0.0061, 'learning_rate': 4.813180812232346e-05, 'epoch': 0.19}\n","{'loss': 0.0067, 'learning_rate': 4.805995458856667e-05, 'epoch': 0.19}\n","{'loss': 0.0074, 'learning_rate': 4.7988101054809874e-05, 'epoch': 0.2}\n","{'loss': 0.0051, 'learning_rate': 4.7916247521053086e-05, 'epoch': 0.21}\n","{'loss': 0.006, 'learning_rate': 4.78443939872963e-05, 'epoch': 0.22}\n","  4% 15000/347930 [29:35<10:29:25,  8.82it/s][INFO|trainer.py:1600] 2021-05-04 10:52:51,878 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-15000\n","[INFO|configuration_utils.py:318] 2021-05-04 10:52:51,884 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-15000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 10:52:53,627 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-15000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 10:52:54,563 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-15000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 10:52:54,569 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-15000/special_tokens_map.json\n","{'loss': 0.0054, 'learning_rate': 4.77725404535395e-05, 'epoch': 0.22}\n","{'loss': 0.0048, 'learning_rate': 4.7700686919782714e-05, 'epoch': 0.23}\n","{'loss': 0.0058, 'learning_rate': 4.7628833386025925e-05, 'epoch': 0.24}\n","{'loss': 0.0054, 'learning_rate': 4.755697985226914e-05, 'epoch': 0.24}\n","{'loss': 0.0053, 'learning_rate': 4.748512631851235e-05, 'epoch': 0.25}\n","{'loss': 0.0061, 'learning_rate': 4.741327278475556e-05, 'epoch': 0.26}\n","{'loss': 0.0042, 'learning_rate': 4.7341419250998765e-05, 'epoch': 0.27}\n","{'loss': 0.0052, 'learning_rate': 4.726956571724198e-05, 'epoch': 0.27}\n","{'loss': 0.0076, 'learning_rate': 4.719771218348519e-05, 'epoch': 0.28}\n","{'loss': 0.0079, 'learning_rate': 4.712585864972839e-05, 'epoch': 0.29}\n","{'loss': 0.0066, 'learning_rate': 4.7054005115971605e-05, 'epoch': 0.29}\n","{'loss': 0.0063, 'learning_rate': 4.6982151582214816e-05, 'epoch': 0.3}\n","{'loss': 0.0097, 'learning_rate': 4.691029804845802e-05, 'epoch': 0.31}\n","{'loss': 0.0063, 'learning_rate': 4.683844451470123e-05, 'epoch': 0.32}\n","{'loss': 0.0111, 'learning_rate': 4.6766590980944445e-05, 'epoch': 0.32}\n","{'loss': 0.0074, 'learning_rate': 4.6694737447187656e-05, 'epoch': 0.33}\n","{'loss': 0.0073, 'learning_rate': 4.662288391343087e-05, 'epoch': 0.34}\n","{'loss': 0.0088, 'learning_rate': 4.655103037967408e-05, 'epoch': 0.34}\n","{'loss': 0.0084, 'learning_rate': 4.6479176845917284e-05, 'epoch': 0.35}\n","{'loss': 0.0108, 'learning_rate': 4.6407323312160496e-05, 'epoch': 0.36}\n","{'loss': 0.0114, 'learning_rate': 4.633546977840371e-05, 'epoch': 0.37}\n","{'loss': 0.0082, 'learning_rate': 4.626361624464691e-05, 'epoch': 0.37}\n","{'loss': 0.0072, 'learning_rate': 4.6191762710890124e-05, 'epoch': 0.38}\n","{'loss': 0.0076, 'learning_rate': 4.6119909177133336e-05, 'epoch': 0.39}\n","{'loss': 0.0075, 'learning_rate': 4.604805564337654e-05, 'epoch': 0.4}\n","{'loss': 0.0088, 'learning_rate': 4.597620210961975e-05, 'epoch': 0.4}\n","{'loss': 0.0126, 'learning_rate': 4.5904348575862964e-05, 'epoch': 0.41}\n","{'loss': 0.0091, 'learning_rate': 4.583249504210617e-05, 'epoch': 0.42}\n","{'loss': 0.0104, 'learning_rate': 4.576064150834938e-05, 'epoch': 0.42}\n","{'loss': 0.0087, 'learning_rate': 4.568878797459259e-05, 'epoch': 0.43}\n","  9% 30000/347930 [59:30<10:04:04,  8.77it/s][INFO|trainer.py:1600] 2021-05-04 11:22:47,365 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-30000\n","[INFO|configuration_utils.py:318] 2021-05-04 11:22:47,370 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-30000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 11:22:50,028 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-30000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 11:22:50,033 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-30000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 11:22:50,037 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-30000/special_tokens_map.json\n","{'loss': 0.0088, 'learning_rate': 4.5616934440835804e-05, 'epoch': 0.44}\n","{'loss': 0.0188, 'learning_rate': 4.5545080907079015e-05, 'epoch': 0.45}\n","{'loss': 0.0404, 'learning_rate': 4.547322737332223e-05, 'epoch': 0.45}\n","{'loss': 0.0406, 'learning_rate': 4.540137383956543e-05, 'epoch': 0.46}\n","{'loss': 0.0308, 'learning_rate': 4.5329520305808643e-05, 'epoch': 0.47}\n","{'loss': 0.024, 'learning_rate': 4.5257666772051855e-05, 'epoch': 0.47}\n","{'loss': 0.0237, 'learning_rate': 4.518581323829506e-05, 'epoch': 0.48}\n","{'loss': 0.0338, 'learning_rate': 4.511395970453827e-05, 'epoch': 0.49}\n","{'loss': 0.0284, 'learning_rate': 4.5042106170781476e-05, 'epoch': 0.5}\n","{'loss': 0.0278, 'learning_rate': 4.497025263702469e-05, 'epoch': 0.5}\n","{'loss': 0.0338, 'learning_rate': 4.48983991032679e-05, 'epoch': 0.51}\n","{'loss': 0.0257, 'learning_rate': 4.482654556951111e-05, 'epoch': 0.52}\n","{'loss': 0.0288, 'learning_rate': 4.475469203575432e-05, 'epoch': 0.52}\n","{'loss': 0.0312, 'learning_rate': 4.4682838501997535e-05, 'epoch': 0.53}\n","{'loss': 0.0333, 'learning_rate': 4.461098496824074e-05, 'epoch': 0.54}\n","{'loss': 0.0432, 'learning_rate': 4.453913143448395e-05, 'epoch': 0.55}\n","{'loss': 0.0666, 'learning_rate': 4.446727790072716e-05, 'epoch': 0.55}\n","{'loss': 0.0532, 'learning_rate': 4.439542436697037e-05, 'epoch': 0.56}\n","{'loss': 0.0549, 'learning_rate': 4.432357083321358e-05, 'epoch': 0.57}\n","{'loss': 0.0554, 'learning_rate': 4.425171729945679e-05, 'epoch': 0.57}\n","{'loss': 0.0565, 'learning_rate': 4.4179863765699996e-05, 'epoch': 0.58}\n","{'loss': 0.0552, 'learning_rate': 4.410801023194321e-05, 'epoch': 0.59}\n","{'loss': 0.0544, 'learning_rate': 4.403615669818642e-05, 'epoch': 0.6}\n","{'loss': 0.0441, 'learning_rate': 4.3964303164429624e-05, 'epoch': 0.6}\n","{'loss': 0.0548, 'learning_rate': 4.3892449630672835e-05, 'epoch': 0.61}\n","{'loss': 0.0562, 'learning_rate': 4.382059609691605e-05, 'epoch': 0.62}\n","{'loss': 0.0503, 'learning_rate': 4.374874256315926e-05, 'epoch': 0.63}\n","{'loss': 0.0497, 'learning_rate': 4.367688902940247e-05, 'epoch': 0.63}\n","{'loss': 0.0511, 'learning_rate': 4.360503549564568e-05, 'epoch': 0.64}\n","{'loss': 0.0581, 'learning_rate': 4.353318196188889e-05, 'epoch': 0.65}\n"," 13% 45000/347930 [1:29:24<9:20:51,  9.00it/s][INFO|trainer.py:1600] 2021-05-04 11:52:41,299 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-45000\n","[INFO|configuration_utils.py:318] 2021-05-04 11:52:41,304 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-45000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 11:52:43,200 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-45000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 11:52:44,047 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-45000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 11:52:44,052 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-45000/special_tokens_map.json\n","{'loss': 0.0541, 'learning_rate': 4.34613284281321e-05, 'epoch': 0.65}\n","{'loss': 0.0576, 'learning_rate': 4.338947489437531e-05, 'epoch': 0.66}\n","{'loss': 0.055, 'learning_rate': 4.3317621360618515e-05, 'epoch': 0.67}\n","{'loss': 0.0523, 'learning_rate': 4.3245767826861727e-05, 'epoch': 0.68}\n","{'loss': 0.0565, 'learning_rate': 4.317391429310494e-05, 'epoch': 0.68}\n","{'loss': 0.0542, 'learning_rate': 4.310206075934814e-05, 'epoch': 0.69}\n","{'loss': 0.0579, 'learning_rate': 4.3030207225591355e-05, 'epoch': 0.7}\n","{'loss': 0.0567, 'learning_rate': 4.2958353691834566e-05, 'epoch': 0.7}\n","{'loss': 0.0553, 'learning_rate': 4.288650015807778e-05, 'epoch': 0.71}\n","{'loss': 0.059, 'learning_rate': 4.281464662432099e-05, 'epoch': 0.72}\n","{'loss': 0.0568, 'learning_rate': 4.27427930905642e-05, 'epoch': 0.73}\n","{'loss': 0.0499, 'learning_rate': 4.2670939556807406e-05, 'epoch': 0.73}\n","{'loss': 0.0513, 'learning_rate': 4.259908602305062e-05, 'epoch': 0.74}\n","{'loss': 0.0563, 'learning_rate': 4.252723248929383e-05, 'epoch': 0.75}\n","{'loss': 0.052, 'learning_rate': 4.2455378955537034e-05, 'epoch': 0.75}\n","{'loss': 0.0533, 'learning_rate': 4.2383525421780246e-05, 'epoch': 0.76}\n","{'loss': 0.0524, 'learning_rate': 4.231167188802346e-05, 'epoch': 0.77}\n","{'loss': 0.0523, 'learning_rate': 4.223981835426666e-05, 'epoch': 0.78}\n","{'loss': 0.0548, 'learning_rate': 4.2167964820509874e-05, 'epoch': 0.78}\n","{'loss': 0.0571, 'learning_rate': 4.2096111286753086e-05, 'epoch': 0.79}\n","{'loss': 0.0523, 'learning_rate': 4.202425775299629e-05, 'epoch': 0.8}\n","{'loss': 0.0584, 'learning_rate': 4.19524042192395e-05, 'epoch': 0.8}\n","{'loss': 0.0547, 'learning_rate': 4.1880550685482714e-05, 'epoch': 0.81}\n","{'loss': 0.0531, 'learning_rate': 4.1808697151725925e-05, 'epoch': 0.82}\n","{'loss': 0.0553, 'learning_rate': 4.173684361796914e-05, 'epoch': 0.83}\n","{'loss': 0.053, 'learning_rate': 4.166499008421234e-05, 'epoch': 0.83}\n","{'loss': 0.0535, 'learning_rate': 4.1593136550455554e-05, 'epoch': 0.84}\n","{'loss': 0.0554, 'learning_rate': 4.1521283016698765e-05, 'epoch': 0.85}\n","{'loss': 0.0549, 'learning_rate': 4.144942948294197e-05, 'epoch': 0.86}\n","{'loss': 0.059, 'learning_rate': 4.137757594918518e-05, 'epoch': 0.86}\n"," 17% 60000/347930 [1:59:19<10:46:21,  7.42it/s][INFO|trainer.py:1600] 2021-05-04 12:22:35,944 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-60000\n","[INFO|configuration_utils.py:318] 2021-05-04 12:22:35,949 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-60000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 12:22:37,794 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-60000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 12:22:38,719 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-60000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 12:22:38,723 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-60000/special_tokens_map.json\n","{'loss': 0.0591, 'learning_rate': 4.130572241542839e-05, 'epoch': 0.87}\n","{'loss': 0.0531, 'learning_rate': 4.12338688816716e-05, 'epoch': 0.88}\n","{'loss': 0.0562, 'learning_rate': 4.116201534791481e-05, 'epoch': 0.88}\n","{'loss': 0.0571, 'learning_rate': 4.109016181415802e-05, 'epoch': 0.89}\n","{'loss': 0.0543, 'learning_rate': 4.101830828040123e-05, 'epoch': 0.9}\n","{'loss': 0.0561, 'learning_rate': 4.0946454746644445e-05, 'epoch': 0.91}\n","{'loss': 0.0556, 'learning_rate': 4.0874601212887656e-05, 'epoch': 0.91}\n","{'loss': 0.0627, 'learning_rate': 4.080274767913086e-05, 'epoch': 0.92}\n","{'loss': 0.0584, 'learning_rate': 4.073089414537407e-05, 'epoch': 0.93}\n","{'loss': 0.0584, 'learning_rate': 4.0659040611617284e-05, 'epoch': 0.93}\n","{'loss': 0.0573, 'learning_rate': 4.058718707786049e-05, 'epoch': 0.94}\n","{'loss': 0.0537, 'learning_rate': 4.05153335441037e-05, 'epoch': 0.95}\n","{'loss': 0.0566, 'learning_rate': 4.044348001034691e-05, 'epoch': 0.96}\n","{'loss': 0.0593, 'learning_rate': 4.037162647659012e-05, 'epoch': 0.96}\n","{'loss': 0.0524, 'learning_rate': 4.029977294283333e-05, 'epoch': 0.97}\n","{'loss': 0.0557, 'learning_rate': 4.022791940907654e-05, 'epoch': 0.98}\n","{'loss': 0.0549, 'learning_rate': 4.0156065875319746e-05, 'epoch': 0.98}\n","{'loss': 0.061, 'learning_rate': 4.008421234156296e-05, 'epoch': 0.99}\n","{'loss': 0.0572, 'learning_rate': 4.001235880780617e-05, 'epoch': 1.0}\n","{'loss': 0.0543, 'learning_rate': 3.994050527404938e-05, 'epoch': 1.01}\n","{'loss': 0.0543, 'learning_rate': 3.986865174029259e-05, 'epoch': 1.01}\n","{'loss': 0.0486, 'learning_rate': 3.9796798206535804e-05, 'epoch': 1.02}\n","{'loss': 0.0568, 'learning_rate': 3.972494467277901e-05, 'epoch': 1.03}\n","{'loss': 0.0562, 'learning_rate': 3.965309113902222e-05, 'epoch': 1.03}\n","{'loss': 0.0515, 'learning_rate': 3.958123760526543e-05, 'epoch': 1.04}\n","{'loss': 0.0564, 'learning_rate': 3.950938407150864e-05, 'epoch': 1.05}\n","{'loss': 0.0547, 'learning_rate': 3.943753053775185e-05, 'epoch': 1.06}\n","{'loss': 0.0531, 'learning_rate': 3.936567700399506e-05, 'epoch': 1.06}\n","{'loss': 0.0545, 'learning_rate': 3.9293823470238265e-05, 'epoch': 1.07}\n","{'loss': 0.059, 'learning_rate': 3.9221969936481476e-05, 'epoch': 1.08}\n"," 22% 75000/347930 [2:29:56<8:26:16,  8.98it/s][INFO|trainer.py:1600] 2021-05-04 12:53:12,823 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-75000\n","[INFO|configuration_utils.py:318] 2021-05-04 12:53:12,830 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-75000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 12:53:14,678 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-75000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 12:53:14,683 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-75000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 12:53:14,686 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-75000/special_tokens_map.json\n","{'loss': 0.052, 'learning_rate': 3.915011640272469e-05, 'epoch': 1.08}\n","{'loss': 0.0491, 'learning_rate': 3.90782628689679e-05, 'epoch': 1.09}\n","{'loss': 0.0573, 'learning_rate': 3.900640933521111e-05, 'epoch': 1.1}\n","{'loss': 0.0616, 'learning_rate': 3.893455580145432e-05, 'epoch': 1.11}\n","{'loss': 0.0531, 'learning_rate': 3.886270226769753e-05, 'epoch': 1.11}\n","{'loss': 0.0552, 'learning_rate': 3.879084873394074e-05, 'epoch': 1.12}\n","{'loss': 0.0541, 'learning_rate': 3.871899520018395e-05, 'epoch': 1.13}\n","{'loss': 0.0517, 'learning_rate': 3.8647141666427156e-05, 'epoch': 1.14}\n","{'loss': 0.0555, 'learning_rate': 3.857528813267037e-05, 'epoch': 1.14}\n","{'loss': 0.0564, 'learning_rate': 3.850343459891358e-05, 'epoch': 1.15}\n","{'loss': 0.0554, 'learning_rate': 3.8431581065156784e-05, 'epoch': 1.16}\n","{'loss': 0.057, 'learning_rate': 3.8359727531399996e-05, 'epoch': 1.16}\n","{'loss': 0.0566, 'learning_rate': 3.82878739976432e-05, 'epoch': 1.17}\n","{'loss': 0.0557, 'learning_rate': 3.821602046388641e-05, 'epoch': 1.18}\n","{'loss': 0.0582, 'learning_rate': 3.8144166930129624e-05, 'epoch': 1.19}\n","{'loss': 0.0573, 'learning_rate': 3.8072313396372836e-05, 'epoch': 1.19}\n","{'loss': 0.0647, 'learning_rate': 3.800045986261605e-05, 'epoch': 1.2}\n","{'loss': 0.0566, 'learning_rate': 3.792860632885926e-05, 'epoch': 1.21}\n","{'loss': 0.054, 'learning_rate': 3.7856752795102464e-05, 'epoch': 1.21}\n","{'loss': 0.0588, 'learning_rate': 3.7784899261345675e-05, 'epoch': 1.22}\n","{'loss': 0.0553, 'learning_rate': 3.771304572758889e-05, 'epoch': 1.23}\n","{'loss': 0.0534, 'learning_rate': 3.764119219383209e-05, 'epoch': 1.24}\n","{'loss': 0.0581, 'learning_rate': 3.7569338660075303e-05, 'epoch': 1.24}\n","{'loss': 0.0574, 'learning_rate': 3.7497485126318515e-05, 'epoch': 1.25}\n","{'loss': 0.0563, 'learning_rate': 3.742563159256172e-05, 'epoch': 1.26}\n","{'loss': 0.052, 'learning_rate': 3.735377805880493e-05, 'epoch': 1.26}\n","{'loss': 0.0536, 'learning_rate': 3.728192452504814e-05, 'epoch': 1.27}\n","{'loss': 0.0585, 'learning_rate': 3.7210070991291355e-05, 'epoch': 1.28}\n","{'loss': 0.0497, 'learning_rate': 3.7138217457534566e-05, 'epoch': 1.29}\n","{'loss': 0.0549, 'learning_rate': 3.706636392377778e-05, 'epoch': 1.29}\n"," 26% 90000/347930 [3:00:27<8:11:38,  8.74it/s][INFO|trainer.py:1600] 2021-05-04 13:23:44,250 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-90000\n","[INFO|configuration_utils.py:318] 2021-05-04 13:23:44,267 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-90000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 13:23:45,991 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-90000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 13:23:45,995 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-90000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 13:23:45,999 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-90000/special_tokens_map.json\n","{'loss': 0.0593, 'learning_rate': 3.699451039002098e-05, 'epoch': 1.3}\n","{'loss': 0.0583, 'learning_rate': 3.6922656856264195e-05, 'epoch': 1.31}\n","{'loss': 0.0559, 'learning_rate': 3.6850803322507406e-05, 'epoch': 1.31}\n","{'loss': 0.0598, 'learning_rate': 3.677894978875061e-05, 'epoch': 1.32}\n","{'loss': 0.0584, 'learning_rate': 3.670709625499382e-05, 'epoch': 1.33}\n","{'loss': 0.0546, 'learning_rate': 3.6635242721237034e-05, 'epoch': 1.34}\n","{'loss': 0.0546, 'learning_rate': 3.656338918748024e-05, 'epoch': 1.34}\n","{'loss': 0.0525, 'learning_rate': 3.649153565372345e-05, 'epoch': 1.35}\n","{'loss': 0.0517, 'learning_rate': 3.641968211996666e-05, 'epoch': 1.36}\n","{'loss': 0.053, 'learning_rate': 3.634782858620987e-05, 'epoch': 1.37}\n","{'loss': 0.0504, 'learning_rate': 3.627597505245308e-05, 'epoch': 1.37}\n","{'loss': 0.0538, 'learning_rate': 3.620412151869629e-05, 'epoch': 1.38}\n","{'loss': 0.0591, 'learning_rate': 3.61322679849395e-05, 'epoch': 1.39}\n","{'loss': 0.0529, 'learning_rate': 3.6060414451182714e-05, 'epoch': 1.39}\n","{'loss': 0.0513, 'learning_rate': 3.5988560917425925e-05, 'epoch': 1.4}\n","{'loss': 0.0564, 'learning_rate': 3.591670738366913e-05, 'epoch': 1.41}\n","{'loss': 0.0546, 'learning_rate': 3.584485384991234e-05, 'epoch': 1.42}\n","{'loss': 0.0565, 'learning_rate': 3.5773000316155554e-05, 'epoch': 1.42}\n","{'loss': 0.0547, 'learning_rate': 3.570114678239876e-05, 'epoch': 1.43}\n","{'loss': 0.0548, 'learning_rate': 3.562929324864197e-05, 'epoch': 1.44}\n","{'loss': 0.0508, 'learning_rate': 3.555743971488518e-05, 'epoch': 1.44}\n","{'loss': 0.0542, 'learning_rate': 3.5485586181128387e-05, 'epoch': 1.45}\n","{'loss': 0.056, 'learning_rate': 3.54137326473716e-05, 'epoch': 1.46}\n","{'loss': 0.0549, 'learning_rate': 3.534187911361481e-05, 'epoch': 1.47}\n","{'loss': 0.0566, 'learning_rate': 3.527002557985802e-05, 'epoch': 1.47}\n","{'loss': 0.0532, 'learning_rate': 3.519817204610123e-05, 'epoch': 1.48}\n","{'loss': 0.052, 'learning_rate': 3.512631851234444e-05, 'epoch': 1.49}\n","{'loss': 0.0616, 'learning_rate': 3.505446497858765e-05, 'epoch': 1.49}\n","{'loss': 0.0508, 'learning_rate': 3.498261144483086e-05, 'epoch': 1.5}\n","{'loss': 0.0568, 'learning_rate': 3.4910757911074066e-05, 'epoch': 1.51}\n"," 30% 105000/347930 [3:30:58<8:06:16,  8.33it/s][INFO|trainer.py:1600] 2021-05-04 13:54:15,317 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-105000\n","[INFO|configuration_utils.py:318] 2021-05-04 13:54:15,336 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-105000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 13:54:17,124 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-105000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 13:54:17,128 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-105000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 13:54:17,132 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-105000/special_tokens_map.json\n","{'loss': 0.0572, 'learning_rate': 3.483890437731728e-05, 'epoch': 1.52}\n","{'loss': 0.0536, 'learning_rate': 3.476705084356049e-05, 'epoch': 1.52}\n","{'loss': 0.0546, 'learning_rate': 3.4695197309803694e-05, 'epoch': 1.53}\n","{'loss': 0.0554, 'learning_rate': 3.4623343776046906e-05, 'epoch': 1.54}\n","{'loss': 0.0562, 'learning_rate': 3.455149024229012e-05, 'epoch': 1.54}\n","{'loss': 0.0588, 'learning_rate': 3.447963670853332e-05, 'epoch': 1.55}\n","{'loss': 0.0533, 'learning_rate': 3.4407783174776534e-05, 'epoch': 1.56}\n","{'loss': 0.0574, 'learning_rate': 3.4335929641019746e-05, 'epoch': 1.57}\n","{'loss': 0.0587, 'learning_rate': 3.426407610726296e-05, 'epoch': 1.57}\n","{'loss': 0.057, 'learning_rate': 3.419222257350617e-05, 'epoch': 1.58}\n","{'loss': 0.0583, 'learning_rate': 3.412036903974938e-05, 'epoch': 1.59}\n","{'loss': 0.0612, 'learning_rate': 3.4048515505992585e-05, 'epoch': 1.6}\n","{'loss': 0.0545, 'learning_rate': 3.39766619722358e-05, 'epoch': 1.6}\n","{'loss': 0.0549, 'learning_rate': 3.390480843847901e-05, 'epoch': 1.61}\n","{'loss': 0.0513, 'learning_rate': 3.3832954904722214e-05, 'epoch': 1.62}\n","{'loss': 0.0642, 'learning_rate': 3.3761101370965425e-05, 'epoch': 1.62}\n","{'loss': 0.053, 'learning_rate': 3.368924783720864e-05, 'epoch': 1.63}\n","{'loss': 0.0559, 'learning_rate': 3.361739430345184e-05, 'epoch': 1.64}\n","{'loss': 0.0517, 'learning_rate': 3.354554076969505e-05, 'epoch': 1.65}\n","{'loss': 0.0556, 'learning_rate': 3.3473687235938265e-05, 'epoch': 1.65}\n","{'loss': 0.0515, 'learning_rate': 3.340183370218147e-05, 'epoch': 1.66}\n","{'loss': 0.0543, 'learning_rate': 3.332998016842469e-05, 'epoch': 1.67}\n","{'loss': 0.0574, 'learning_rate': 3.32581266346679e-05, 'epoch': 1.67}\n","{'loss': 0.0496, 'learning_rate': 3.3186273100911105e-05, 'epoch': 1.68}\n","{'loss': 0.056, 'learning_rate': 3.3114419567154316e-05, 'epoch': 1.69}\n","{'loss': 0.0504, 'learning_rate': 3.304256603339753e-05, 'epoch': 1.7}\n","{'loss': 0.0565, 'learning_rate': 3.297071249964073e-05, 'epoch': 1.7}\n","{'loss': 0.055, 'learning_rate': 3.2898858965883944e-05, 'epoch': 1.71}\n","{'loss': 0.0563, 'learning_rate': 3.2827005432127156e-05, 'epoch': 1.72}\n","{'loss': 0.0541, 'learning_rate': 3.275515189837036e-05, 'epoch': 1.72}\n"," 34% 120000/347930 [4:01:23<7:44:57,  8.17it/s][INFO|trainer.py:1600] 2021-05-04 14:24:40,359 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-120000\n","[INFO|configuration_utils.py:318] 2021-05-04 14:24:40,364 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-120000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 14:24:42,161 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-120000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 14:24:42,166 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-120000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 14:24:42,169 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-120000/special_tokens_map.json\n","{'loss': 0.0552, 'learning_rate': 3.268329836461357e-05, 'epoch': 1.73}\n","{'loss': 0.0557, 'learning_rate': 3.2611444830856784e-05, 'epoch': 1.74}\n","{'loss': 0.0515, 'learning_rate': 3.253959129709999e-05, 'epoch': 1.75}\n","{'loss': 0.0536, 'learning_rate': 3.24677377633432e-05, 'epoch': 1.75}\n","{'loss': 0.0582, 'learning_rate': 3.239588422958641e-05, 'epoch': 1.76}\n","{'loss': 0.0552, 'learning_rate': 3.2324030695829624e-05, 'epoch': 1.77}\n","{'loss': 0.0542, 'learning_rate': 3.2252177162072836e-05, 'epoch': 1.77}\n","{'loss': 0.0554, 'learning_rate': 3.218032362831605e-05, 'epoch': 1.78}\n","{'loss': 0.0553, 'learning_rate': 3.210847009455925e-05, 'epoch': 1.79}\n","{'loss': 0.0518, 'learning_rate': 3.2036616560802464e-05, 'epoch': 1.8}\n","{'loss': 0.0594, 'learning_rate': 3.1964763027045675e-05, 'epoch': 1.8}\n","{'loss': 0.0611, 'learning_rate': 3.189290949328888e-05, 'epoch': 1.81}\n","{'loss': 0.0538, 'learning_rate': 3.182105595953209e-05, 'epoch': 1.82}\n","{'loss': 0.0549, 'learning_rate': 3.17492024257753e-05, 'epoch': 1.83}\n","{'loss': 0.0554, 'learning_rate': 3.167734889201851e-05, 'epoch': 1.83}\n","{'loss': 0.0559, 'learning_rate': 3.160549535826172e-05, 'epoch': 1.84}\n","{'loss': 0.0653, 'learning_rate': 3.1533641824504925e-05, 'epoch': 1.85}\n","{'loss': 0.0522, 'learning_rate': 3.1461788290748136e-05, 'epoch': 1.85}\n","{'loss': 0.0538, 'learning_rate': 3.1389934756991355e-05, 'epoch': 1.86}\n","{'loss': 0.0563, 'learning_rate': 3.131808122323456e-05, 'epoch': 1.87}\n","{'loss': 0.0532, 'learning_rate': 3.124622768947777e-05, 'epoch': 1.88}\n","{'loss': 0.0535, 'learning_rate': 3.117437415572098e-05, 'epoch': 1.88}\n","{'loss': 0.0581, 'learning_rate': 3.110252062196419e-05, 'epoch': 1.89}\n","{'loss': 0.0549, 'learning_rate': 3.10306670882074e-05, 'epoch': 1.9}\n","{'loss': 0.0596, 'learning_rate': 3.095881355445061e-05, 'epoch': 1.9}\n","{'loss': 0.0529, 'learning_rate': 3.0886960020693816e-05, 'epoch': 1.91}\n","{'loss': 0.0496, 'learning_rate': 3.081510648693703e-05, 'epoch': 1.92}\n","{'loss': 0.0528, 'learning_rate': 3.074325295318024e-05, 'epoch': 1.93}\n","{'loss': 0.0566, 'learning_rate': 3.0671399419423444e-05, 'epoch': 1.93}\n","{'loss': 0.058, 'learning_rate': 3.0599545885666656e-05, 'epoch': 1.94}\n"," 39% 135000/347930 [4:31:18<6:28:31,  9.13it/s][INFO|trainer.py:1600] 2021-05-04 14:54:35,168 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-135000\n","[INFO|configuration_utils.py:318] 2021-05-04 14:54:35,173 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-135000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 14:54:36,855 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-135000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 14:54:37,745 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-135000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 14:54:37,749 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-135000/special_tokens_map.json\n","{'loss': 0.0561, 'learning_rate': 3.052769235190987e-05, 'epoch': 1.95}\n","{'loss': 0.0542, 'learning_rate': 3.0455838818153076e-05, 'epoch': 1.95}\n","{'loss': 0.0566, 'learning_rate': 3.0383985284396287e-05, 'epoch': 1.96}\n","{'loss': 0.0607, 'learning_rate': 3.03121317506395e-05, 'epoch': 1.97}\n","{'loss': 0.0579, 'learning_rate': 3.0240278216882707e-05, 'epoch': 1.98}\n","{'loss': 0.0597, 'learning_rate': 3.016842468312592e-05, 'epoch': 1.98}\n","{'loss': 0.0554, 'learning_rate': 3.009657114936913e-05, 'epoch': 1.99}\n","{'loss': 0.055, 'learning_rate': 3.0024717615612335e-05, 'epoch': 2.0}\n","{'loss': 0.0576, 'learning_rate': 2.9952864081855547e-05, 'epoch': 2.0}\n","{'loss': 0.0583, 'learning_rate': 2.988101054809876e-05, 'epoch': 2.01}\n","{'loss': 0.0645, 'learning_rate': 2.9809157014341967e-05, 'epoch': 2.02}\n","{'loss': 0.0575, 'learning_rate': 2.973730348058518e-05, 'epoch': 2.03}\n","{'loss': 0.0517, 'learning_rate': 2.966544994682839e-05, 'epoch': 2.03}\n","{'loss': 0.0598, 'learning_rate': 2.9593596413071595e-05, 'epoch': 2.04}\n","{'loss': 0.0573, 'learning_rate': 2.9521742879314807e-05, 'epoch': 2.05}\n","{'loss': 0.056, 'learning_rate': 2.9449889345558018e-05, 'epoch': 2.06}\n","{'loss': 0.0493, 'learning_rate': 2.9378035811801223e-05, 'epoch': 2.06}\n","{'loss': 0.0549, 'learning_rate': 2.9306182278044435e-05, 'epoch': 2.07}\n","{'loss': 0.0541, 'learning_rate': 2.9234328744287646e-05, 'epoch': 2.08}\n","{'loss': 0.0581, 'learning_rate': 2.9162475210530855e-05, 'epoch': 2.08}\n","{'loss': 0.0624, 'learning_rate': 2.9090621676774066e-05, 'epoch': 2.09}\n","{'loss': 0.0543, 'learning_rate': 2.9018768143017278e-05, 'epoch': 2.1}\n","{'loss': 0.0551, 'learning_rate': 2.8946914609260483e-05, 'epoch': 2.11}\n","{'loss': 0.0604, 'learning_rate': 2.8875061075503694e-05, 'epoch': 2.11}\n","{'loss': 0.0486, 'learning_rate': 2.8803207541746906e-05, 'epoch': 2.12}\n","{'loss': 0.0611, 'learning_rate': 2.8731354007990114e-05, 'epoch': 2.13}\n","{'loss': 0.0588, 'learning_rate': 2.8659500474233326e-05, 'epoch': 2.13}\n","{'loss': 0.0607, 'learning_rate': 2.858764694047653e-05, 'epoch': 2.14}\n","{'loss': 0.0535, 'learning_rate': 2.8515793406719742e-05, 'epoch': 2.15}\n","{'loss': 0.0562, 'learning_rate': 2.8443939872962954e-05, 'epoch': 2.16}\n"," 43% 150000/347930 [5:00:55<6:55:36,  7.94it/s][INFO|trainer.py:1600] 2021-05-04 15:24:12,194 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-150000\n","[INFO|configuration_utils.py:318] 2021-05-04 15:24:12,200 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-150000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 15:24:13,945 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-150000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 15:24:13,951 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-150000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 15:24:13,954 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-150000/special_tokens_map.json\n","{'loss': 0.0474, 'learning_rate': 2.8372086339206162e-05, 'epoch': 2.16}\n","{'loss': 0.0533, 'learning_rate': 2.8300232805449374e-05, 'epoch': 2.17}\n","{'loss': 0.0549, 'learning_rate': 2.8228379271692585e-05, 'epoch': 2.18}\n","{'loss': 0.0558, 'learning_rate': 2.815652573793579e-05, 'epoch': 2.18}\n","{'loss': 0.055, 'learning_rate': 2.8084672204179002e-05, 'epoch': 2.19}\n","{'loss': 0.0496, 'learning_rate': 2.8012818670422214e-05, 'epoch': 2.2}\n","{'loss': 0.0571, 'learning_rate': 2.794096513666542e-05, 'epoch': 2.21}\n","{'loss': 0.0536, 'learning_rate': 2.7869111602908633e-05, 'epoch': 2.21}\n","{'loss': 0.0519, 'learning_rate': 2.7797258069151845e-05, 'epoch': 2.22}\n","{'loss': 0.0586, 'learning_rate': 2.772540453539505e-05, 'epoch': 2.23}\n","{'loss': 0.0552, 'learning_rate': 2.765355100163826e-05, 'epoch': 2.23}\n","{'loss': 0.0592, 'learning_rate': 2.7581697467881473e-05, 'epoch': 2.24}\n","{'loss': 0.0548, 'learning_rate': 2.7509843934124678e-05, 'epoch': 2.25}\n","{'loss': 0.0615, 'learning_rate': 2.743799040036789e-05, 'epoch': 2.26}\n","{'loss': 0.0505, 'learning_rate': 2.73661368666111e-05, 'epoch': 2.26}\n","{'loss': 0.0557, 'learning_rate': 2.729428333285431e-05, 'epoch': 2.27}\n","{'loss': 0.0604, 'learning_rate': 2.722242979909752e-05, 'epoch': 2.28}\n","{'loss': 0.0563, 'learning_rate': 2.7150576265340733e-05, 'epoch': 2.28}\n","{'loss': 0.0574, 'learning_rate': 2.7078722731583938e-05, 'epoch': 2.29}\n","{'loss': 0.05, 'learning_rate': 2.700686919782715e-05, 'epoch': 2.3}\n","{'loss': 0.0538, 'learning_rate': 2.693501566407036e-05, 'epoch': 2.31}\n","{'loss': 0.0516, 'learning_rate': 2.686316213031357e-05, 'epoch': 2.31}\n","{'loss': 0.0594, 'learning_rate': 2.679130859655678e-05, 'epoch': 2.32}\n","{'loss': 0.0531, 'learning_rate': 2.6719455062799993e-05, 'epoch': 2.33}\n","{'loss': 0.0558, 'learning_rate': 2.6647601529043197e-05, 'epoch': 2.34}\n","{'loss': 0.0543, 'learning_rate': 2.657574799528641e-05, 'epoch': 2.34}\n","{'loss': 0.0526, 'learning_rate': 2.650389446152962e-05, 'epoch': 2.35}\n","{'loss': 0.0562, 'learning_rate': 2.643204092777283e-05, 'epoch': 2.36}\n","{'loss': 0.052, 'learning_rate': 2.636018739401604e-05, 'epoch': 2.36}\n","{'loss': 0.0512, 'learning_rate': 2.6288333860259252e-05, 'epoch': 2.37}\n"," 47% 165000/347930 [5:30:40<6:19:00,  8.04it/s][INFO|trainer.py:1600] 2021-05-04 15:53:56,768 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-165000\n","[INFO|configuration_utils.py:318] 2021-05-04 15:53:56,777 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-165000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 15:53:58,582 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-165000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 15:53:58,586 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-165000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 15:53:58,590 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-165000/special_tokens_map.json\n","{'loss': 0.055, 'learning_rate': 2.6216480326502457e-05, 'epoch': 2.38}\n","{'loss': 0.0541, 'learning_rate': 2.614462679274567e-05, 'epoch': 2.39}\n","{'loss': 0.0566, 'learning_rate': 2.607277325898888e-05, 'epoch': 2.39}\n","{'loss': 0.0551, 'learning_rate': 2.600091972523209e-05, 'epoch': 2.4}\n","{'loss': 0.0526, 'learning_rate': 2.59290661914753e-05, 'epoch': 2.41}\n","{'loss': 0.059, 'learning_rate': 2.5857212657718512e-05, 'epoch': 2.41}\n","{'loss': 0.0524, 'learning_rate': 2.5785359123961717e-05, 'epoch': 2.42}\n","{'loss': 0.0568, 'learning_rate': 2.5713505590204928e-05, 'epoch': 2.43}\n","{'loss': 0.0574, 'learning_rate': 2.564165205644814e-05, 'epoch': 2.44}\n","{'loss': 0.0541, 'learning_rate': 2.5569798522691345e-05, 'epoch': 2.44}\n","{'loss': 0.0568, 'learning_rate': 2.5497944988934556e-05, 'epoch': 2.45}\n","{'loss': 0.0558, 'learning_rate': 2.5426091455177768e-05, 'epoch': 2.46}\n","{'loss': 0.0591, 'learning_rate': 2.5354237921420976e-05, 'epoch': 2.46}\n","{'loss': 0.0582, 'learning_rate': 2.5282384387664188e-05, 'epoch': 2.47}\n","{'loss': 0.0574, 'learning_rate': 2.5210530853907393e-05, 'epoch': 2.48}\n","{'loss': 0.0558, 'learning_rate': 2.5138677320150604e-05, 'epoch': 2.49}\n","{'loss': 0.0465, 'learning_rate': 2.5066823786393816e-05, 'epoch': 2.49}\n","{'loss': 0.0577, 'learning_rate': 2.4994970252637028e-05, 'epoch': 2.5}\n","{'loss': 0.0558, 'learning_rate': 2.4923116718880236e-05, 'epoch': 2.51}\n","{'loss': 0.0532, 'learning_rate': 2.4851263185123448e-05, 'epoch': 2.51}\n","{'loss': 0.0587, 'learning_rate': 2.4779409651366656e-05, 'epoch': 2.52}\n","{'loss': 0.0567, 'learning_rate': 2.4707556117609864e-05, 'epoch': 2.53}\n","{'loss': 0.0486, 'learning_rate': 2.4635702583853076e-05, 'epoch': 2.54}\n","{'loss': 0.0548, 'learning_rate': 2.4563849050096284e-05, 'epoch': 2.54}\n","{'loss': 0.0533, 'learning_rate': 2.4491995516339496e-05, 'epoch': 2.55}\n","{'loss': 0.0595, 'learning_rate': 2.4420141982582704e-05, 'epoch': 2.56}\n","{'loss': 0.0501, 'learning_rate': 2.4348288448825915e-05, 'epoch': 2.57}\n","{'loss': 0.0512, 'learning_rate': 2.4276434915069124e-05, 'epoch': 2.57}\n","{'loss': 0.0497, 'learning_rate': 2.4204581381312332e-05, 'epoch': 2.58}\n","{'loss': 0.056, 'learning_rate': 2.4132727847555544e-05, 'epoch': 2.59}\n"," 52% 180000/347930 [6:00:06<5:41:47,  8.19it/s][INFO|trainer.py:1600] 2021-05-04 16:23:23,307 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-180000\n","[INFO|configuration_utils.py:318] 2021-05-04 16:23:23,319 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-180000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 16:23:25,053 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-180000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 16:23:25,058 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-180000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 16:23:25,062 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-180000/special_tokens_map.json\n","{'loss': 0.0521, 'learning_rate': 2.4060874313798755e-05, 'epoch': 2.59}\n","{'loss': 0.0527, 'learning_rate': 2.3989020780041963e-05, 'epoch': 2.6}\n","{'loss': 0.0553, 'learning_rate': 2.3917167246285175e-05, 'epoch': 2.61}\n","{'loss': 0.0539, 'learning_rate': 2.3845313712528383e-05, 'epoch': 2.62}\n","{'loss': 0.0537, 'learning_rate': 2.377346017877159e-05, 'epoch': 2.62}\n","{'loss': 0.0584, 'learning_rate': 2.3701606645014803e-05, 'epoch': 2.63}\n","{'loss': 0.0537, 'learning_rate': 2.362975311125801e-05, 'epoch': 2.64}\n","{'loss': 0.0571, 'learning_rate': 2.3557899577501223e-05, 'epoch': 2.64}\n","{'loss': 0.0546, 'learning_rate': 2.3486046043744435e-05, 'epoch': 2.65}\n","{'loss': 0.0579, 'learning_rate': 2.3414192509987643e-05, 'epoch': 2.66}\n","{'loss': 0.0604, 'learning_rate': 2.334233897623085e-05, 'epoch': 2.67}\n","{'loss': 0.0606, 'learning_rate': 2.3270485442474063e-05, 'epoch': 2.67}\n","{'loss': 0.0512, 'learning_rate': 2.319863190871727e-05, 'epoch': 2.68}\n","{'loss': 0.0589, 'learning_rate': 2.312677837496048e-05, 'epoch': 2.69}\n","{'loss': 0.0542, 'learning_rate': 2.3054924841203694e-05, 'epoch': 2.69}\n","{'loss': 0.0602, 'learning_rate': 2.2983071307446903e-05, 'epoch': 2.7}\n","{'loss': 0.0494, 'learning_rate': 2.291121777369011e-05, 'epoch': 2.71}\n","{'loss': 0.0535, 'learning_rate': 2.283936423993332e-05, 'epoch': 2.72}\n","{'loss': 0.0571, 'learning_rate': 2.276751070617653e-05, 'epoch': 2.72}\n","{'loss': 0.0541, 'learning_rate': 2.269565717241974e-05, 'epoch': 2.73}\n","{'loss': 0.0537, 'learning_rate': 2.262380363866295e-05, 'epoch': 2.74}\n","{'loss': 0.0571, 'learning_rate': 2.2551950104906162e-05, 'epoch': 2.74}\n","{'loss': 0.0587, 'learning_rate': 2.248009657114937e-05, 'epoch': 2.75}\n","{'loss': 0.0545, 'learning_rate': 2.240824303739258e-05, 'epoch': 2.76}\n","{'loss': 0.0598, 'learning_rate': 2.233638950363579e-05, 'epoch': 2.77}\n","{'loss': 0.0544, 'learning_rate': 2.2264535969879e-05, 'epoch': 2.77}\n","{'loss': 0.0544, 'learning_rate': 2.2192682436122207e-05, 'epoch': 2.78}\n","{'loss': 0.0517, 'learning_rate': 2.2120828902365422e-05, 'epoch': 2.79}\n","{'loss': 0.0596, 'learning_rate': 2.204897536860863e-05, 'epoch': 2.8}\n","{'loss': 0.0535, 'learning_rate': 2.197712183485184e-05, 'epoch': 2.8}\n"," 56% 195000/347930 [6:29:59<5:35:15,  7.60it/s][INFO|trainer.py:1600] 2021-05-04 16:53:16,321 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-195000\n","[INFO|configuration_utils.py:318] 2021-05-04 16:53:16,327 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-195000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 16:53:18,090 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-195000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 16:53:18,095 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-195000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 16:53:18,099 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-195000/special_tokens_map.json\n","{'loss': 0.0562, 'learning_rate': 2.190526830109505e-05, 'epoch': 2.81}\n","{'loss': 0.0557, 'learning_rate': 2.1833414767338258e-05, 'epoch': 2.82}\n","{'loss': 0.0558, 'learning_rate': 2.1761561233581467e-05, 'epoch': 2.82}\n","{'loss': 0.0562, 'learning_rate': 2.1689707699824678e-05, 'epoch': 2.83}\n","{'loss': 0.0563, 'learning_rate': 2.161785416606789e-05, 'epoch': 2.84}\n","{'loss': 0.05, 'learning_rate': 2.1546000632311098e-05, 'epoch': 2.85}\n","{'loss': 0.0565, 'learning_rate': 2.147414709855431e-05, 'epoch': 2.85}\n","{'loss': 0.0502, 'learning_rate': 2.1402293564797518e-05, 'epoch': 2.86}\n","{'loss': 0.0563, 'learning_rate': 2.1330440031040726e-05, 'epoch': 2.87}\n","{'loss': 0.053, 'learning_rate': 2.1258586497283938e-05, 'epoch': 2.87}\n","{'loss': 0.0597, 'learning_rate': 2.1186732963527146e-05, 'epoch': 2.88}\n","{'loss': 0.0607, 'learning_rate': 2.1114879429770358e-05, 'epoch': 2.89}\n","{'loss': 0.0499, 'learning_rate': 2.1043025896013566e-05, 'epoch': 2.9}\n","{'loss': 0.0588, 'learning_rate': 2.0971172362256778e-05, 'epoch': 2.9}\n","{'loss': 0.0515, 'learning_rate': 2.0899318828499986e-05, 'epoch': 2.91}\n","{'loss': 0.0531, 'learning_rate': 2.0827465294743194e-05, 'epoch': 2.92}\n","{'loss': 0.0596, 'learning_rate': 2.0755611760986406e-05, 'epoch': 2.92}\n","{'loss': 0.0548, 'learning_rate': 2.0683758227229617e-05, 'epoch': 2.93}\n","{'loss': 0.0544, 'learning_rate': 2.0611904693472826e-05, 'epoch': 2.94}\n","{'loss': 0.0538, 'learning_rate': 2.0540051159716037e-05, 'epoch': 2.95}\n","{'loss': 0.0525, 'learning_rate': 2.0468197625959245e-05, 'epoch': 2.95}\n","{'loss': 0.0541, 'learning_rate': 2.0396344092202454e-05, 'epoch': 2.96}\n","{'loss': 0.055, 'learning_rate': 2.0324490558445665e-05, 'epoch': 2.97}\n","{'loss': 0.0593, 'learning_rate': 2.0252637024688874e-05, 'epoch': 2.97}\n","{'loss': 0.0632, 'learning_rate': 2.0180783490932085e-05, 'epoch': 2.98}\n","{'loss': 0.0589, 'learning_rate': 2.0108929957175297e-05, 'epoch': 2.99}\n","{'loss': 0.0534, 'learning_rate': 2.0037076423418505e-05, 'epoch': 3.0}\n","{'loss': 0.0548, 'learning_rate': 1.9965222889661713e-05, 'epoch': 3.0}\n","{'loss': 0.0562, 'learning_rate': 1.9893369355904925e-05, 'epoch': 3.01}\n","{'loss': 0.0535, 'learning_rate': 1.9821515822148133e-05, 'epoch': 3.02}\n"," 60% 210000/347930 [7:00:17<4:24:21,  8.70it/s][INFO|trainer.py:1600] 2021-05-04 17:23:34,005 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-210000\n","[INFO|configuration_utils.py:318] 2021-05-04 17:23:34,010 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-210000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 17:23:35,767 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-210000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 17:23:35,774 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-210000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 17:23:35,783 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-210000/special_tokens_map.json\n","{'loss': 0.0519, 'learning_rate': 1.9749662288391345e-05, 'epoch': 3.03}\n","{'loss': 0.0558, 'learning_rate': 1.9677808754634557e-05, 'epoch': 3.03}\n","{'loss': 0.0603, 'learning_rate': 1.9605955220877765e-05, 'epoch': 3.04}\n","{'loss': 0.0629, 'learning_rate': 1.9534101687120973e-05, 'epoch': 3.05}\n","{'loss': 0.0554, 'learning_rate': 1.946224815336418e-05, 'epoch': 3.05}\n","{'loss': 0.0545, 'learning_rate': 1.9390394619607393e-05, 'epoch': 3.06}\n","{'loss': 0.0506, 'learning_rate': 1.93185410858506e-05, 'epoch': 3.07}\n","{'loss': 0.0555, 'learning_rate': 1.9246687552093813e-05, 'epoch': 3.08}\n","{'loss': 0.0537, 'learning_rate': 1.9174834018337024e-05, 'epoch': 3.08}\n","{'loss': 0.0566, 'learning_rate': 1.9102980484580233e-05, 'epoch': 3.09}\n","{'loss': 0.0555, 'learning_rate': 1.903112695082344e-05, 'epoch': 3.1}\n","{'loss': 0.0505, 'learning_rate': 1.8959273417066653e-05, 'epoch': 3.1}\n","{'loss': 0.0571, 'learning_rate': 1.888741988330986e-05, 'epoch': 3.11}\n","{'loss': 0.0587, 'learning_rate': 1.8815566349553072e-05, 'epoch': 3.12}\n","{'loss': 0.0564, 'learning_rate': 1.8743712815796284e-05, 'epoch': 3.13}\n","{'loss': 0.0583, 'learning_rate': 1.8671859282039492e-05, 'epoch': 3.13}\n","{'loss': 0.054, 'learning_rate': 1.86000057482827e-05, 'epoch': 3.14}\n","{'loss': 0.0587, 'learning_rate': 1.8528152214525912e-05, 'epoch': 3.15}\n","{'loss': 0.0558, 'learning_rate': 1.845629868076912e-05, 'epoch': 3.15}\n","{'loss': 0.0503, 'learning_rate': 1.838444514701233e-05, 'epoch': 3.16}\n","{'loss': 0.0592, 'learning_rate': 1.831259161325554e-05, 'epoch': 3.17}\n","{'loss': 0.0558, 'learning_rate': 1.8240738079498752e-05, 'epoch': 3.18}\n","{'loss': 0.0583, 'learning_rate': 1.816888454574196e-05, 'epoch': 3.18}\n","{'loss': 0.0532, 'learning_rate': 1.8097031011985172e-05, 'epoch': 3.19}\n","{'loss': 0.0575, 'learning_rate': 1.802517747822838e-05, 'epoch': 3.2}\n","{'loss': 0.0557, 'learning_rate': 1.7953323944471588e-05, 'epoch': 3.2}\n","{'loss': 0.0602, 'learning_rate': 1.78814704107148e-05, 'epoch': 3.21}\n","{'loss': 0.0556, 'learning_rate': 1.780961687695801e-05, 'epoch': 3.22}\n","{'loss': 0.0511, 'learning_rate': 1.773776334320122e-05, 'epoch': 3.23}\n","{'loss': 0.0538, 'learning_rate': 1.7665909809444428e-05, 'epoch': 3.23}\n"," 65% 225000/347930 [7:30:11<3:57:39,  8.62it/s][INFO|trainer.py:1600] 2021-05-04 17:53:27,887 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-225000\n","[INFO|configuration_utils.py:318] 2021-05-04 17:53:27,907 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-225000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 17:53:29,673 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-225000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 17:53:29,677 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-225000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 17:53:29,681 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-225000/special_tokens_map.json\n","{'loss': 0.0539, 'learning_rate': 1.759405627568764e-05, 'epoch': 3.24}\n","{'loss': 0.059, 'learning_rate': 1.7522202741930848e-05, 'epoch': 3.25}\n","{'loss': 0.0532, 'learning_rate': 1.7450349208174056e-05, 'epoch': 3.25}\n","{'loss': 0.0571, 'learning_rate': 1.7378495674417268e-05, 'epoch': 3.26}\n","{'loss': 0.0543, 'learning_rate': 1.730664214066048e-05, 'epoch': 3.27}\n","{'loss': 0.0545, 'learning_rate': 1.7234788606903688e-05, 'epoch': 3.28}\n","{'loss': 0.0609, 'learning_rate': 1.71629350731469e-05, 'epoch': 3.28}\n","{'loss': 0.0563, 'learning_rate': 1.7091081539390108e-05, 'epoch': 3.29}\n","{'loss': 0.0553, 'learning_rate': 1.7019228005633316e-05, 'epoch': 3.3}\n","{'loss': 0.0493, 'learning_rate': 1.6947374471876527e-05, 'epoch': 3.31}\n","{'loss': 0.0535, 'learning_rate': 1.687552093811974e-05, 'epoch': 3.31}\n","{'loss': 0.0529, 'learning_rate': 1.6803667404362947e-05, 'epoch': 3.32}\n","{'loss': 0.0544, 'learning_rate': 1.673181387060616e-05, 'epoch': 3.33}\n","{'loss': 0.0556, 'learning_rate': 1.6659960336849367e-05, 'epoch': 3.33}\n","{'loss': 0.054, 'learning_rate': 1.6588106803092575e-05, 'epoch': 3.34}\n","{'loss': 0.0505, 'learning_rate': 1.6516253269335787e-05, 'epoch': 3.35}\n","{'loss': 0.0521, 'learning_rate': 1.6444399735578995e-05, 'epoch': 3.36}\n","{'loss': 0.0524, 'learning_rate': 1.6372546201822207e-05, 'epoch': 3.36}\n","{'loss': 0.0549, 'learning_rate': 1.630069266806542e-05, 'epoch': 3.37}\n","{'loss': 0.0598, 'learning_rate': 1.6228839134308627e-05, 'epoch': 3.38}\n","{'loss': 0.0563, 'learning_rate': 1.6156985600551835e-05, 'epoch': 3.38}\n","{'loss': 0.0568, 'learning_rate': 1.6085132066795043e-05, 'epoch': 3.39}\n","{'loss': 0.0632, 'learning_rate': 1.6013278533038255e-05, 'epoch': 3.4}\n","{'loss': 0.0553, 'learning_rate': 1.5941424999281467e-05, 'epoch': 3.41}\n","{'loss': 0.0487, 'learning_rate': 1.5869571465524675e-05, 'epoch': 3.41}\n","{'loss': 0.0565, 'learning_rate': 1.5797717931767887e-05, 'epoch': 3.42}\n","{'loss': 0.0504, 'learning_rate': 1.5725864398011095e-05, 'epoch': 3.43}\n","{'loss': 0.0504, 'learning_rate': 1.5654010864254303e-05, 'epoch': 3.43}\n","{'loss': 0.0583, 'learning_rate': 1.5582157330497515e-05, 'epoch': 3.44}\n","{'loss': 0.0549, 'learning_rate': 1.5510303796740723e-05, 'epoch': 3.45}\n"," 69% 240000/347930 [7:59:36<3:22:42,  8.87it/s][INFO|trainer.py:1600] 2021-05-04 18:22:53,526 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-240000\n","[INFO|configuration_utils.py:318] 2021-05-04 18:22:53,531 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-240000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 18:22:55,333 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-240000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 18:22:55,337 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-240000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 18:22:55,341 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-240000/special_tokens_map.json\n","{'loss': 0.0614, 'learning_rate': 1.5438450262983935e-05, 'epoch': 3.46}\n","{'loss': 0.0578, 'learning_rate': 1.5366596729227146e-05, 'epoch': 3.46}\n","{'loss': 0.0491, 'learning_rate': 1.5294743195470354e-05, 'epoch': 3.47}\n","{'loss': 0.0569, 'learning_rate': 1.5222889661713563e-05, 'epoch': 3.48}\n","{'loss': 0.0541, 'learning_rate': 1.5151036127956774e-05, 'epoch': 3.48}\n","{'loss': 0.0569, 'learning_rate': 1.5079182594199984e-05, 'epoch': 3.49}\n","{'loss': 0.058, 'learning_rate': 1.5007329060443192e-05, 'epoch': 3.5}\n","{'loss': 0.0523, 'learning_rate': 1.4935475526686404e-05, 'epoch': 3.51}\n","{'loss': 0.0574, 'learning_rate': 1.4863621992929614e-05, 'epoch': 3.51}\n","{'loss': 0.0556, 'learning_rate': 1.4791768459172822e-05, 'epoch': 3.52}\n","{'loss': 0.0544, 'learning_rate': 1.4719914925416034e-05, 'epoch': 3.53}\n","{'loss': 0.055, 'learning_rate': 1.4648061391659242e-05, 'epoch': 3.54}\n","{'loss': 0.0527, 'learning_rate': 1.4576207857902452e-05, 'epoch': 3.54}\n","{'loss': 0.0536, 'learning_rate': 1.450435432414566e-05, 'epoch': 3.55}\n","{'loss': 0.0558, 'learning_rate': 1.4432500790388872e-05, 'epoch': 3.56}\n","{'loss': 0.0533, 'learning_rate': 1.4360647256632082e-05, 'epoch': 3.56}\n","{'loss': 0.0555, 'learning_rate': 1.428879372287529e-05, 'epoch': 3.57}\n","{'loss': 0.0553, 'learning_rate': 1.4216940189118502e-05, 'epoch': 3.58}\n","{'loss': 0.0592, 'learning_rate': 1.4145086655361712e-05, 'epoch': 3.59}\n","{'loss': 0.0558, 'learning_rate': 1.407323312160492e-05, 'epoch': 3.59}\n","{'loss': 0.0562, 'learning_rate': 1.4001379587848132e-05, 'epoch': 3.6}\n","{'loss': 0.0567, 'learning_rate': 1.3929526054091342e-05, 'epoch': 3.61}\n","{'loss': 0.0528, 'learning_rate': 1.385767252033455e-05, 'epoch': 3.61}\n","{'loss': 0.0554, 'learning_rate': 1.3785818986577761e-05, 'epoch': 3.62}\n","{'loss': 0.0579, 'learning_rate': 1.371396545282097e-05, 'epoch': 3.63}\n","{'loss': 0.0541, 'learning_rate': 1.364211191906418e-05, 'epoch': 3.64}\n","{'loss': 0.0579, 'learning_rate': 1.3570258385307391e-05, 'epoch': 3.64}\n","{'loss': 0.05, 'learning_rate': 1.34984048515506e-05, 'epoch': 3.65}\n","{'loss': 0.0552, 'learning_rate': 1.342655131779381e-05, 'epoch': 3.66}\n","{'loss': 0.0602, 'learning_rate': 1.3354697784037021e-05, 'epoch': 3.66}\n"," 73% 255000/347930 [8:29:14<2:55:44,  8.81it/s][INFO|trainer.py:1600] 2021-05-04 18:52:31,653 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-255000\n","[INFO|configuration_utils.py:318] 2021-05-04 18:52:31,658 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-255000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 18:52:33,354 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-255000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 18:52:33,359 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-255000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 18:52:33,363 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-255000/special_tokens_map.json\n","{'loss': 0.0524, 'learning_rate': 1.328284425028023e-05, 'epoch': 3.67}\n","{'loss': 0.0601, 'learning_rate': 1.321099071652344e-05, 'epoch': 3.68}\n","{'loss': 0.0548, 'learning_rate': 1.3139137182766651e-05, 'epoch': 3.69}\n","{'loss': 0.0543, 'learning_rate': 1.3067283649009859e-05, 'epoch': 3.69}\n","{'loss': 0.0533, 'learning_rate': 1.2995430115253069e-05, 'epoch': 3.7}\n","{'loss': 0.0534, 'learning_rate': 1.2923576581496277e-05, 'epoch': 3.71}\n","{'loss': 0.0571, 'learning_rate': 1.2851723047739489e-05, 'epoch': 3.71}\n","{'loss': 0.0516, 'learning_rate': 1.2779869513982697e-05, 'epoch': 3.72}\n","{'loss': 0.0499, 'learning_rate': 1.2708015980225907e-05, 'epoch': 3.73}\n","{'loss': 0.0568, 'learning_rate': 1.2636162446469119e-05, 'epoch': 3.74}\n","{'loss': 0.0573, 'learning_rate': 1.2564308912712327e-05, 'epoch': 3.74}\n","{'loss': 0.051, 'learning_rate': 1.2492455378955539e-05, 'epoch': 3.75}\n","{'loss': 0.0544, 'learning_rate': 1.2420601845198747e-05, 'epoch': 3.76}\n","{'loss': 0.0493, 'learning_rate': 1.2348748311441957e-05, 'epoch': 3.77}\n","{'loss': 0.0578, 'learning_rate': 1.2276894777685167e-05, 'epoch': 3.77}\n","{'loss': 0.0616, 'learning_rate': 1.2205041243928377e-05, 'epoch': 3.78}\n","{'loss': 0.0554, 'learning_rate': 1.2133187710171587e-05, 'epoch': 3.79}\n","{'loss': 0.0543, 'learning_rate': 1.2061334176414797e-05, 'epoch': 3.79}\n","{'loss': 0.0579, 'learning_rate': 1.1989480642658007e-05, 'epoch': 3.8}\n","{'loss': 0.0569, 'learning_rate': 1.1917627108901216e-05, 'epoch': 3.81}\n","{'loss': 0.0542, 'learning_rate': 1.1845773575144426e-05, 'epoch': 3.82}\n","{'loss': 0.0536, 'learning_rate': 1.1773920041387636e-05, 'epoch': 3.82}\n","{'loss': 0.0593, 'learning_rate': 1.1702066507630846e-05, 'epoch': 3.83}\n","{'loss': 0.0527, 'learning_rate': 1.1630212973874055e-05, 'epoch': 3.84}\n","{'loss': 0.0519, 'learning_rate': 1.1558359440117266e-05, 'epoch': 3.84}\n","{'loss': 0.0541, 'learning_rate': 1.1486505906360476e-05, 'epoch': 3.85}\n","{'loss': 0.055, 'learning_rate': 1.1414652372603684e-05, 'epoch': 3.86}\n","{'loss': 0.0518, 'learning_rate': 1.1342798838846894e-05, 'epoch': 3.87}\n","{'loss': 0.0505, 'learning_rate': 1.1270945305090106e-05, 'epoch': 3.87}\n","{'loss': 0.059, 'learning_rate': 1.1199091771333314e-05, 'epoch': 3.88}\n"," 78% 270000/347930 [8:58:41<2:28:37,  8.74it/s][INFO|trainer.py:1600] 2021-05-04 19:21:58,208 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-270000\n","[INFO|configuration_utils.py:318] 2021-05-04 19:21:58,213 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-270000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 19:21:59,968 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-270000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 19:21:59,973 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-270000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 19:21:59,977 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-270000/special_tokens_map.json\n","{'loss': 0.0596, 'learning_rate': 1.1127238237576524e-05, 'epoch': 3.89}\n","{'loss': 0.0538, 'learning_rate': 1.1055384703819736e-05, 'epoch': 3.89}\n","{'loss': 0.0531, 'learning_rate': 1.0983531170062944e-05, 'epoch': 3.9}\n","{'loss': 0.0563, 'learning_rate': 1.0911677636306154e-05, 'epoch': 3.91}\n","{'loss': 0.0579, 'learning_rate': 1.0839824102549364e-05, 'epoch': 3.92}\n","{'loss': 0.0593, 'learning_rate': 1.0767970568792574e-05, 'epoch': 3.92}\n","{'loss': 0.0593, 'learning_rate': 1.0696117035035784e-05, 'epoch': 3.93}\n","{'loss': 0.0505, 'learning_rate': 1.0624263501278994e-05, 'epoch': 3.94}\n","{'loss': 0.057, 'learning_rate': 1.0552409967522204e-05, 'epoch': 3.94}\n","{'loss': 0.0562, 'learning_rate': 1.0480556433765414e-05, 'epoch': 3.95}\n","{'loss': 0.0545, 'learning_rate': 1.0408702900008622e-05, 'epoch': 3.96}\n","{'loss': 0.056, 'learning_rate': 1.0336849366251833e-05, 'epoch': 3.97}\n","{'loss': 0.0557, 'learning_rate': 1.0264995832495043e-05, 'epoch': 3.97}\n","{'loss': 0.0548, 'learning_rate': 1.0193142298738252e-05, 'epoch': 3.98}\n","{'loss': 0.0504, 'learning_rate': 1.0121288764981463e-05, 'epoch': 3.99}\n","{'loss': 0.0585, 'learning_rate': 1.0049435231224672e-05, 'epoch': 4.0}\n","{'loss': 0.0547, 'learning_rate': 9.977581697467881e-06, 'epoch': 4.0}\n","{'loss': 0.0555, 'learning_rate': 9.905728163711091e-06, 'epoch': 4.01}\n","{'loss': 0.0565, 'learning_rate': 9.833874629954301e-06, 'epoch': 4.02}\n","{'loss': 0.057, 'learning_rate': 9.762021096197511e-06, 'epoch': 4.02}\n","{'loss': 0.0581, 'learning_rate': 9.690167562440721e-06, 'epoch': 4.03}\n","{'loss': 0.0577, 'learning_rate': 9.618314028683931e-06, 'epoch': 4.04}\n","{'loss': 0.0556, 'learning_rate': 9.546460494927141e-06, 'epoch': 4.05}\n","{'loss': 0.0506, 'learning_rate': 9.474606961170351e-06, 'epoch': 4.05}\n","{'loss': 0.0533, 'learning_rate': 9.402753427413561e-06, 'epoch': 4.06}\n","{'loss': 0.0537, 'learning_rate': 9.330899893656771e-06, 'epoch': 4.07}\n","{'loss': 0.0564, 'learning_rate': 9.25904635989998e-06, 'epoch': 4.07}\n","{'loss': 0.0609, 'learning_rate': 9.18719282614319e-06, 'epoch': 4.08}\n","{'loss': 0.0579, 'learning_rate': 9.1153392923864e-06, 'epoch': 4.09}\n","{'loss': 0.0523, 'learning_rate': 9.043485758629609e-06, 'epoch': 4.1}\n"," 82% 285000/347930 [9:28:32<1:56:36,  8.99it/s][INFO|trainer.py:1600] 2021-05-04 19:51:49,575 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-285000\n","[INFO|configuration_utils.py:318] 2021-05-04 19:51:49,582 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-285000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 19:51:51,364 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-285000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 19:51:51,369 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-285000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 19:51:51,372 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-285000/special_tokens_map.json\n","{'loss': 0.0514, 'learning_rate': 8.971632224872819e-06, 'epoch': 4.1}\n","{'loss': 0.0564, 'learning_rate': 8.89977869111603e-06, 'epoch': 4.11}\n","{'loss': 0.0564, 'learning_rate': 8.827925157359239e-06, 'epoch': 4.12}\n","{'loss': 0.0538, 'learning_rate': 8.756071623602449e-06, 'epoch': 4.12}\n","{'loss': 0.0502, 'learning_rate': 8.68421808984566e-06, 'epoch': 4.13}\n","{'loss': 0.0506, 'learning_rate': 8.612364556088869e-06, 'epoch': 4.14}\n","{'loss': 0.0517, 'learning_rate': 8.540511022332079e-06, 'epoch': 4.15}\n","{'loss': 0.0564, 'learning_rate': 8.468657488575289e-06, 'epoch': 4.15}\n","{'loss': 0.0527, 'learning_rate': 8.396803954818498e-06, 'epoch': 4.16}\n","{'loss': 0.0561, 'learning_rate': 8.324950421061708e-06, 'epoch': 4.17}\n","{'loss': 0.0569, 'learning_rate': 8.253096887304917e-06, 'epoch': 4.17}\n","{'loss': 0.0538, 'learning_rate': 8.181243353548128e-06, 'epoch': 4.18}\n","{'loss': 0.0547, 'learning_rate': 8.109389819791338e-06, 'epoch': 4.19}\n","{'loss': 0.057, 'learning_rate': 8.037536286034546e-06, 'epoch': 4.2}\n","{'loss': 0.0561, 'learning_rate': 7.965682752277758e-06, 'epoch': 4.2}\n","{'loss': 0.0524, 'learning_rate': 7.893829218520968e-06, 'epoch': 4.21}\n","{'loss': 0.0545, 'learning_rate': 7.821975684764176e-06, 'epoch': 4.22}\n","{'loss': 0.058, 'learning_rate': 7.750122151007386e-06, 'epoch': 4.22}\n","{'loss': 0.0562, 'learning_rate': 7.678268617250598e-06, 'epoch': 4.23}\n","{'loss': 0.057, 'learning_rate': 7.606415083493806e-06, 'epoch': 4.24}\n","{'loss': 0.0551, 'learning_rate': 7.534561549737017e-06, 'epoch': 4.25}\n","{'loss': 0.0598, 'learning_rate': 7.462708015980226e-06, 'epoch': 4.25}\n","{'loss': 0.0519, 'learning_rate': 7.390854482223436e-06, 'epoch': 4.26}\n","{'loss': 0.0581, 'learning_rate': 7.319000948466647e-06, 'epoch': 4.27}\n","{'loss': 0.0524, 'learning_rate': 7.247147414709855e-06, 'epoch': 4.28}\n","{'loss': 0.0592, 'learning_rate': 7.175293880953066e-06, 'epoch': 4.28}\n","{'loss': 0.0544, 'learning_rate': 7.103440347196276e-06, 'epoch': 4.29}\n","{'loss': 0.0552, 'learning_rate': 7.031586813439485e-06, 'epoch': 4.3}\n","{'loss': 0.048, 'learning_rate': 6.959733279682696e-06, 'epoch': 4.3}\n","{'loss': 0.0618, 'learning_rate': 6.8878797459259055e-06, 'epoch': 4.31}\n"," 86% 300000/347930 [9:58:49<1:58:40,  6.73it/s][INFO|trainer.py:1600] 2021-05-04 20:22:05,993 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-300000\n","[INFO|configuration_utils.py:318] 2021-05-04 20:22:05,998 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-300000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 20:22:07,843 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-300000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 20:22:07,849 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-300000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 20:22:07,852 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-300000/special_tokens_map.json\n","{'loss': 0.0584, 'learning_rate': 6.816026212169115e-06, 'epoch': 4.32}\n","{'loss': 0.0549, 'learning_rate': 6.7441726784123246e-06, 'epoch': 4.33}\n","{'loss': 0.0529, 'learning_rate': 6.672319144655534e-06, 'epoch': 4.33}\n","{'loss': 0.0547, 'learning_rate': 6.6004656108987444e-06, 'epoch': 4.34}\n","{'loss': 0.0541, 'learning_rate': 6.528612077141954e-06, 'epoch': 4.35}\n","{'loss': 0.0619, 'learning_rate': 6.4567585433851635e-06, 'epoch': 4.35}\n","{'loss': 0.0556, 'learning_rate': 6.384905009628374e-06, 'epoch': 4.36}\n","{'loss': 0.0525, 'learning_rate': 6.313051475871584e-06, 'epoch': 4.37}\n","{'loss': 0.0568, 'learning_rate': 6.241197942114794e-06, 'epoch': 4.38}\n","{'loss': 0.0574, 'learning_rate': 6.169344408358003e-06, 'epoch': 4.38}\n","{'loss': 0.055, 'learning_rate': 6.097490874601213e-06, 'epoch': 4.39}\n","{'loss': 0.0561, 'learning_rate': 6.025637340844423e-06, 'epoch': 4.4}\n","{'loss': 0.0548, 'learning_rate': 5.953783807087633e-06, 'epoch': 4.4}\n","{'loss': 0.0523, 'learning_rate': 5.881930273330843e-06, 'epoch': 4.41}\n","{'loss': 0.0561, 'learning_rate': 5.810076739574052e-06, 'epoch': 4.42}\n","{'loss': 0.0555, 'learning_rate': 5.738223205817262e-06, 'epoch': 4.43}\n","{'loss': 0.0587, 'learning_rate': 5.666369672060472e-06, 'epoch': 4.43}\n","{'loss': 0.0506, 'learning_rate': 5.594516138303682e-06, 'epoch': 4.44}\n","{'loss': 0.0544, 'learning_rate': 5.522662604546892e-06, 'epoch': 4.45}\n","{'loss': 0.057, 'learning_rate': 5.450809070790102e-06, 'epoch': 4.45}\n","{'loss': 0.0533, 'learning_rate': 5.378955537033312e-06, 'epoch': 4.46}\n","{'loss': 0.0557, 'learning_rate': 5.307102003276522e-06, 'epoch': 4.47}\n","{'loss': 0.0573, 'learning_rate': 5.235248469519731e-06, 'epoch': 4.48}\n","{'loss': 0.053, 'learning_rate': 5.1633949357629416e-06, 'epoch': 4.48}\n","{'loss': 0.0559, 'learning_rate': 5.091541402006151e-06, 'epoch': 4.49}\n","{'loss': 0.0535, 'learning_rate': 5.019687868249361e-06, 'epoch': 4.5}\n","{'loss': 0.0562, 'learning_rate': 4.9478343344925705e-06, 'epoch': 4.51}\n","{'loss': 0.053, 'learning_rate': 4.8759808007357805e-06, 'epoch': 4.51}\n","{'loss': 0.0516, 'learning_rate': 4.80412726697899e-06, 'epoch': 4.52}\n","{'loss': 0.0577, 'learning_rate': 4.7322737332221995e-06, 'epoch': 4.53}\n"," 91% 315000/347930 [10:28:47<1:00:24,  9.09it/s][INFO|trainer.py:1600] 2021-05-04 20:52:04,430 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-315000\n","[INFO|configuration_utils.py:318] 2021-05-04 20:52:04,436 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-315000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 20:52:06,237 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-315000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 20:52:06,242 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-315000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 20:52:06,245 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-315000/special_tokens_map.json\n","{'loss': 0.0581, 'learning_rate': 4.66042019946541e-06, 'epoch': 4.53}\n","{'loss': 0.0573, 'learning_rate': 4.58856666570862e-06, 'epoch': 4.54}\n","{'loss': 0.0596, 'learning_rate': 4.516713131951829e-06, 'epoch': 4.55}\n","{'loss': 0.0531, 'learning_rate': 4.444859598195039e-06, 'epoch': 4.56}\n","{'loss': 0.0483, 'learning_rate': 4.373006064438249e-06, 'epoch': 4.56}\n","{'loss': 0.0611, 'learning_rate': 4.301152530681459e-06, 'epoch': 4.57}\n","{'loss': 0.0496, 'learning_rate': 4.229298996924669e-06, 'epoch': 4.58}\n","{'loss': 0.0509, 'learning_rate': 4.157445463167879e-06, 'epoch': 4.58}\n","{'loss': 0.057, 'learning_rate': 4.085591929411089e-06, 'epoch': 4.59}\n","{'loss': 0.0566, 'learning_rate': 4.013738395654298e-06, 'epoch': 4.6}\n","{'loss': 0.0538, 'learning_rate': 3.941884861897508e-06, 'epoch': 4.61}\n","{'loss': 0.0529, 'learning_rate': 3.870031328140719e-06, 'epoch': 4.61}\n","{'loss': 0.0559, 'learning_rate': 3.798177794383928e-06, 'epoch': 4.62}\n","{'loss': 0.0579, 'learning_rate': 3.726324260627138e-06, 'epoch': 4.63}\n","{'loss': 0.0515, 'learning_rate': 3.6544707268703473e-06, 'epoch': 4.63}\n","{'loss': 0.056, 'learning_rate': 3.5826171931135577e-06, 'epoch': 4.64}\n","{'loss': 0.0555, 'learning_rate': 3.5107636593567672e-06, 'epoch': 4.65}\n","{'loss': 0.0574, 'learning_rate': 3.438910125599977e-06, 'epoch': 4.66}\n","{'loss': 0.0626, 'learning_rate': 3.3670565918431875e-06, 'epoch': 4.66}\n","{'loss': 0.0598, 'learning_rate': 3.295203058086397e-06, 'epoch': 4.67}\n","{'loss': 0.0563, 'learning_rate': 3.2233495243296066e-06, 'epoch': 4.68}\n","{'loss': 0.0536, 'learning_rate': 3.151495990572816e-06, 'epoch': 4.68}\n","{'loss': 0.0542, 'learning_rate': 3.0796424568160264e-06, 'epoch': 4.69}\n","{'loss': 0.0542, 'learning_rate': 3.0077889230592364e-06, 'epoch': 4.7}\n","{'loss': 0.0557, 'learning_rate': 2.935935389302446e-06, 'epoch': 4.71}\n","{'loss': 0.0636, 'learning_rate': 2.864081855545656e-06, 'epoch': 4.71}\n","{'loss': 0.0518, 'learning_rate': 2.7922283217888653e-06, 'epoch': 4.72}\n","{'loss': 0.0499, 'learning_rate': 2.7203747880320757e-06, 'epoch': 4.73}\n","{'loss': 0.0591, 'learning_rate': 2.6485212542752857e-06, 'epoch': 4.74}\n","{'loss': 0.0529, 'learning_rate': 2.576667720518495e-06, 'epoch': 4.74}\n"," 95% 330000/347930 [10:59:05<33:33,  8.90it/s][INFO|trainer.py:1600] 2021-05-04 21:22:22,219 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-330000\n","[INFO|configuration_utils.py:318] 2021-05-04 21:22:22,224 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-330000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 21:22:24,098 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-330000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 21:22:24,103 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-330000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 21:22:24,107 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-330000/special_tokens_map.json\n","{'loss': 0.0571, 'learning_rate': 2.504814186761705e-06, 'epoch': 4.75}\n","{'loss': 0.0574, 'learning_rate': 2.4329606530049146e-06, 'epoch': 4.76}\n","{'loss': 0.0582, 'learning_rate': 2.361107119248125e-06, 'epoch': 4.76}\n","{'loss': 0.0553, 'learning_rate': 2.2892535854913345e-06, 'epoch': 4.77}\n","{'loss': 0.051, 'learning_rate': 2.2174000517345444e-06, 'epoch': 4.78}\n","{'loss': 0.0585, 'learning_rate': 2.145546517977754e-06, 'epoch': 4.79}\n","{'loss': 0.0533, 'learning_rate': 2.073692984220964e-06, 'epoch': 4.79}\n","{'loss': 0.0551, 'learning_rate': 2.0018394504641743e-06, 'epoch': 4.8}\n","{'loss': 0.0533, 'learning_rate': 1.9299859167073838e-06, 'epoch': 4.81}\n","{'loss': 0.0611, 'learning_rate': 1.8581323829505937e-06, 'epoch': 4.81}\n","{'loss': 0.059, 'learning_rate': 1.7862788491938035e-06, 'epoch': 4.82}\n","{'loss': 0.0604, 'learning_rate': 1.7144253154370134e-06, 'epoch': 4.83}\n","{'loss': 0.0522, 'learning_rate': 1.6425717816802231e-06, 'epoch': 4.84}\n","{'loss': 0.0522, 'learning_rate': 1.570718247923433e-06, 'epoch': 4.84}\n","{'loss': 0.0536, 'learning_rate': 1.4988647141666428e-06, 'epoch': 4.85}\n","{'loss': 0.0537, 'learning_rate': 1.4270111804098527e-06, 'epoch': 4.86}\n","{'loss': 0.0575, 'learning_rate': 1.3551576466530625e-06, 'epoch': 4.86}\n","{'loss': 0.0481, 'learning_rate': 1.2833041128962724e-06, 'epoch': 4.87}\n","{'loss': 0.0587, 'learning_rate': 1.2114505791394821e-06, 'epoch': 4.88}\n","{'loss': 0.0505, 'learning_rate': 1.1395970453826919e-06, 'epoch': 4.89}\n","{'loss': 0.0512, 'learning_rate': 1.0677435116259018e-06, 'epoch': 4.89}\n","{'loss': 0.0493, 'learning_rate': 9.958899778691115e-07, 'epoch': 4.9}\n","{'loss': 0.0628, 'learning_rate': 9.240364441123216e-07, 'epoch': 4.91}\n","{'loss': 0.0565, 'learning_rate': 8.521829103555314e-07, 'epoch': 4.91}\n","{'loss': 0.0587, 'learning_rate': 7.803293765987411e-07, 'epoch': 4.92}\n","{'loss': 0.0529, 'learning_rate': 7.084758428419511e-07, 'epoch': 4.93}\n","{'loss': 0.0588, 'learning_rate': 6.366223090851608e-07, 'epoch': 4.94}\n","{'loss': 0.0565, 'learning_rate': 5.647687753283706e-07, 'epoch': 4.94}\n","{'loss': 0.0537, 'learning_rate': 4.929152415715805e-07, 'epoch': 4.95}\n","{'loss': 0.0589, 'learning_rate': 4.210617078147903e-07, 'epoch': 4.96}\n"," 99% 345000/347930 [11:29:41<05:45,  8.47it/s][INFO|trainer.py:1600] 2021-05-04 21:52:58,182 >> Saving model checkpoint to nb005-pytorch-bert-for-ner/checkpoint-345000\n","[INFO|configuration_utils.py:318] 2021-05-04 21:52:58,187 >> Configuration saved in nb005-pytorch-bert-for-ner/checkpoint-345000/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 21:52:59,953 >> Model weights saved in nb005-pytorch-bert-for-ner/checkpoint-345000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 21:52:59,957 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/checkpoint-345000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 21:52:59,961 >> Special tokens file saved in nb005-pytorch-bert-for-ner/checkpoint-345000/special_tokens_map.json\n","{'loss': 0.0523, 'learning_rate': 3.492081740580002e-07, 'epoch': 4.97}\n","{'loss': 0.0545, 'learning_rate': 2.7735464030121003e-07, 'epoch': 4.97}\n","{'loss': 0.0574, 'learning_rate': 2.0550110654441986e-07, 'epoch': 4.98}\n","{'loss': 0.055, 'learning_rate': 1.336475727876297e-07, 'epoch': 4.99}\n","{'loss': 0.0524, 'learning_rate': 6.179403903083954e-08, 'epoch': 4.99}\n","100% 347930/347930 [11:35:51<00:00,  9.07it/s][INFO|trainer.py:1171] 2021-05-04 21:59:08,568 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 41751.9015, 'train_samples_per_second': 8.333, 'epoch': 5.0}\n","100% 347930/347930 [11:35:51<00:00,  8.33it/s]\n","[INFO|trainer.py:1600] 2021-05-04 21:59:08,876 >> Saving model checkpoint to nb005-pytorch-bert-for-ner\n","[INFO|configuration_utils.py:318] 2021-05-04 21:59:08,881 >> Configuration saved in nb005-pytorch-bert-for-ner/config.json\n","[INFO|modeling_utils.py:837] 2021-05-04 21:59:10,578 >> Model weights saved in nb005-pytorch-bert-for-ner/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1896] 2021-05-04 21:59:10,582 >> tokenizer config file saved in nb005-pytorch-bert-for-ner/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-05-04 21:59:10,585 >> Special tokens file saved in nb005-pytorch-bert-for-ner/special_tokens_map.json\n","[INFO|trainer_pt_utils.py:735] 2021-05-04 21:59:10,628 >> ***** train metrics *****\n","[INFO|trainer_pt_utils.py:740] 2021-05-04 21:59:10,640 >>   epoch                      =         5.0\n","[INFO|trainer_pt_utils.py:740] 2021-05-04 21:59:10,641 >>   init_mem_cpu_alloc_delta   =      2258MB\n","[INFO|trainer_pt_utils.py:740] 2021-05-04 21:59:10,641 >>   init_mem_cpu_peaked_delta  =         0MB\n","[INFO|trainer_pt_utils.py:740] 2021-05-04 21:59:10,641 >>   init_mem_gpu_alloc_delta   =       411MB\n","[INFO|trainer_pt_utils.py:740] 2021-05-04 21:59:10,641 >>   init_mem_gpu_peaked_delta  =         0MB\n","[INFO|trainer_pt_utils.py:740] 2021-05-04 21:59:10,641 >>   train_mem_cpu_alloc_delta  =      -325MB\n","[INFO|trainer_pt_utils.py:740] 2021-05-04 21:59:10,641 >>   train_mem_cpu_peaked_delta =       657MB\n","[INFO|trainer_pt_utils.py:740] 2021-05-04 21:59:10,641 >>   train_mem_gpu_alloc_delta  =      1240MB\n","[INFO|trainer_pt_utils.py:740] 2021-05-04 21:59:10,641 >>   train_mem_gpu_peaked_delta =      6201MB\n","[INFO|trainer_pt_utils.py:740] 2021-05-04 21:59:10,641 >>   train_runtime              = 11:35:51.90\n","[INFO|trainer_pt_utils.py:740] 2021-05-04 21:59:10,641 >>   train_samples              =      556688\n","[INFO|trainer_pt_utils.py:740] 2021-05-04 21:59:10,641 >>   train_samples_per_second   =       8.333\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pZGAvQsZsAMA"},"source":["After the tuning finishes, we should find our model in './nb005-pytorch-bert-for-ner'."]},{"cell_type":"markdown","metadata":{"id":"HoXJ_mpt8dRg"},"source":["# Upload to Kaggle\n","./nb005-pytorch-bert-for-nerをKaggle datasetsにupload"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1dricfw02NvC","executionInfo":{"status":"ok","timestamp":1620165559742,"user_tz":-540,"elapsed":4673,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"054dd854-d825-486f-d15a-e395fe838776"},"source":["!pip install --upgrade --force-reinstall --no-deps kaggle\n","f = open(\"/content/drive/MyDrive/colab_notebooks/kaggle/kaggle.json\", \"r\")\n","json_data = json.load(f)\n","os.environ[\"KAGGLE_USERNAME\"] = json_data[\"username\"]\n","os.environ[\"KAGGLE_KEY\"] = json_data[\"key\"]"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Collecting kaggle\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/e7/3bac01547d2ed3d308ac92a0878fbdb0ed0f3d41fb1906c319ccbba1bfbc/kaggle-1.5.12.tar.gz (58kB)\n","\r\u001b[K     |█████▋                          | 10kB 18.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20kB 26.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30kB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40kB 21.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51kB 17.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 5.4MB/s \n","\u001b[?25hBuilding wheels for collected packages: kaggle\n","  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kaggle: filename=kaggle-1.5.12-cp37-none-any.whl size=73053 sha256=2312eee6e13e071adec9db2a2ec84a845573093985ab2b6e90ed9195389cecd0\n","  Stored in directory: /root/.cache/pip/wheels/a1/6a/26/d30b7499ff85a4a4593377a87ecf55f7d08af42f0de9b60303\n","Successfully built kaggle\n","Installing collected packages: kaggle\n","  Found existing installation: kaggle 1.5.12\n","    Uninstalling kaggle-1.5.12:\n","      Successfully uninstalled kaggle-1.5.12\n","Successfully installed kaggle-1.5.12\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uL9bNwj9cFZ-","executionInfo":{"status":"ok","timestamp":1620166338190,"user_tz":-540,"elapsed":829,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"98002a89-220d-4765-ec73-c52f1fe25494"},"source":["!ls"],"execution_count":23,"outputs":[{"output_type":"stream","text":["eda-coleridge-initiative.ipynb\n","kagglenb003-annotation-data.ipynb\n","kagglenb005-pytorch-bert-for-ner.ipynb\n","l2knb001-transformers-ner\n","localnb001-transformers-ner\n","localnb001-transformers-ner.ipynb\n","my_seqeval.py\n","my_seqeval.py.lock\n","nb003-annotation-data\n","nb003-annotation-data.ipynb\n","nb005-pytorch-bert-for-ner-512\n","nb005-pytorch-bert-for-ner.ipynb\n","NERDA\n","output_folder\n","pytorch-bert-for-named-entity-recognition.ipynb\n","pytorch-xla-env-setup.py\n","setup_kaggle.ipynb\n","torch-nightly+20200515-cp37-cp37m-linux_x86_64.whl\n","torchvision-nightly+20200515-cp37-cp37m-linux_x86_64.whl\n","torch_xla-nightly+20200515-cp37-cp37m-linux_x86_64.whl\n","train_ner.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DngF6fAd2YRI","executionInfo":{"status":"ok","timestamp":1620166418845,"user_tz":-540,"elapsed":66533,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"b02a4825-6f24-44b3-9dce-47d25c3bbcd7"},"source":["dname = \"nb005-pytorch-bert-for-ner-512\"\n","#!mkdir {dname}\n","!kaggle datasets init -p {dname}\n","# referene: https://kaeru-nantoka.hatenablog.com/entry/2020/01/17/015551\n","\n","with open(f\"{dname}/dataset-metadata.json\", \"r\") as jsonFile:\n","    data = json.load(jsonFile)\n","\n","data[\"id\"] = f\"riow1983/{dname}\"\n","data[\"title\"] = dname\n","\n","with open(f\"{dname}/dataset-metadata.json\", \"w\") as jsonFile:\n","    json.dump(data, jsonFile)\n","\n","# Copy train_ner.json\n","!cp train_ner.json ./{dname}/train_ner.json\n","!kaggle datasets create -p {dname}\n","#!kaggle datasets version -p {dname} -m \"[] hogehoge\""],"execution_count":24,"outputs":[{"output_type":"stream","text":["Data package template written to: nb005-pytorch-bert-for-ner-512/dataset-metadata.json\n","Skipping folder: checkpoint-15000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-30000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-45000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-60000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-75000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-90000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-105000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-120000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-135000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-150000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-165000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-180000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-195000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-210000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-225000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-240000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-255000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-270000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-285000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-300000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-315000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-330000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-345000; use '--dir-mode' to upload folders\n","Starting upload for file config.json\n","100% 801/801 [00:03<00:00, 211B/s]\n","Upload successful: config.json (801B)\n","Starting upload for file pytorch_model.bin\n","100% 411M/411M [00:17<00:00, 24.3MB/s]\n","Upload successful: pytorch_model.bin (411MB)\n","Starting upload for file tokenizer_config.json\n","100% 284/284 [00:02<00:00, 102B/s]\n","Upload successful: tokenizer_config.json (284B)\n","Starting upload for file special_tokens_map.json\n","100% 112/112 [00:03<00:00, 28.7B/s]\n","Upload successful: special_tokens_map.json (112B)\n","Starting upload for file vocab.txt\n","100% 208k/208k [00:03<00:00, 57.8kB/s]\n","Upload successful: vocab.txt (208KB)\n","Starting upload for file training_args.bin\n","100% 2.23k/2.23k [00:02<00:00, 849B/s]\n","Upload successful: training_args.bin (2KB)\n","Starting upload for file train_results.json\n","100% 461/461 [00:01<00:00, 233B/s]\n","Upload successful: train_results.json (461B)\n","Starting upload for file all_results.json\n","100% 461/461 [00:06<00:00, 73.1B/s]\n","Upload successful: all_results.json (461B)\n","Starting upload for file trainer_state.json\n","100% 84.0k/84.0k [00:04<00:00, 19.9kB/s]\n","Upload successful: trainer_state.json (84KB)\n","Starting upload for file train_ner.json\n","100% 220M/220M [00:10<00:00, 21.2MB/s]\n","Upload successful: train_ner.json (220MB)\n","Your private Dataset is being created. Please check progress at https://www.kaggle.com/riow1983/nb005-pytorch-bert-for-ner-512\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"guopcCF0CjGl","executionInfo":{"status":"ok","timestamp":1620009107498,"user_tz":-540,"elapsed":114954,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"d98dcaf5-8fcc-48fa-a137-f8b4376334f1"},"source":["# Copy train_ner.json\n","#!cp train_ner.json ./{dname}/train_ner.json\n","#!kaggle datasets version -p {dname} -m \"[Add] train_ner.json\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Skipping folder: checkpoint-15000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-30000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-45000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-60000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-75000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-90000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-105000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-120000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-135000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-150000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-165000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-180000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-195000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-210000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-225000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-240000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-255000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-270000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-285000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-300000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-315000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-330000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-345000; use '--dir-mode' to upload folders\n","Starting upload for file config.json\n","100% 829/829 [00:05<00:00, 158B/s]\n","Upload successful: config.json (829B)\n","Starting upload for file pytorch_model.bin\n","100% 411M/411M [00:32<00:00, 13.4MB/s]\n","Upload successful: pytorch_model.bin (411MB)\n","Starting upload for file tokenizer_config.json\n","100% 362/362 [00:08<00:00, 42.2B/s]\n","Upload successful: tokenizer_config.json (362B)\n","Starting upload for file special_tokens_map.json\n","100% 112/112 [00:08<00:00, 13.3B/s]\n","Upload successful: special_tokens_map.json (112B)\n","Starting upload for file vocab.txt\n","100% 208k/208k [00:05<00:00, 39.4kB/s]\n","Upload successful: vocab.txt (208KB)\n","Starting upload for file training_args.bin\n","100% 2.23k/2.23k [00:08<00:00, 275B/s]\n","Upload successful: training_args.bin (2KB)\n","Starting upload for file train_results.json\n","100% 462/462 [00:08<00:00, 54.8B/s]\n","Upload successful: train_results.json (462B)\n","Starting upload for file all_results.json\n","100% 462/462 [00:08<00:00, 53.5B/s]\n","Upload successful: all_results.json (462B)\n","Starting upload for file trainer_state.json\n","100% 84.6k/84.6k [00:04<00:00, 18.4kB/s]\n","Upload successful: trainer_state.json (85KB)\n","Starting upload for file train_ner.json\n","100% 217M/217M [00:17<00:00, 12.9MB/s]\n","Upload successful: train_ner.json (217MB)\n","Dataset version is being created. Please check progress at https://www.kaggle.com/riow1983/nb005-pytorch-bert-for-ner\n"],"name":"stdout"}]}]}