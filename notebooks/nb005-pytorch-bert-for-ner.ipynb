{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nb005-pytorch-bert-for-ner.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xbVABiahs17b","executionInfo":{"status":"ok","timestamp":1624231676578,"user_tz":-540,"elapsed":14,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"b3755520-1b70-44bf-8f02-afad354c0b77"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Sun Jun 20 23:27:56 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P0    28W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bdaoU9CrtClz","executionInfo":{"status":"ok","timestamp":1624231738792,"user_tz":-540,"elapsed":56535,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"f4ec62d7-d23a-4850-d6ee-5c20bef06a67"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","%cd /content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/notebooks"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RQVjAeyRmQ-8","executionInfo":{"status":"ok","timestamp":1624231743944,"user_tz":-540,"elapsed":3121,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"89d491ad-f621-441b-a33d-8003b2f0f966"},"source":["# Kaggle API\n","!pip install --upgrade --force-reinstall --no-deps kaggle\n","import json\n","import os\n","f = open(\"/content/drive/MyDrive/colab_notebooks/kaggle/kaggle.json\", \"r\")\n","json_data = json.load(f)\n","os.environ[\"KAGGLE_USERNAME\"] = json_data[\"username\"]\n","os.environ[\"KAGGLE_KEY\"] = json_data[\"key\"]"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting kaggle\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/e7/3bac01547d2ed3d308ac92a0878fbdb0ed0f3d41fb1906c319ccbba1bfbc/kaggle-1.5.12.tar.gz (58kB)\n","\r\u001b[K     |█████▋                          | 10kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20kB 20.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30kB 25.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40kB 25.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 6.8MB/s \n","\u001b[?25hBuilding wheels for collected packages: kaggle\n","  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kaggle: filename=kaggle-1.5.12-cp37-none-any.whl size=73053 sha256=b451873c670650c2a6c95293a1ba36b3baacb6a10d300c70b1706073042bec66\n","  Stored in directory: /root/.cache/pip/wheels/a1/6a/26/d30b7499ff85a4a4593377a87ecf55f7d08af42f0de9b60303\n","Successfully built kaggle\n","Installing collected packages: kaggle\n","  Found existing installation: kaggle 1.5.12\n","    Uninstalling kaggle-1.5.12:\n","      Successfully uninstalled kaggle-1.5.12\n","Successfully installed kaggle-1.5.12\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dHugIDq0sALu"},"source":["This notebook shows how to fine-tune a BERT model (from huggingface) for our dataset recognition task.\n","\n","Note that internet is needed during the training phase (for downloading the bert-base-cased model). Internet can be turned off during prediction."]},{"cell_type":"code","metadata":{"id":"KgFkN-NWGhhY","executionInfo":{"status":"ok","timestamp":1624231747246,"user_tz":-540,"elapsed":367,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["# CFG\n","create_dataset = False"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kTPntd9esAL4"},"source":["## Install packages"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5TpzFYS6tlxC","executionInfo":{"status":"ok","timestamp":1624170921834,"user_tz":-540,"elapsed":13493,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"b51c0aef-1a76-453e-8a5d-e53b33fee675"},"source":["#!pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n","!pip install datasets --no-index --find-links=file:///content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/input/coleridge-packages/packages/datasets"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Looking in links: file:///content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/input/coleridge-packages/packages/datasets\n","Processing /content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/input/coleridge-packages/packages/datasets/datasets-1.5.0-py3-none-any.whl\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Processing /content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/input/coleridge-packages/packages/datasets/fsspec-0.8.7-py3-none-any.whl\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (4.5.0)\n","Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Processing /content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/input/coleridge-packages/packages/datasets/huggingface_hub-0.0.7-py3-none-any.whl\n","Processing /content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/input/coleridge-packages/packages/datasets/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: fsspec, huggingface-hub, xxhash, datasets\n","Successfully installed datasets-1.5.0 fsspec-0.8.7 huggingface-hub-0.0.7 xxhash-2.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jVuAv21HsAL4","executionInfo":{"status":"ok","timestamp":1624170936607,"user_tz":-540,"elapsed":14800,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"d9ad883a-4a16-4b01-bc37-cdd250960a1b"},"source":["!pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n","!pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n","!pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Processing /content/drive/My Drive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval==1.2.2) (1.19.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval==1.2.2) (0.22.2.post1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.0.1)\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n","Processing /content/drive/My Drive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n","Installing collected packages: tokenizers\n","Successfully installed tokenizers-0.10.1\n","Processing /content/drive/My Drive/colab_notebooks/kaggle/Kaggle-Coleridge-Initiative/input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (3.0.12)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (0.10.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (20.9)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 4.3MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (4.5.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (4.41.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.0.dev0) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0.dev0) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0.dev0) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0.dev0) (1.0.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0.dev0) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0.dev0) (3.4.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (1.24.3)\n","Installing collected packages: sacremoses, transformers\n","Successfully installed sacremoses-0.0.45 transformers-4.5.0.dev0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DDBDvo4csAL5"},"source":["# Import"]},{"cell_type":"code","metadata":{"id":"jfEdYkccsAL6","executionInfo":{"status":"ok","timestamp":1624231753289,"user_tz":-540,"elapsed":810,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["import re\n","import time\n","import datetime\n","import random\n","import glob\n","import importlib\n","\n","import numpy as np\n","import pandas as pd\n","\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import gc\n","\n","random.seed(123)\n","np.random.seed(456)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"9QIuiG0asAL6"},"source":["# copy my_seqeval.py to the working directory because the input directory is non-writable\n","#!cp /kaggle/input/coleridge-packages/my_seqeval.py ./\n","!cp ../input/coleridge-packages/my_seqeval.py ./"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y1TvCs3NsAL7"},"source":["# Hyper-parameters"]},{"cell_type":"code","metadata":{"id":"RhW7ZDKRsAL7","executionInfo":{"status":"ok","timestamp":1624231757333,"user_tz":-540,"elapsed":309,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["MAX_LENGTH = 64 #512 #64 # max no. words for each sentence.\n","OVERLAP = 20 #170 #20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n","\n","MAX_SAMPLE = None # set a small number for experimentation, set None for production."],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_XwNllYFsAL7"},"source":["# Load data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29kmVrIWmD9B","executionInfo":{"status":"ok","timestamp":1624170988499,"user_tz":-540,"elapsed":49800,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"c3705db2-53c8-4f70-a2d0-7a3e4bf00cb6"},"source":["#papers = {}\n","#for paper_id in train['Id'].unique():\n","#    with open(f'{paper_train_folder}/{paper_id}.json', 'r') as f:\n","#        paper = json.load(f)\n","#        papers[paper_id] = paper\n","\n","#!cp -r {paper_train_folder} /content\n","\n","\n","# Download kagglenb007 output files to local ../input folder\n","dname = \"kagglenb007-get-text\"\n","!mkdir ../input/{dname}\n","!kaggle kernels output riow1983/{dname} -p ../input/{dname}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘../input/kagglenb007-get-text’: File exists\n","Output file downloaded to ../input/kagglenb007-get-text/folds_pubcat.pkl\n","Kernel log downloaded to ../input/kagglenb007-get-text/kagglenb007-get-text.log \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UeYZ7gvVsAL9","executionInfo":{"status":"ok","timestamp":1624170989695,"user_tz":-540,"elapsed":1230,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"4b0377bb-6306-4003-a6f1-83c6e5767097"},"source":["folds = pd.read_pickle(\"../input/kagglenb007-get-text/folds_pubcat.pkl\")\n","folds.drop_duplicates(subset=[\"Id\"], keep='first', ignore_index=True, inplace=True)\n","papers = {k:v for k,v in zip(folds[\"Id\"], folds[\"text\"])}\n","\n","\n","#train_path = '../input/coleridgeinitiative-show-us-the-data/train.csv'\n","#paper_train_folder = '../input/coleridgeinitiative-show-us-the-data/train'\n","#train = pd.read_csv(train_path)\n","folds = folds[:MAX_SAMPLE]\n","print(f'No. raw folds rows: {len(folds)}')\n","\n","# train = train.groupby('Id').agg({\n","#     'pub_title': 'first',\n","#     'dataset_title': '|'.join,\n","#     'dataset_label': '|'.join,\n","#     'cleaned_label': '|'.join\n","# }).reset_index()\n","#print(f'No. grouped training rows: {len(train)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["No. raw folds rows: 14271\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QxEkA-UhsAL9"},"source":["# Transform data to NER format"]},{"cell_type":"code","metadata":{"id":"Wn3QCVHrt2_Q","executionInfo":{"status":"ok","timestamp":1624231766821,"user_tz":-540,"elapsed":310,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["dname = \"nb005-pytorch-bert-for-ner\"\n","#!rm -r {dname}\n","#!mkdir {dname}"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"HfqzpqX4sAL9"},"source":["def clean_training_text(txt):\n","    \"\"\"\n","    similar to the default clean_text function but without lowercasing.\n","    \"\"\"\n","    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n","\n","def shorten_sentences(sentences):\n","    short_sentences = []\n","    for sentence in sentences:\n","        words = sentence.split()\n","        if len(words) > MAX_LENGTH:\n","            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n","                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n","        else:\n","            short_sentences.append(sentence)\n","    return short_sentences\n","\n","def find_sublist(big_list, small_list):\n","    all_positions = []\n","    for i in range(len(big_list) - len(small_list) + 1):\n","        if small_list == big_list[i:i+len(small_list)]:\n","            all_positions.append(i)\n","    \n","    return all_positions\n","\n","def tag_sentence(sentence, labels, verbose=False): # requirement: both sentence and labels are already cleaned\n","    sentence_words = sentence.split()\n","    \n","    if labels is not None and any(re.findall(f'\\\\b{label}\\\\b', sentence)\n","                                  for label in labels): # positive sample\n","        nes = ['O'] * len(sentence_words)\n","        if verbose:\n","            print(\"len(nes): \", len(nes))\n","        for label in labels:\n","            label_words = label.split()\n","\n","            all_pos = find_sublist(sentence_words, label_words)\n","            for pos in all_pos:\n","                nes[pos] = 'B'\n","                for i in range(pos+1, pos+len(label_words)):\n","                    nes[i] = 'I'\n","\n","        return True, list(zip(sentence_words, nes))\n","        \n","    else: # negative sample\n","        nes = ['O'] * len(sentence_words)\n","        return False, list(zip(sentence_words, nes))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y3Ocy5y6ZJ8P"},"source":["#papers['d0fa7568-7d8e-4db9-870f-f9c6f668c17b']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cD7k0HejsAL-"},"source":["def get_ner_json(df, fold, train_or_valid, verbose=False):\n","    \"\"\"\n","    Args:\n","        df: pd.DataFrame\n","        fold: int\n","        train_or_valid: str\n","        verbose: bool\n","    \"\"\"\n","    assert train_or_valid in [\"train\", \"valid\"], \"`train_or_valid` must be either 'train' or 'valid'.\"\n","    \n","    if verbose:\n","        cnt_pos, cnt_neg = 0, 0 # number of sentences that contain/not contain labels\n","    \n","    ner_data = []\n","    pbar = tqdm(total=len(df))\n","    for i, id, dataset_label in df[['Id', 'dataset_label']].itertuples():\n","        # paper\n","        paper = papers[id]\n","        \n","        # labels\n","        #labels = dataset_label.split('|')\n","        labels = dataset_label\n","        labels = [clean_training_text(label) for label in labels]\n","        \n","        # sentences\n","        sentences = set([clean_training_text(sentence) for section in paper \n","                    for sentence in section['text'].split('.') \n","                    ])\n","        sentences = shorten_sentences(sentences) # make sentences short\n","        sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n","        \n","        # positive sample\n","        for sentence in sentences:\n","            is_positive, tags = tag_sentence(sentence, labels, verbose=verbose)\n","            if is_positive:\n","                if verbose:\n","                    cnt_pos += 1\n","                ner_data.append(tags)\n","            elif any(word in sentence.lower() for word in ['data', 'study']): \n","                ner_data.append(tags)\n","                if verbose:\n","                    cnt_neg += 1\n","        \n","        # process bar\n","        pbar.update(1)\n","        if verbose:\n","            pbar.set_description(f\"Training data size: {cnt_pos} positives + {cnt_neg} negatives\")\n","\n","    # shuffling\n","    random.shuffle(ner_data)\n","\n","\n","    # write data to file.\n","    with open(f'./{dname}/fold_{fold}_{train_or_valid}_ner.json', 'w') as f:\n","        for row in ner_data:\n","            words, nes = list(zip(*row))\n","            row_json = {'tokens' : words, 'tags' : nes}\n","            json.dump(row_json, f)\n","            f.write('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bJm3H2sG6ndn"},"source":["if create_dataset:\n","    # get _ner.json files\n","    for fold in range(5):\n","        train = folds[folds[\"fold\"]!=fold+1].reset_index(drop=True)\n","        valid = folds[folds[\"fold\"]==fold+1].reset_index(drop=True)\n","        get_ner_json(train, fold+1, \"train\", verbose=False)\n","        get_ner_json(valid, fold+1, \"valid\", verbose=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"an-PfAp_sAMA"},"source":["# Fine-tune a BERT model for NER"]},{"cell_type":"code","metadata":{"id":"CJ7-PkmLIo5Z"},"source":["#os.environ[\"LOAD_BEST_MODEL_AT_END\"] = \"true\"\n","#!echo $LOAD_BEST_MODEL_AT_END"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xwVpZ4KaLWDM"},"source":["def train(fold=None, resume_training=False, num_checkpoint=None):\n","    \"\"\"\n","    Args:\n","        fold: int\n","        resume_training: bool\n","        num_checkpoint: int\n","    \"\"\"\n","    dname_child = f\"fold_{fold}\"\n","\n","    # foldごとにKaggle Datasetにuploadするためフォルダ名にfold数を入れる\n","    os.environ[\"OUTPUT_DIR\"] = dname_fold+\"/\"+dname_child\n","\n","    # train_ner.json, valid_ner.jsonは本家{dname}フォルダのものを共有\n","    os.environ[\"TRAIN_FILE\"] = f'./{dname}/fold_{fold}_train_ner.json'\n","    os.environ[\"VALID_FILE\"] = f'./{dname}/fold_{fold}_valid_ner.json'\n","\n","    if resume_training:\n","        if os.path.exists(f\"./{dname}/{dname_child}\"):\n","            pass\n","        else:\n","            raise KeyError(f\"./{dname}/{dname_child} does not exist! Please consider to train w/ resume_training=False.\")\n","        os.environ[\"MODEL_PATH\"] = f\"./{dname}/{dname_child}/checkpoint-{num_checkpoint}\"\n","        \n","        !python ../input/kaggle-ner-utils/kaggle_run_ner.py \\\n","        --model_name_or_path $MODEL_PATH \\\n","        --train_file $TRAIN_FILE \\\n","        --validation_file $VALID_FILE \\\n","        --num_train_epochs 1 \\\n","        --per_device_train_batch_size 8 \\\n","        --per_device_eval_batch_size 8 \\\n","        --save_strategy 'steps' \\\n","        --save_steps 15000 \\\n","        --output_dir $OUTPUT_DIR \\\n","        --report_to 'none' \\\n","        --seed 123 \\\n","        --do_train \\\n","        --do_eval \\\n","        --evaluation_strategy 'steps' \\\n","        --load_best_model_at_end 'true'\n","    else:\n","        if os.path.exists(f\"./{dname}/{dname_child}\"):\n","            #!rm -r {dname}/{dname_child}\n","            raise KeyError(f\"./{dname}/{dname_child} exists. Please consider to resume training w/ resume_training=True.\")\n","        else:\n","            !mkdir {dname}/{dname_child}\n","        \n","        !python ../input/kaggle-ner-utils/kaggle_run_ner.py \\\n","        --model_name_or_path 'bert-base-cased' \\\n","        --train_file $TRAIN_FILE \\\n","        --validation_file $VALID_FILE \\\n","        --num_train_epochs 1 \\\n","        --per_device_train_batch_size 8 \\\n","        --per_device_eval_batch_size 8 \\\n","        --save_strategy 'steps' \\\n","        --save_steps 15000 \\\n","        --output_dir $OUTPUT_DIR \\\n","        --report_to 'none' \\\n","        --seed 123 \\\n","        --do_train \\\n","        --do_eval \\\n","        --evaluation_strategy 'steps' \\\n","        --load_best_model_at_end 'true'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1n31gu8HKoAf"},"source":["#  for fold in range(5):\n","#      print()\n","#      print(f\"#################################### Start training for fold {fold+1} #################################################\")\n","#      train(fold=fold+1, resume_training=False, num_checkpoint=None)\n","\n","#train(fold=3, resume_training=False, num_checkpoint=None)\n","#train(resume_training=True, num_checkpoint=45000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VEyQoPERmfQY","executionInfo":{"status":"ok","timestamp":1624233198192,"user_tz":-540,"elapsed":319,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["i = 2 # set fold index to use\n","dname_fold = dname + f\"-fold-{i}\"\n","train(fold=i, resume_training=False, num_checkpoint=None)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pZGAvQsZsAMA"},"source":["After the tuning finishes, we should find our model in './nb005-pytorch-bert-for-ner-fold_{fold}'."]},{"cell_type":"markdown","metadata":{"id":"HoXJ_mpt8dRg"},"source":["# Upload to Kaggle\n","./nb005-pytorch-bert-for-ner-fold_{fold}をKaggle datasetsにupload"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_BAJDlqqZUW","executionInfo":{"status":"ok","timestamp":1624233216334,"user_tz":-540,"elapsed":323,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"9c596e52-8733-424e-ee54-651316443701"},"source":["print(dname_fold)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["nb005-pytorch-bert-for-ner-fold-2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DngF6fAd2YRI","executionInfo":{"status":"ok","timestamp":1624233334981,"user_tz":-540,"elapsed":85904,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"8b2042fb-d55a-4796-814d-2013c0db59ed"},"source":["# [Init] -----------------------------------------------------------------------------------\n","# foldごとにKaggle Datasetにuploadするためフォルダ名にfold数を入れる\n","#!mkdir {dname_fold}\n","!kaggle datasets init -p {dname_fold}\n","# referene: https://kaeru-nantoka.hatenablog.com/entry/2020/01/17/015551\n","\n","with open(f\"{dname_fold}/dataset-metadata.json\", \"r\") as jsonFile:\n","    data = json.load(jsonFile)\n","\n","data[\"id\"] = f\"riow1983/{dname_fold}\"\n","data[\"title\"] = dname_fold\n","\n","with open(f\"{dname_fold}/dataset-metadata.json\", \"w\") as jsonFile:\n","    json.dump(data, jsonFile)\n","\n","!kaggle datasets create -p {dname_fold}\n","\n","\n","\n","\n","# [Update] -----------------------------------------------------------------------------------\n","#!kaggle datasets version -p {dname_fold} -m \"[Update] model.bin file added\""],"execution_count":14,"outputs":[{"output_type":"stream","text":["Data package template written to: nb005-pytorch-bert-for-ner-fold-2/dataset-metadata.json\n","Skipping folder: fold_2; use '--dir-mode' to upload folders\n","Starting upload for file config.json\n","100% 801/801 [00:03<00:00, 212B/s]\n","Upload successful: config.json (801B)\n","Starting upload for file pytorch_model.bin\n","100% 411M/411M [00:18<00:00, 23.3MB/s]\n","Upload successful: pytorch_model.bin (411MB)\n","Starting upload for file tokenizer_config.json\n","100% 284/284 [00:04<00:00, 70.4B/s]\n","Upload successful: tokenizer_config.json (284B)\n","Starting upload for file special_tokens_map.json\n","100% 112/112 [00:04<00:00, 22.7B/s]\n","Upload successful: special_tokens_map.json (112B)\n","Starting upload for file vocab.txt\n","100% 208k/208k [00:03<00:00, 63.4kB/s]\n","Upload successful: vocab.txt (208KB)\n","Starting upload for file training_args.bin\n","100% 2.30k/2.30k [00:05<00:00, 435B/s]\n","Upload successful: training_args.bin (2KB)\n","Starting upload for file optimizer.pt\n","100% 822M/822M [00:32<00:00, 26.9MB/s]\n","Upload successful: optimizer.pt (822MB)\n","Starting upload for file scheduler.pt\n","100% 623/623 [00:02<00:00, 227B/s]\n","Upload successful: scheduler.pt (623B)\n","Starting upload for file trainer_state.json\n","100% 1.62k/1.62k [00:05<00:00, 314B/s]\n","Upload successful: trainer_state.json (2KB)\n","Your private Dataset is being created. Please check progress at /api/v1/datasets/status//riow1983/nb005-pytorch-bert-for-ner-fold-2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YeydqU73vQjX","executionInfo":{"status":"ok","timestamp":1624231929542,"user_tz":-540,"elapsed":1291,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"33865f88-b1b0-4319-cde3-d15af03042bf"},"source":["!kaggle datasets list --mine"],"execution_count":11,"outputs":[{"output_type":"stream","text":["ref                                        title                               size  lastUpdated          downloadCount  voteCount  usabilityRating  \n","-----------------------------------------  ---------------------------------  -----  -------------------  -------------  ---------  ---------------  \n","riow1983/localnb001-transformers-ner       localnb001-transformers-ner          9GB  2021-06-05 14:48:02              0          0  0.375            \n","ti110106/spacy-cv-4-model                  SpaCy_cv_4_model                   868MB  2021-06-20 02:26:22              0          0  0.1875           \n","riow1983/nb009-cv                          nb009-cv                           876KB  2021-06-17 23:10:56              2          0  0.125            \n","ti110106/spacy-train-set                   SPACY_TRAIN_SET                      4GB  2021-06-20 11:29:11              2          0  0.125            \n","riow1983/nb005-pytorch-bert-for-ner        nb005-pytorch-bert-for-ner         599MB  2021-06-19 05:05:32              0          0  0.25             \n","riow1983/nb005-pytorch-bert-for-ner-512    nb005-pytorch-bert-for-ner-512     426MB  2021-05-04 22:13:35              0          0  0.25             \n","riow1983/nb003-annotation-data             nb003-annotation-data              290MB  2021-05-14 06:36:16              0          0  0.125            \n","riow1983/informer2020                      Informer2020                       843KB  2021-02-12 00:23:51              0          0  0.125            \n","riow1983/saint-plus-dataset-v2             saint_plus_dataset_v2              925MB  2021-01-07 09:44:10              2          0  0.0625           \n","ti110106/riiid-lgbm-full-data              Riiid_LGBM_FULL_DATA               440MB  2021-01-05 10:00:06              1          0  0.0              \n","riow1983/saint-plus-v7                     saint_plus_v7                       15MB  2021-01-07 15:11:44              2          0  0.1875           \n","riow1983/saint-dataset                     saint_dataset                      473MB  2021-01-06 00:21:31              0          0  0.0625           \n","riow1983/saint-v16                         saint_v16                           21MB  2021-01-05 22:57:15              1          0  0.1875           \n","riow1983/saint-plus-dataset-v3             saint_plus_dataset_v3              913MB  2021-01-06 23:03:23              0          0  0.0              \n","riow1983/saint-plus-v10                    saint_plus_v10                      15MB  2021-01-07 04:37:30              0          0  0.1875           \n","riow1983/sakt-dataset                      sakt_dataset                       403MB  2021-01-09 01:23:25              0          0  0.0625           \n","riow1983/saint-plus                        saint_plus                          22MB  2021-01-03 03:26:02              0          0  0.125            \n","riow1983/riiid-test-df-generated-from-api  Riiid! test df generated from API   10KB  2020-11-17 01:10:25              0          0  0.11764706       \n","riow1983/osic-vae                          OSIC VAE                           295KB  2020-08-23 00:39:32              0          0  0.25             \n","riow1983/weights-for-gwd                   weights_for_gwd                    519MB  2020-07-27 13:12:40              0          0  0.125            \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ROlh7blUtBpD","executionInfo":{"status":"ok","timestamp":1624231026267,"user_tz":-540,"elapsed":154257,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"801c5dee-5dd3-40a0-eb17-94ed761867a7"},"source":["!kaggle datasets create -p {dname_fold}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Skipping folder: fold_2; use '--dir-mode' to upload folders\n","Starting upload for file config.json\n","100% 801/801 [00:12<00:00, 65.6B/s]\n","Upload successful: config.json (801B)\n","Starting upload for file pytorch_model.bin\n","100% 411M/411M [00:31<00:00, 13.8MB/s]\n","Upload successful: pytorch_model.bin (411MB)\n","Starting upload for file tokenizer_config.json\n","100% 284/284 [00:08<00:00, 34.3B/s]\n","Upload successful: tokenizer_config.json (284B)\n","Starting upload for file special_tokens_map.json\n","100% 112/112 [00:06<00:00, 17.2B/s]\n","Upload successful: special_tokens_map.json (112B)\n","Starting upload for file vocab.txt\n","100% 208k/208k [00:09<00:00, 21.5kB/s]\n","Upload successful: vocab.txt (208KB)\n","Starting upload for file training_args.bin\n","100% 2.30k/2.30k [00:06<00:00, 338B/s]\n","Upload successful: training_args.bin (2KB)\n","Starting upload for file optimizer.pt\n","100% 822M/822M [00:54<00:00, 15.7MB/s]\n","Upload successful: optimizer.pt (822MB)\n","Starting upload for file scheduler.pt\n","100% 623/623 [00:10<00:00, 58.9B/s]\n","Upload successful: scheduler.pt (623B)\n","Starting upload for file trainer_state.json\n","100% 1.62k/1.62k [00:10<00:00, 161B/s]\n","Upload successful: trainer_state.json (2KB)\n","400 - Bad Request\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"guopcCF0CjGl","executionInfo":{"elapsed":114954,"status":"ok","timestamp":1620009107498,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"},"user_tz":-540},"outputId":"d98dcaf5-8fcc-48fa-a137-f8b4376334f1"},"source":["# Copy train_ner.json\n","#!cp train_ner.json ./{dname}/train_ner.json\n","#!kaggle datasets version -p {dname} -m \"[Add] train_ner.json\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Skipping folder: checkpoint-15000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-30000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-45000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-60000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-75000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-90000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-105000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-120000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-135000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-150000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-165000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-180000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-195000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-210000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-225000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-240000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-255000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-270000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-285000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-300000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-315000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-330000; use '--dir-mode' to upload folders\n","Skipping folder: checkpoint-345000; use '--dir-mode' to upload folders\n","Starting upload for file config.json\n","100% 829/829 [00:05<00:00, 158B/s]\n","Upload successful: config.json (829B)\n","Starting upload for file pytorch_model.bin\n","100% 411M/411M [00:32<00:00, 13.4MB/s]\n","Upload successful: pytorch_model.bin (411MB)\n","Starting upload for file tokenizer_config.json\n","100% 362/362 [00:08<00:00, 42.2B/s]\n","Upload successful: tokenizer_config.json (362B)\n","Starting upload for file special_tokens_map.json\n","100% 112/112 [00:08<00:00, 13.3B/s]\n","Upload successful: special_tokens_map.json (112B)\n","Starting upload for file vocab.txt\n","100% 208k/208k [00:05<00:00, 39.4kB/s]\n","Upload successful: vocab.txt (208KB)\n","Starting upload for file training_args.bin\n","100% 2.23k/2.23k [00:08<00:00, 275B/s]\n","Upload successful: training_args.bin (2KB)\n","Starting upload for file train_results.json\n","100% 462/462 [00:08<00:00, 54.8B/s]\n","Upload successful: train_results.json (462B)\n","Starting upload for file all_results.json\n","100% 462/462 [00:08<00:00, 53.5B/s]\n","Upload successful: all_results.json (462B)\n","Starting upload for file trainer_state.json\n","100% 84.6k/84.6k [00:04<00:00, 18.4kB/s]\n","Upload successful: trainer_state.json (85KB)\n","Starting upload for file train_ner.json\n","100% 217M/217M [00:17<00:00, 12.9MB/s]\n","Upload successful: train_ner.json (217MB)\n","Dataset version is being created. Please check progress at https://www.kaggle.com/riow1983/nb005-pytorch-bert-for-ner\n"],"name":"stdout"}]}]}